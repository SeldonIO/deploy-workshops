{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Workshop - *Predicting product ratings from reviews*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop is focused on the creation, deployment, monitoring and management of a machine learning model for predicting product ratings from reviews.\n",
    "\n",
    "### Tom - this isn't completely relevant anymore, will change it before workshop to talk about pretrained model being used etc, so just ignore for now\n",
    "\n",
    "In this notebook we will be exploring the data, and training the machine learning model itself; in the form of a Tensorflow Keras model. We will then deploy a second Tensorflow model with differing architecture as a Canary model to demonstrate the A/B testing functionality Seldon provides. Then we will begin to add the advanced monitoring and explainability that Seldon Alibi is famed for. \n",
    "\n",
    "-----------------------------------\n",
    "Firstly, we will install and import the relevant packages which we will use throughout the exploration, training, and deployment process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install seldon_deploy_sdk\n",
    "!pip install alibi==0.6.4\n",
    "!pip install alibi_detect==0.8.1\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datasets\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, create_optimizer, DefaultDataCollator, TFAutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from seldon_deploy_sdk import Configuration, ApiClient, SeldonDeploymentsApi, ModelMetadataServiceApi, DriftDetectorApi, BatchJobsApi, BatchJobDefinition\n",
    "from seldon_deploy_sdk.auth import OIDCAuthenticator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://kelly-seldon/nlp-ratings/review_data.csv review_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"review_data.csv\", delimiter=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Can we do more here? Remove punctuation etc? What does the pretrained tokenizer do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].astype(str)\n",
    "df['rating'] = df['rating'].astype(str)\n",
    "\n",
    "rating_mapping = {\n",
    "    '1.0': 0,\n",
    "    '1.5': 1,\n",
    "    '2.0': 2,\n",
    "    '2.5': 3,\n",
    "    '3.0': 4,\n",
    "    '3.5': 5,\n",
    "    '4.0': 6, \n",
    "    '4.5': 7,\n",
    "    '5.0': 8\n",
    "}\n",
    "\n",
    "df['label'] = df['rating'].apply(lambda x: rating_mapping[x])\n",
    "df.drop(columns=['product', 'user_id', 'date_created', 'rating'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into Dataset type\n",
    "train_ds = datasets.Dataset.from_pandas(train, preserve_index=False)\n",
    "test_ds = datasets.Dataset.from_pandas(test, preserve_index=False)\n",
    "comp_ds = datasets.DatasetDict({\"train\":train_ds,\"test\":test_ds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(df):\n",
    "    return tokenizer(df[\"review\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_revs = comp_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = tokenized_revs[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tf_test_set = tokenized_revs[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(tf_train_set, validation_data=tf_test_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = []\n",
    "# labels = []\n",
    "\n",
    "# for row in df.iterrows():\n",
    "#     labels.append(row[1]['label'])\n",
    "#     review = row[1]['review']\n",
    "#     for word in STOPWORDS:\n",
    "#         token = ' ' + word + ' '\n",
    "#         review = review.replace(token, ' ')\n",
    "#         review = review.replace(' ', ' ')\n",
    "#     reviews.append(review)\n",
    "# print(len(labels))\n",
    "# print(len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "# mpl.rcParams['figure.figsize'] = (12, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_metrics(history):\n",
    "#     metrics = ['loss', 'accuracy']\n",
    "#     for n, metric in enumerate(metrics):\n",
    "#         name = metric.replace(\"_\",\" \").capitalize()\n",
    "#         plt.subplot(2,2,n+1)\n",
    "#         plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "#         plt.plot(history.epoch, history.history['val_'+metric],\n",
    "#                  color=colors[0], linestyle=\"--\", label='Val')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.ylabel(name)\n",
    "#         if metric == 'loss':\n",
    "#             plt.ylim([0, plt.ylim()[1]])\n",
    "#         elif metric == 'auc':\n",
    "#             plt.ylim([0.8,1])\n",
    "#         else:\n",
    "#             plt.ylim([0,1])\n",
    "\n",
    "#         plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_loss, int_accuracy = int_model.evaluate(int_test_ds[0], int_test_ds[1])\n",
    "\n",
    "# print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = int_model.predict(int_test_ds[0])\n",
    "# print(predictions[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_re = int_test_ds[1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_val_list = []\n",
    "# pred_val_list = []\n",
    "\n",
    "# for i in range(0,len(test_re)):\n",
    "#     test_val = test_re[i]\n",
    "#     test_val_list.append(test_val)\n",
    "#     pred_val = list(predictions[i]).index(predictions[i].max())\n",
    "#     pred_val_list.append(pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame(\n",
    "#         {'Test_val':test_val_list,\n",
    "#             'Pred_val':pred_val_list}\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results['match']='False'\n",
    "# df_results.loc[df_results['Test_val']==df_results['Pred_val'],'match'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_group = df_results.groupby(['Test_val','match']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results['Pred_val'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation if accuracy not provided as metric when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(tf_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[\"logits\"][3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_preds = []\n",
    "for i in y_preds[\"logits\"]:\n",
    "    rating_preds.append(np.argmax(i, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(rating_preds, test[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now deploy your model to the dedicated Seldon Deploy cluster which we have configured for this workshop. To do so you will interact with the Seldon Deploy SDK and deploy your model using that.\n",
    "\n",
    "First, setting up the configuration and authentication required to access the cluster. Make sure to fill in the `SD_IP` variable to be the same as the cluster you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD_IP = \"34.141.146.222\"\n",
    "\n",
    "config = Configuration()\n",
    "config.host = f\"http://{SD_IP}/seldon-deploy/api/v1alpha1\"\n",
    "config.oidc_client_id = \"sd-api\"\n",
    "config.oidc_server = f\"http://{SD_IP}/auth/realms/deploy-realm\"\n",
    "config.oidc_client_secret = \"sd-api-secret\"\n",
    "config.auth_method = \"client_credentials\"\n",
    "\n",
    "def auth():\n",
    "    auth = OIDCAuthenticator(config)\n",
    "    config.id_token = auth.authenticate()\n",
    "    api_client = ApiClient(configuration=config, authenticator=auth)\n",
    "    return api_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have configured the IP correctly as well as setup your authentication function you can describe the deployment you would like to create. \n",
    "\n",
    "You will need to fill in the `YOUR_NAME` variable. This MUST be lower case as it will be used for the `DEPLOYMENT_NAME` variable later on.\n",
    "\n",
    "The `MODEL_NAME` and `MODEL_PATH` variables have been prefilled for you as we are using a pretrained model and a pre-built container image for the sake of saving time in the workshop. \n",
    "\n",
    "The rest of the `mldeployment` description has been completed for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = \"kellyspry\"\n",
    "MODEL_NAME = \"review-ratings\"\n",
    "\n",
    "DEPLOYMENT_NAME = f\"{YOUR_NAME}-{MODEL_NAME}\"\n",
    "CONTAINER_NAME = f\"kellyspry0316/{MODEL_NAME}:0.6\"\n",
    "\n",
    "NAMESPACE = \"seldon-gitops\"\n",
    "\n",
    "CPU_REQUESTS = \"0.1\"\n",
    "MEMORY_REQUESTS = \"1Gi\"\n",
    "\n",
    "CPU_LIMITS = \"0.1\"\n",
    "MEMORY_LIMITS = \"1Gi\"\n",
    "\n",
    "MODEL_PATH = \"nlp-ratings/1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldeployment = {\n",
    "    \"kind\": \"SeldonDeployment\",\n",
    "    \"metadata\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"namespace\": NAMESPACE,\n",
    "        \"labels\": {\n",
    "            \"fluentd\": \"true\"\n",
    "        }\n",
    "    },\n",
    "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "    \"spec\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"annotations\": {\n",
    "            \"seldon.io/engine-seldon-log-messages-externally\": \"true\"\n",
    "        },\n",
    "        \"protocol\": \"seldon\",\n",
    "        \"predictors\": [\n",
    "            {\n",
    "                \"componentSpecs\": [\n",
    "                    {\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                                    \"image\": CONTAINER_NAME,\n",
    "                                    \"resources\": {\n",
    "                                        \"requests\": {\n",
    "                                            \"cpu\": CPU_REQUESTS,\n",
    "                                            \"memory\": MEMORY_REQUESTS\n",
    "                                        },\n",
    "                                        \"limits\": {\n",
    "                                            \"cpu\": CPU_LIMITS,\n",
    "                                            \"memory\": MEMORY_LIMITS\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": \"default\",\n",
    "                \"replicas\": 1,\n",
    "                \"traffic\": 100,\n",
    "                \"graph\": {\n",
    "                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                    \"parameters\": [\n",
    "                        {\n",
    "                            \"name\":\"model_path\",\n",
    "                            \"value\":MODEL_PATH,\n",
    "                            \"type\":\"STRING\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"children\": [],\n",
    "                    \"logger\": {\n",
    "                        \"mode\": \"all\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"status\": {}\n",
    "}\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now invoke the SeldonDeploymentsApi and create a new Seldon Deployment.\n",
    "\n",
    "Time for you to get your hands dirty. You will use the Seldon Deploy SDK to create a new Seldon deployment. You can find the reference documentation [here](https://github.com/SeldonIO/seldon-deploy-sdk/blob/master/python/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a good review:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "                \"ndarray\": [\n",
    "                    [\n",
    "                    \"This product is the best, it is so amazing. Incredible!\"\n",
    "                    ]\n",
    "                 ]\n",
    "           }\n",
    "}\n",
    "                        \n",
    "```\n",
    "\n",
    "\n",
    "Example of a bad review:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "                \"ndarray\": [\n",
    "                    [\n",
    "                    \"This product is the worst, it is absolutely terrible. Awful!\"\n",
    "                    ]\n",
    "                 ]\n",
    "           }\n",
    "}\n",
    "                        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the Seldon Deploy cluster and view your freshly created deployment here:\n",
    "\n",
    "### To do - add cluster IP in here when cluster is decided on\n",
    "\n",
    "* URL: http://34.141.146.222/seldon-deploy/\n",
    "* Username: admin@seldon.io\n",
    "* Password: 12341234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Prediction Schema and Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seldon Deploy has a model catalog where all deployed models are automatically registered. The model catalog can store custom metadata as well as prediction schemas for your models.\n",
    "\n",
    "Metadata promotes lineage from across different machine learning systems, aids knowledge transfer between teams, and allows for faster deployment. Meanwhile, prediction schemas allow Seldon Deploy to automatically profile tabular data into histograms, allowing for filtering on features to explore trends.\n",
    "\n",
    "In order to effectively construct a prediction schema Seldon has the ML Prediction Schema project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_schema = {\n",
    "    \"requests\": [\n",
    "        {\n",
    "            \"name\": \"Product Review\",\n",
    "            \"type\": \"TEXT\",\n",
    "        }\n",
    "    ],\n",
    "    \"responses\": [\n",
    "        {\n",
    "            \"name\": \"Rating\",\n",
    "            \"type\": \"CATEGORICAL\",\n",
    "            \"data_type\": \"INT\",\n",
    "            \"n_categories\": 9,\n",
    "            \"category_map\": {\n",
    "            \"0\": \"1.0\",\n",
    "            \"1\": \"1.5\",\n",
    "            \"2\": \"2.0\",\n",
    "            \"3\": \"2.5\",\n",
    "            \"4\": \"3.0\",\n",
    "            \"5\": \"3.5\",\n",
    "            \"6\": \"4.0\",\n",
    "            \"7\": \"4.5\",\n",
    "            \"8\": \"5.0\"\n",
    "        }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then add the prediction schema to the wider model catalog metadata. This includes information such as the model storage location, the name, who authored the model etc. The metadata tags and metrics which can be associated with a model are freeform and can therefore be determined based upon the use case which is being developed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do if time - add accuracy as a metric in here once have a working model\n",
    "\n",
    "model_catalog_metadata = {\n",
    "      \"URI\": MODEL_LOCATION,\n",
    "      \"name\": f\"{DEPLOYMENT_NAME}-model\",\n",
    "      \"version\": \"v1.0\",\n",
    "      \"artifactType\": \"Tensorflow\",\n",
    "      \"taskType\": \"Product review rating classification\",\n",
    "      \"tags\": {\n",
    "        \"auto_created\": \"true\",\n",
    "        \"author\": f\"{YOUR_NAME}\"\n",
    "      },\n",
    "      \"metrics\": {\n",
    "          \"accuracy\": 0,\n",
    "      },\n",
    "      \"project\": \"default\",\n",
    "      \"prediction_schema\": prediction_schema\n",
    "    }\n",
    "\n",
    "model_catalog_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using the metadata API you can add this to the model which you have just created in Seldon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_api = ModelMetadataServiceApi(auth())\n",
    "metadata_api.model_metadata_service_update_model_metadata(model_catalog_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then list the metadata via the API, or view it in the UI, to confirm that it has been successfully added to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_response = metadata_api.model_metadata_service_list_model_metadata(uri=MODEL_LOCATION)\n",
    "metadata_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can have a go at sending some requests to your model using the 'Predict' tab in the UI. \n",
    "\n",
    "An example of a good review that we would expect to correspond to a higher rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "        \"names\": [\"Review\"],\n",
    "        \"ndarray\": [\"_product_ is excellent! I love it, it's great!\"]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "And an example of a negative review that we would expect to correspond to a lower rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "        \"names\": [\"Review\"],\n",
    "        \"ndarray\": [\"_product_ was terrible, I would not use it again, it was awful!\"]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although powerful, modern machine learning models can be sensitive. Seemingly subtle changes in a data distribution can destroy the performance of otherwise state-of-the art models, which can be especially problematic when ML models are deployed in production. Typically, ML models are tested on held out data in order to estimate their future performance. Crucially, this assumes that the process underlying the input data `X` and output data `Y` remains constant.\n",
    "\n",
    "Drift can be classified into the following types:\n",
    "* **Covariate drift**: Also referred to as input drift, this occurs when the distribution of the input data has shifted `P(X) != Pref(X)`, whilst `P(Y|X) = Pref(Y|X)`. This may result in the model giving unreliable predictions.\n",
    "\n",
    "* **Prior drift**: Also referred to as label drift, this occurs when the distribution of the outputs has shifted `P(Y) != Pref(Y)`, whilst `P(X|Y) = Pref(X|Y)`. This can affect the model’s decision boundary, as well as the model’s performance metrics.\n",
    "\n",
    "* **Concept drift**: This occurs when the process generating `Y` from `X` has changed, such that `P(Y|X) != Pref(Y|X)`. It is possible that the model might no longer give a suitable approximation of the true process.\n",
    "\n",
    "-----------------\n",
    "\n",
    "In this instance we will train a Kolmgorov-Smirnov drift detector to pick up on covariate drift. The KS Drift detector applies a two-sample KS test to compare the distance between the new probability distribution and the reference distribution. \n",
    "\n",
    "This is done on a feature by feature basis and the results are then aggregated using a correction, i.e. Bonferroni, to determine whether drift has occurred overall within the sample. \n",
    "\n",
    "We will use the training set as our reference distribution. Creating our drift detector is then as simple as writing a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_array = np.asarray(encoded_dataset['train']['input_ids'][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = KSDrift(dd_array, p_val=.05, correction='bonferroni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dd.predict(dd_array)\n",
    "labels = [\"No\", \"Yes\"]\n",
    "\n",
    "print(f'Drift? {labels[preds[\"data\"][\"is_drift\"]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dd.predict(\n",
    "    np.asarray(encoded_dataset['test']['input_ids'][6700:6800]), \n",
    "    return_p_val=True, \n",
    "    return_distance=True)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need something here to decode before using for explanations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_detector(dd, \"reviews-drift-detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Will add this earlier when saving the model\n",
    "\n",
    "YOUR_NAME = \"kelly-spry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r reviews-drift-detector gs://kelly-seldon/nlp-ratings/models/{YOUR_NAME}/reviews-drift-detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the drift detector you will use Seldon Deploy's user interface. Simply navigate to your deployment, and select the \"Create\" button for your drift detector.\n",
    "\n",
    "This will bring up a form. Add your detector name, the URI which will be gs://kelly-seldon/nlp-ratings/models/{YOUR_NAME}/reviews-drift-detector, and set a batch size of 10.\n",
    "\n",
    "The batch size configuration sets how many data points have to be sent to the endpoint before drift is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Batch Job\n",
    "\n",
    "The simplest way to test that your drift detector and prediction schema are behaving correctly is to kick off a batch job. \n",
    "\n",
    "There is already a pre-prepared batch of data stored in a Minio bucket which Seldon can access, therefore all you have to do is provide the configuration which is outlined below. This shows all of the available parameters which can be changed, but not all are required and are shown for educational purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Tom about batch_gateway_endpoint - is this the name of the cluster? is this arbituary? \n",
    "# Should this be hard coded or changed to match peoples model names in the workshop\n",
    "\n",
    "workflow = BatchJobDefinition(\n",
    "    batch_data_type='data',\n",
    "    batch_gateway_type='seldon',\n",
    "    batch_interval=0.0,\n",
    "    batch_method='predict',\n",
    "    batch_payload_type='ndarray',\n",
    "    batch_retries='3',\n",
    "    batch_size=0,\n",
    "    batch_transport_protocol='rest',\n",
    "    batch_workers='1',\n",
    "    input_data='minio://batch/reviews-batch.txt',\n",
    "    object_store_secret_name='minio-bucket',\n",
    "    output_data='minio://batch/reviews-batch-output.txt',\n",
    "    pvc_size='1Gi'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then pass the above configuration to the batch job API and kick it off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_api = BatchJobsApi(auth())\n",
    "batch_api.create_seldon_deployment_batch_job(DEPLOYMENT_NAME, NAMESPACE, workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will train an explainer to glean deeper insights into the decisions being made by your model. \n",
    "\n",
    "You will make use of the Anchors algorithm, which has a [production grade implementation available](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html) using the Seldon Alibi Explain library. \n",
    "\n",
    "The first step will be to write a simple prediction function which the explainer can call in order to query your Tensorflow model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(x):\n",
    "    return clf.predict_proba(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then initialise your Anchor explainer, using the AnchorText flavour provided by Alibi due to your data modality. \n",
    "\n",
    "### This text might need changing based on what we use as the language model for this. One to discuss with Tom.\n",
    "\n",
    "The AnchorText class expects the prediction function which you defined above, as well as a sampling strategy and nlp/language model. You can find a sample notebook in the Alibi docs [here]https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_text_movie.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(\n",
    "    predictor=predict_fn,\n",
    "    sampling_strategy='similarity',   # Replace masked words by simialar words\n",
    "    nlp=nlp,                          # Spacy object\n",
    "    sample_proba=0.5,                 # Probability of a word to be masked and replace by as similar word\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then run an explanation for a given review locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain(text, threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now save your explainer, and upload it to the GS bucket. You can use the explainer's built-in save method to do this easily and reproducibly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.save(\"review-explainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r reviews-explainer gs://kelly-seldon/nlp-ratings/models/{YOUR_NAME}/reviews-explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now deploy our explainer alongside our model. First you define the explainer configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLAINER_TYPE = \"AnchorText\"\n",
    "EXPLAINER_URI = f\"gs://kelly-seldon/nlp-ratings/models/{YOUR_NAME}/reviews-explainer\"\n",
    "\n",
    "explainer_spec = {\n",
    "                    \"type\": EXPLAINER_TYPE,\n",
    "                    \"modelUri\": EXPLAINER_URI,\n",
    "                    \"containerSpec\": {\n",
    "                        \"name\": \"\",\n",
    "                        \"resources\": {}\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then insert this additional configuration into your original `mldeployment` specification which you defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldeployment['spec']['predictors'][0]['explainer'] = explainer_spec\n",
    "mldeployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then deploy the explainer to the Seldon Deploy cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then run the same predictions through your model as above and generate explanations on them:\n",
    "\n",
    "An example of a good review that we would expect to correspond to a higher rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "        \"names\": [\"Review\"],\n",
    "        \"ndarray\": [\"_product_ is excellent! I love it, it's great!\"]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "And an example of a negative review that we would expect to correspond to a lower rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "        \"names\": [\"Review\"],\n",
    "        \"ndarray\": [\"_product_ was terrible, I would not use it again, it was awful!\"]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
