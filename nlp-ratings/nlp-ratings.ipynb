{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Workshop - *Predicting product ratings from reviews*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop is focused on the creation, deployment, monitoring and management of a machine learning model for predicting product ratings from reviews.\n",
    "\n",
    "In this notebook we will be exploring the data, and going through the steps to train the machine learning model itself. We will use a fine tuned DistilBERT hugging face transformer model (https://huggingface.co/docs/transformers/main/en/model_doc/distilbert), which is a \"is a small, fast, cheap and light Transformer model trained by distilling BERT base\". For the sake of time, we will use a pretrained model rather than training the model in the workshop. \n",
    "\n",
    "We will then deploy our trained model using the Seldon Deploy SDK and view our running deployments in the Seldon Deploy UI. \n",
    "\n",
    "We will deploy a second Tensorflow model with slightly differing architecture as a Canary model to demonstrate the A/B testing functionality Seldon Deploy provides. This time we will deploy direct from the UI. \n",
    "\n",
    "Then we will begin to add the advanced monitoring that Seldon Alibi is famed for. \n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "Firstly, we will install and import the relevant packages which we will use throughout the exploration, training, and deployment process. Google Colab comes with a number of packages pre-installed, so we only need to install any additional packages we may need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install seldon_deploy_sdk\n",
    "!pip install alibi==0.6.4\n",
    "!pip install alibi_detect==0.8.1\n",
    "!pip install datasets\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datasets\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "from transformers import AutoTokenizer, DefaultDataCollator, TFAutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from seldon_deploy_sdk import Configuration, ApiClient, SeldonDeploymentsApi, ModelMetadataServiceApi, DriftDetectorApi, BatchJobsApi, BatchJobDefinition\n",
    "from seldon_deploy_sdk.auth import OIDCAuthenticator\n",
    "\n",
    "import spacy\n",
    "from alibi.utils.download import spacy_model\n",
    "from alibi.explainers import AnchorText\n",
    "from alibi_detect.cd import KSDrift\n",
    "from alibi_detect.utils.saving import save_detector, load_detector\n",
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "The reviews data is held in a Google Storage bucket. We can download the data using the gsutil tool, which enables us to access Google Cloud storage from the command line. We then load the data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://kelly-seldon/nlp-ratings/review_data.csv review_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"review_data.csv\", delimiter=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can drop all columns that we don't need to just leave us with ```rating``` and ```review```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['product', 'user_id', 'date_created'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check for any missing data in our DataFrame and drop rows where the review is missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_NaN = df.isnull()\n",
    "row_has_NaN = is_NaN.any(axis=1)\n",
    "rows_with_NaN = df[row_has_NaN]\n",
    "print(rows_with_NaN.head(), \"\\n\\n\", \"Number of rows with missing values:\", len(rows_with_NaN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(rows_with_NaN.index)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then ensure that our reviews and ratings columns are strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].astype(str)\n",
    "df['rating'] = df['rating'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can map our string rating categories to integers, which will be the output labels for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mapping = {\n",
    "    '1.0': 0,\n",
    "    '1.5': 1,\n",
    "    '2.0': 2,\n",
    "    '2.5': 3,\n",
    "    '3.0': 4,\n",
    "    '3.5': 5,\n",
    "    '4.0': 6, \n",
    "    '4.5': 7,\n",
    "    '5.0': 8\n",
    "}\n",
    "\n",
    "df['label'] = df['rating'].apply(lambda x: rating_mapping[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then drop the rating column to leave us with ```review``` and ```label```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"rating\", axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at some of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review[49977]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review[29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a clear need for some text preprocessing of the reviews. \n",
    "\n",
    "We will carry out the following preprocessing steps:\n",
    "\n",
    "- Removing punctuation\n",
    "- Lowercasing\n",
    "- Removing stopwords\n",
    "- Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can remove all punctuation, using the string python library, which contains punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuationfree = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "#storing the puntuation free text\n",
    "df_proc['processed_review']= df_proc['review'].apply(lambda x:remove_punctuation(x))\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can ensure all text is lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc['processed_review']= df_proc['processed_review'].apply(lambda x: x.lower())\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can remove stopwords that don't add any predictive power. The NLTK library consists of a list of words that are considered stopwords for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in (stopwords)])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc['processed_review']= df_proc['processed_review'].apply(lambda x:remove_stopwords(x))\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can carry out Lemmatisation to stem the words but ensure they maintain their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Defining the object for Lemmatisation\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    lemm_text = ' '. join([wordnet_lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return lemm_text\n",
    "\n",
    "df_proc['processed_review']=df_proc['processed_review'].apply(lambda x:lemmatizer(x))\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc.drop(columns=\"review\", inplace=True, axis=1)\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the transformer model\n",
    "\n",
    "Now we will run through the steps to train a hugging face Distilbert transformer using tensorflow. As mentioned previously, we won't actually train the model here, this has been done prior to the workshop, but we will load the trained model into the notebook so we can evaluate it here.\n",
    "\n",
    "First we split the data into train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_proc, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert to hugging face Datasets for ease in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.Dataset.from_pandas(train, preserve_index=False)\n",
    "test_ds = datasets.Dataset.from_pandas(test, preserve_index=False)\n",
    "comp_ds = datasets.DatasetDict({\"train\":train_ds,\"test\":test_ds})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tokenize the text using a pretrained tokenizer. We need to first instantiate the Distilbert tokenizer class of the library from a pre-trained model vocabulary. Then we create a preprocessing function to tokenize text, truncate sequences to be no longer than DistilBERT’s maximum input length, and pad the text to maximum length accepted by the model, so all inputs are a uniform length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(df):\n",
    "    return tokenizer(df[\"processed_review\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the hugging face Datasets map function to apply the preprocessing function over the entire dataset. We can speed up the map function by setting ```batched=True``` to process multiple elements of the dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_revs = comp_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialise a simple data collator creates batches of examples and returns tensors of the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fine-tune a model in TensorFlow, we then convert our train and test datasets to the tf.data.Dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = tokenized_revs[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tf_test_set = tokenized_revs[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load DistilBERT with ```TFAutoModelForSequenceClassification```, along with setting the number of expected labels, which in our case is 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run ```model.summary()``` to see the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first layer is the pretrained distilbert layer, so we can set ```trainable``` to ```False``` to save time during model training as we don't need to retrain this layer.\n",
    "\n",
    "Here we also set up an optimizer function, learning rate schedule and loss and accuracy metrics to monitor during training. We then compile the model to configure it for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we call ```fit``` to fine-tune the model. \n",
    "\n",
    "We will not run this and instead we will load a pre-trained model from a Google Storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```model.fit(tf_train_set, validation_data=tf_test_set, epochs=5)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "# mpl.rcParams['figure.figsize'] = (12, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_metrics(history):\n",
    "#     metrics = ['loss', 'accuracy']\n",
    "#     for n, metric in enumerate(metrics):\n",
    "#         name = metric.replace(\"_\",\" \").capitalize()\n",
    "#         plt.subplot(2,2,n+1)\n",
    "#         plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "#         plt.plot(history.epoch, history.history['val_'+metric],\n",
    "#                  color=colors[0], linestyle=\"--\", label='Val')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.ylabel(name)\n",
    "#         if metric == 'loss':\n",
    "#             plt.ylim([0, plt.ylim()[1]])\n",
    "#         elif metric == 'auc':\n",
    "#             plt.ylim([0.8,1])\n",
    "#         else:\n",
    "#             plt.ylim([0,1])\n",
    "\n",
    "#         plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_loss, int_accuracy = int_model.evaluate(int_test_ds[0], int_test_ds[1])\n",
    "\n",
    "# print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = int_model.predict(int_test_ds[0])\n",
    "# print(predictions[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_re = int_test_ds[1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_val_list = []\n",
    "# pred_val_list = []\n",
    "\n",
    "# for i in range(0,len(test_re)):\n",
    "#     test_val = test_re[i]\n",
    "#     test_val_list.append(test_val)\n",
    "#     pred_val = list(predictions[i]).index(predictions[i].max())\n",
    "#     pred_val_list.append(pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame(\n",
    "#         {'Test_val':test_val_list,\n",
    "#             'Pred_val':pred_val_list}\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results['match']='False'\n",
    "# df_results.loc[df_results['Test_val']==df_results['Pred_val'],'match'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_group = df_results.groupby(['Test_val','match']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results['Pred_val'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation if accuracy not provided as metric when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(tf_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[\"logits\"][3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_preds = []\n",
    "for i in y_preds[\"logits\"]:\n",
    "    rating_preds.append(np.argmax(i, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(rating_preds, test[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the previous sections the tweets are pre-processed using a variety of techniques. In order to account for this we have 2 options for how to account for the pre-processing logic in production:\n",
    "\n",
    "Custom Model: Incorporate the pre-processing directly in the predict method of a custom model. This provides simplicity when creating the deployment as there is only a single code base to worry about and a single component to be deployed.\n",
    "Input Transformer: Make use of a separate container to perform all of the input transformation and then pass the vectors to the model for prediction. The schematic below outlines how this would work.\n",
    "\n",
    "         ________________________________________\n",
    "         |            SeldonDeployment          |\n",
    "         |                                      |\n",
    "Request -->  Input transformer   -->     Model --> Response\n",
    "         |  (Pre-processing)          (SKLearn) |\n",
    "         |\n",
    "         __________________________________________\n",
    "    \n",
    "         \n",
    "The use of an input transformer allows us to separate the pre-processing logic from the prediction logic. This means we can leverage the pre-packaged SKLearn server provided by Seldon to serve our model, and each of the components can be upgraded independently of one another. However, it does introduce additional complexity in the deployment which is generated, and how that then interacts with advanced monitoring components such as outlier and drift detectors.\n",
    "This workshop will focus on the generation of a custom model for this case, therefore we need to define an __init__ and predict method which shall load and perform inference respectively in our new deployment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up \n",
    "\n",
    "We then define our Seldon custom model. The component parts required to build the custom model are outlined below. Each of the files play a key part in building the eventual Seldon docker container.\n",
    "\n",
    "---\n",
    "\n",
    "### ReviewRatings.py\n",
    "\n",
    "\n",
    "This is the critical file as it contains the logic associated with the deployment wrapped as part of a class by the same name as the Python file.\n",
    "\n",
    "A key thing to note about the way this has been structured is that we have focused on making this deployment reusable. The ```__init__``` method accepts a custom predictor parameter - the path to the saved model (```model_path```).\n",
    "\n",
    "The advantage of this is that it allows us to upgrade the model without having to re-build the container image. Additionally, if the logic was more general it could be used to accept a wider variety of objects for greater reusability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, DefaultDataCollator, TFAutoModelForSequenceClassification\n",
    "from google.cloud import storage\n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "Path(\"1\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ReviewRatings(object):\n",
    "    def __init__(self, model_path):\n",
    "        logger.info(\"Connecting to GCS\")\n",
    "        self.client = storage.Client.create_anonymous_client()\n",
    "        self.bucket = self.client.bucket('kelly-seldon')\n",
    "\n",
    "        logger.info(f\"Model name: {model_path}\")\n",
    "        self.model = None\n",
    "        self.prefix = model_path\n",
    "        self.local_dir = \"1/\"\n",
    "\n",
    "        logger.info(\"Loading tokenizer and data collator\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "        self.ready = False\n",
    "\n",
    "    def load_model(self):\n",
    "        logger.info(\"Getting model artifact from GCS\")\n",
    "        blobs = self.bucket.list_blobs(prefix=self.prefix)\n",
    "        for blob in blobs:\n",
    "            filename = blob.name.split('/')[-1]\n",
    "            blob.download_to_filename(self.local_dir + filename)\n",
    "        logger.info(\"Loading model\")\n",
    "        self.model = TFAutoModelForSequenceClassification.from_pretrained(\"1\", num_labels=9)\n",
    "\n",
    "    def preprocess_text(self, text, feature_names):\n",
    "        logger.info(\"Preprocessing text\")\n",
    "        logger.info(f\"Incoming text: {text}\")\n",
    "        dict_text = {\"review\": text}\n",
    "        df = pd.DataFrame(data=dict_text)\n",
    "        logger.info(f\"Dataframe created: {df}\")\n",
    "\n",
    "        dataset = datasets.Dataset.from_pandas(df, preserve_index=False)\n",
    "        logger.info(f\"Dataset created: {dataset}\")\n",
    "\n",
    "        tokenized_revs = dataset.map(self.tokenize, batched=True)\n",
    "        logger.info(f\"Tokenized reviews: {tokenized_revs}\")\n",
    "\n",
    "        logger.info(\"Converting tokenized reviews to tf dataset\")\n",
    "        tf_inf = tokenized_revs.to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\"],\n",
    "            label_cols=[\"labels\"],\n",
    "            shuffle=True,\n",
    "            batch_size=16,\n",
    "            collate_fn=self.data_collator\n",
    "        )\n",
    "        logger.info(f\"TF dataset created: {tf_inf}\")\n",
    "\n",
    "        return tf_inf\n",
    "\n",
    "    def tokenize(self, ds):\n",
    "    \n",
    "        return self.tokenizer(ds[\"review\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    def process_output(self, preds):\n",
    "        logger.info(\"Processing model predictions\")\n",
    "        rating_preds = []\n",
    "        for i in preds[\"logits\"]:\n",
    "            rating_preds.append(np.argmax(i, axis=0))\n",
    "\n",
    "        logger.info(\"Create output array for predictions\")\n",
    "        rating_preds = np.array(rating_preds)\n",
    "\n",
    "        return rating_preds\n",
    "\n",
    "    def process_whole(self, text):\n",
    "        tf_inf = self.preprocess_text(text, feature_names=None)\n",
    "        logger.info(\"Predictions ready to be made\")\n",
    "        preds = self.model.predict(tf_inf)\n",
    "        logger.info(f\"Prediction type: {type(preds)}\")\n",
    "        logger.info(f\"Predictions: {preds}\")\n",
    "        preds_proc = self.process_output(preds)\n",
    "        logger.info(f\"Processed predictions: {preds_proc}, Processed predictions type: {type(preds_proc)}\")\n",
    "\n",
    "        return preds_proc\n",
    "\n",
    "    def predict(self, text, names=[], meta=[]):\n",
    "        try:\n",
    "            if not self.ready:\n",
    "                self.load_model()\n",
    "                logger.info(\"Model successfully loaded\")\n",
    "                self.ready = True\n",
    "                logger.info(f\"{self.model.summary}\")\n",
    "                pred_proc = self.process_whole(text)\n",
    "            else:\n",
    "                pred_proc = self.process_whole(text)\n",
    "\n",
    "            return pred_proc\n",
    "\n",
    "        except Exception as ex:\n",
    "            logging.exception(f\"Failed during predict: {ex}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now deploy your model to the dedicated Seldon Deploy cluster which we have configured for this workshop. To do so you will interact with the Seldon Deploy SDK and deploy your model using that.\n",
    "\n",
    "First, setting up the configuration and authentication required to access the cluster. Make sure to fill in the `SD_IP` variable to be the same as the cluster you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD_IP = \"34.141.146.222\"\n",
    "\n",
    "config = Configuration()\n",
    "config.host = f\"http://{SD_IP}/seldon-deploy/api/v1alpha1\"\n",
    "config.oidc_client_id = \"sd-api\"\n",
    "config.oidc_server = f\"http://{SD_IP}/auth/realms/deploy-realm\"\n",
    "config.oidc_client_secret = \"sd-api-secret\"\n",
    "config.auth_method = \"client_credentials\"\n",
    "\n",
    "def auth():\n",
    "    auth = OIDCAuthenticator(config)\n",
    "    config.id_token = auth.authenticate()\n",
    "    api_client = ApiClient(configuration=config, authenticator=auth)\n",
    "    return api_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have configured the IP correctly as well as setup your authentication function you can describe the deployment you would like to create. \n",
    "\n",
    "You will need to fill in the `YOUR_NAME` variable. This MUST be lower case as it will be used for the `DEPLOYMENT_NAME` variable later on.\n",
    "\n",
    "The `MODEL_NAME` and `MODEL_PATH` variables have been prefilled for you as we are using a pretrained model and a pre-built container image for the sake of saving time in the workshop. \n",
    "\n",
    "The rest of the `mldeployment` description has been completed for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = \"kellyspry\"\n",
    "MODEL_NAME = \"review-ratings\"\n",
    "\n",
    "DEPLOYMENT_NAME = f\"{YOUR_NAME}-{MODEL_NAME}-test\"\n",
    "CONTAINER_NAME = f\"kellyspry0316/{MODEL_NAME}:0.9\"\n",
    "\n",
    "NAMESPACE = \"seldon-gitops\"\n",
    "\n",
    "CPU_REQUESTS = \"1\"\n",
    "MEMORY_REQUESTS = \"2Gi\"\n",
    "\n",
    "CPU_LIMITS = \"1\"\n",
    "MEMORY_LIMITS = \"2Gi\"\n",
    "\n",
    "MODEL_PATH = \"nlp-ratings/1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldeployment = {\n",
    "    \"kind\": \"SeldonDeployment\",\n",
    "    \"metadata\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"namespace\": NAMESPACE,\n",
    "        \"labels\": {\n",
    "            \"fluentd\": \"true\"\n",
    "        }\n",
    "    },\n",
    "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "    \"spec\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"annotations\": {\n",
    "            \"seldon.io/engine-seldon-log-messages-externally\": \"true\"\n",
    "        },\n",
    "        \"protocol\": \"seldon\",\n",
    "        \"predictors\": [\n",
    "            {\n",
    "                \"componentSpecs\": [\n",
    "                    {\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                                    \"image\": CONTAINER_NAME,\n",
    "                                    \"resources\": {\n",
    "                                        \"requests\": {\n",
    "                                            \"cpu\": CPU_REQUESTS,\n",
    "                                            \"memory\": MEMORY_REQUESTS\n",
    "                                        },\n",
    "                                        \"limits\": {\n",
    "                                            \"cpu\": CPU_LIMITS,\n",
    "                                            \"memory\": MEMORY_LIMITS\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": \"default\",\n",
    "                \"replicas\": 1,\n",
    "                \"traffic\": 100,\n",
    "                \"graph\": {\n",
    "                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                    \"parameters\": [\n",
    "                        {\n",
    "                            \"name\":\"model_path\",\n",
    "                            \"value\":MODEL_PATH,\n",
    "                            \"type\":\"STRING\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"children\": [],\n",
    "                    \"logger\": {\n",
    "                        \"mode\": \"all\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"status\": {}\n",
    "}\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now invoke the SeldonDeploymentsApi and create a new Seldon Deployment.\n",
    "\n",
    "Time for you to get your hands dirty. You will use the Seldon Deploy SDK to create a new Seldon deployment. You can find the reference documentation [here](https://github.com/SeldonIO/seldon-deploy-sdk/blob/master/python/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a good review:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "                \"ndarray\": [\n",
    "                    [\n",
    "                    \"This product is the best, it is so amazing. Incredible!\"\n",
    "                    ]\n",
    "                 ]\n",
    "           }\n",
    "}\n",
    "                        \n",
    "```\n",
    "\n",
    "\n",
    "Example of a bad review:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "                \"ndarray\": [\n",
    "                    [\n",
    "                    \"This product is the worst, it is absolutely terrible. Awful!\"\n",
    "                    ]\n",
    "                 ]\n",
    "           }\n",
    "}\n",
    "                        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the Seldon Deploy cluster and view your freshly created deployment here:\n",
    "\n",
    "### To do - add cluster IP in here when cluster is decided on\n",
    "\n",
    "* URL: http://34.141.146.222/seldon-deploy/\n",
    "* Username: admin@seldon.io\n",
    "* Password: 12341234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Prediction Schema and Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seldon Deploy has a model catalog where all deployed models are automatically registered. The model catalog can store custom metadata as well as prediction schemas for your models.\n",
    "\n",
    "Metadata promotes lineage from across different machine learning systems, aids knowledge transfer between teams, and allows for faster deployment. Meanwhile, prediction schemas allow Seldon Deploy to automatically profile tabular data into histograms, allowing for filtering on features to explore trends.\n",
    "\n",
    "In order to effectively construct a prediction schema Seldon has the ML Prediction Schema project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_schema = {\n",
    "    \"requests\": [\n",
    "        {\n",
    "            \"name\": \"Product Review\",\n",
    "            \"type\": \"TEXT\",\n",
    "        }\n",
    "    ],\n",
    "    \"responses\": [\n",
    "        {\n",
    "            \"name\": \"Rating\",\n",
    "            \"type\": \"CATEGORICAL\",\n",
    "            \"data_type\": \"INT\",\n",
    "            \"n_categories\": 9,\n",
    "            \"category_map\": {\n",
    "            \"0\": \"1.0\",\n",
    "            \"1\": \"1.5\",\n",
    "            \"2\": \"2.0\",\n",
    "            \"3\": \"2.5\",\n",
    "            \"4\": \"3.0\",\n",
    "            \"5\": \"3.5\",\n",
    "            \"6\": \"4.0\",\n",
    "            \"7\": \"4.5\",\n",
    "            \"8\": \"5.0\"\n",
    "        }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then add the prediction schema to the wider model catalog metadata. This includes information such as the model storage location, the name, who authored the model etc. The metadata tags and metrics which can be associated with a model are freeform and can therefore be determined based upon the use case which is being developed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In order to test your components you are able to send the requests directly using CURL/grpCURL or a similar utility, as well as by using our Python SeldonClient SDK.\n",
    "\n",
    "Testing options¶\n",
    "There are several options for testing your# To do if time - add accuracy as a metric in here once have a working model\n",
    "\n",
    "model_catalog_metadata = {\n",
    "      \"URI\": MODEL_LOCATION,\n",
    "      \"name\": f\"{DEPLOYMENT_NAME}-model\",\n",
    "      \"version\": \"v1.0\",\n",
    "      \"artifactType\": \"Tensorflow\",\n",
    "      \"taskType\": \"Product review rating classification\",\n",
    "      \"tags\": {\n",
    "        \"auto_created\": \"true\",\n",
    "        \"author\": f\"{YOUR_NAME}\"\n",
    "      },\n",
    "      \"metrics\": {\n",
    "          \"accuracy\": 0,\n",
    "      },\n",
    "      \"project\": \"default\",\n",
    "      \"prediction_schema\": prediction_schema\n",
    "    }\n",
    "\n",
    "model_catalog_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using the metadata API you can add this to the model which you have just created in Seldon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_api = ModelMetadataServiceApi(auth())\n",
    "metadata_api.model_metadata_service_update_model_metadata(model_catalog_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then list the metadata via the API, or view it in the UI, to confirm that it has been successfully added to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_response = metadata_api.model_metadata_service_list_model_metadata(uri=MODEL_LOCATION)\n",
    "metadata_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can have a go at sending some requests to your model using the 'Predict' tab in the UI. \n",
    "\n",
    "An example of a good review that we would expect to correspond to a higher rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "        \"names\": [\"Review\"],\n",
    "        \"ndarray\": [\"_product_ is excellent! I love it, it's great!\"]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "And an example of a negative review that we would expect to correspond to a lower rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "        \"names\": [\"Review\"],\n",
    "        \"ndarray\": [\"_product_ was terrible, I would not use it again, it was awful!\"]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although powerful, modern machine learning models can be sensitive. Seemingly subtle changes in a data distribution can destroy the performance of otherwise state-of-the art models, which can be especially problematic when ML models are deployed in production. Typically, ML models are tested on held out data in order to estimate their future performance. Crucially, this assumes that the process underlying the input data `X` and output data `Y` remains constant.\n",
    "\n",
    "Drift can be classified into the following types:\n",
    "* **Covariate drift**: Also referred to as input drift, this occurs when the distribution of the input data has shifted `P(X) != Pref(X)`, whilst `P(Y|X) = Pref(Y|X)`. This may result in the model giving unreliable predictions.\n",
    "\n",
    "* **Prior drift**: Also referred to as label drift, this occurs when the distribution of the outputs has shifted `P(Y) != Pref(Y)`, whilst `P(X|Y) = Pref(X|Y)`. This can affect the model’s decision boundary, as well as the model’s performance metrics.\n",
    "\n",
    "* **Concept drift**: This occurs when the process generating `Y` from `X` has changed, such that `P(Y|X) != Pref(Y|X)`. It is possible that the model might no longer give a suitable approximation of the true process.\n",
    "\n",
    "-----------------\n",
    "\n",
    "In this instance we will train a Kolmgorov-Smirnov drift detector to pick up on covariate drift. The KS Drift detector applies a two-sample KS test to compare the distance between the new probability distribution and the reference distribution. \n",
    "\n",
    "This is done on a feature by feature basis and the results are then aggregated using a correction, i.e. Bonferroni, to determine whether drift has occurred overall within the sample. \n",
    "\n",
    "We will use the training set as our reference distribution. Creating our drift detector is then as simple as writing a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_ref = [train[\"review\"][:3000].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.models.tensorflow import TransformerEmbedding\n",
    "\n",
    "emb_type = 'hidden_state'\n",
    "n_layers = 1\n",
    "layers = [-_ for _ in range(1, n_layers + 1)]\n",
    "\n",
    "embedding = TransformerEmbedding(\"distilbert-base-uncased\", emb_type, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(list(x_ref[0][:5]), padding=\"max_length\", return_tensors='tf')\n",
    "x_emb = embedding(tokens)\n",
    "print(x_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd.tensorflow import UAE\n",
    "\n",
    "enc_dim = 32\n",
    "shape = (x_emb.shape[1],)\n",
    "\n",
    "uae = UAE(input_layer=embedding, shape=shape, enc_dim=enc_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch_fn(text):\n",
    "        \n",
    "    text_list = text[0]\n",
    "    dict_text = {\"review\": text_list}\n",
    "    df = pd.DataFrame(data=dict_text)\n",
    "\n",
    "    len_df = len(df)\n",
    "    \n",
    "    dataset = datasets.Dataset.from_pandas(df, preserve_index=False)\n",
    "    \n",
    "    tokenized_revs = dataset.map(tokenize, batched=True)\n",
    "    \n",
    "    input_ids = tokenized_revs['input_ids']\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer(text[\"review\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_fn = partial(preprocess_drift, model=uae, preprocess_batch_fn=preprocess_batch_fn, max_len=512, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from alibi_detect.cd.tensorflow import preprocess_drift\n",
    "\n",
    "# define preprocessing function\n",
    "preprocess_fn = partial(preprocess_drift, model=uae, tokenizer=tokenizer, max_len=512, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = KSDrift(x_ref, p_val=.05, preprocess_fn=preprocess_fn, input_shape=(512,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_0 = x_ref[:600]\n",
    "batch_1 = [x_ref[0]] * 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cd = cd.predict(batch_0[:200])\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_cd['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_cd['data']['p_val']))\n",
    "print('Drift Distance: {}'.format(preds_cd['data']['distance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cd = cd.predict(batch_1[:200])\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_cd['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_cd['data']['p_val']))\n",
    "print('Drift Distance: {}'.format(preds_cd['data']['distance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cd = cd.predict(train[\"review\"][:3000])\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_cd['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_cd['data']['p_val']))\n",
    "print('Drift Distance: {}'.format(preds_cd['data']['distance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cd = cd.predict(train[\"review\"][:1000])\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_cd['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_cd['data']['p_val']))\n",
    "print('Drift Distance: {}'.format(preds_cd['data']['distance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cd = cd.predict(x_ref[5])\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_cd['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_cd['data']['p_val']))\n",
    "print('Drift Distance: {}'.format(preds_cd['data']['distance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_detector(cd, \"reviews-ks-dd-arr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r reviews-ks-dd-arr gs://kelly-seldon/nlp-ratings/dd/kellyspry/reviews-ks-dd-arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Drift Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the drift detector you will use Seldon Deploy's user interface. Simply navigate to your deployment, and select the \"Create\" button for your drift detector.\n",
    "\n",
    "This will bring up a form. Add your ```Detector Name```, the ```Storage URI```:\n",
    "\n",
    "\n",
    "* ```gs://kelly-seldon/nlp-ratings/dd/{YOUR_NAME}/reviews-drift-detector/```\n",
    "\n",
    "\n",
    "and the ```Batch Size```:\n",
    "\n",
    "* ```200```\n",
    "\n",
    "The batch size configuration sets how many data points have to be sent to the endpoint before drift is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Batch Job\n",
    "\n",
    "The simplest way to test that your drift detector and prediction schema are behaving correctly is to kick off a batch job. \n",
    "\n",
    "There is already a pre-prepared batch of data stored in a Minio bucket which Seldon can access, therefore all you have to do is provide the configuration which is outlined below. This shows all of the available parameters which can be changed, but not all are required and are shown for educational purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_0 = x_ref[:2000]\n",
    "batch_1 = [x_ref[0]] * 2000\n",
    "\n",
    "# batch_final = batch_0 + batch_1\n",
    "\n",
    "# textfile = open(\"small_batch.txt\", \"w\")\n",
    "# for element in batch_final:\n",
    "#     textfile.write(f'[[\"{element}\"]]' + \"\\n\")\n",
    "# textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Tom about batch_gateway_endpoint - is this the name of the cluster? is this arbituary? \n",
    "# Should this be hard coded or changed to match peoples model names in the workshop\n",
    "\n",
    "workflow = BatchJobDefinition(\n",
    "    batch_data_type='data',\n",
    "    batch_gateway_type='seldon',\n",
    "    batch_interval=0.0,\n",
    "    batch_method='predict',\n",
    "    batch_payload_type='ndarray',\n",
    "    batch_retries='3',\n",
    "    batch_size=0,\n",
    "    batch_transport_protocol='rest',\n",
    "    batch_workers='1',\n",
    "    input_data='minio://gartner-workshop/batch_data.txt',\n",
    "    object_store_secret_name='minio-bucket',\n",
    "    output_data='minio://gartner-workshop/batch_data_output.txt',\n",
    "    pvc_size='1Gi'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then pass the above configuration to the batch job API and kick it off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_api = BatchJobsApi(auth())\n",
    "batch_api.create_seldon_deployment_batch_job(DEPLOYMENT_NAME, NAMESPACE, workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will train an explainer to glean deeper insights into the decisions being made by your model. \n",
    "\n",
    "You will make use of the Anchors algorithm, which has a [production grade implementation available](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html) using the Seldon Alibi Explain library. \n",
    "\n",
    "The first step will be to write a simple prediction function which the explainer can call in order to query your Tensorflow model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(ds):\n",
    "    return tokenizer(ds[\"review\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(x):\n",
    "    \n",
    "    dict_text = {\"review\": x}\n",
    "    df = pd.DataFrame(data=dict_text)\n",
    "    len_df = len(df)\n",
    "    dataset = datasets.Dataset.from_pandas(df, preserve_index=False)\n",
    "    tokenized_revs = dataset.map(tokenize, batched=True)\n",
    "    tf_inf = tokenized_revs.to_tf_dataset(\n",
    "        columns=[\"attention_mask\", \"input_ids\"],\n",
    "        label_cols=[\"labels\"],\n",
    "        shuffle=True,\n",
    "        batch_size=len_df,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    preds = model.predict(tf_inf)\n",
    "        \n",
    "    rating_preds = []\n",
    "    for i in preds[\"logits\"]:\n",
    "        rating_preds.append(np.argmax(i, axis=0))\n",
    "        \n",
    "    rating_preds = np.array(rating_preds)\n",
    "    \n",
    "    return rating_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then initialise your Anchor explainer, using the AnchorText flavour provided by Alibi due to your data modality. \n",
    "\n",
    "### This text might need changing based on what we use as the language model for this. One to discuss with Tom.\n",
    "\n",
    "The AnchorText class expects the prediction function which you defined above, as well as a sampling strategy and nlp/language model. You can find a sample notebook in the Alibi docs [here]https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_text_movie.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load transformer - this may take a couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"1\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "client = storage.Client.create_anonymous_client()\n",
    "bucket = client.bucket('kelly-seldon')\n",
    "local_dir = \"1/\"\n",
    "\n",
    "blobs = bucket.list_blobs(prefix=\"nlp-ratings/1/\")\n",
    "for blob in blobs:\n",
    "    filename = blob.name.split('/')[-1]\n",
    "    blob.download_to_filename(local_dir + filename)\n",
    "    \n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"1\", num_labels=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model = 'en_core_web_md'\n",
    "spacy_model(model=nlp_model)\n",
    "nlp = spacy.load(nlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(\n",
    "    predictor=predict_fn,\n",
    "    sampling_strategy='similarity',   # Replace masked words by similar words\n",
    "    nlp=nlp,                          # Spacy object\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then run an explanation for a given review locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This product is great, I love it!\"\n",
    "\n",
    "explanation = explainer.explain(text, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('Coverage: %.2f' % explanation.coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now save your explainer, and upload it to the GS bucket. You can use the explainer's built-in save method to do this easily and reproducibly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.save(\"review-explainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r reviews-explainer gs://kelly-seldon/nlp-ratings/models/{YOUR_NAME}/reviews-explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now deploy our explainer alongside our model. First you define the explainer configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLAINER_TYPE = \"AnchorText\"\n",
    "EXPLAINER_URI = f\"gs://kelly-seldon/nlp-ratings/models/{YOUR_NAME}/reviews-explainer\"\n",
    "\n",
    "explainer_spec = {\n",
    "                    \"type\": EXPLAINER_TYPE,\n",
    "                    \"modelUri\": EXPLAINER_URI,\n",
    "                    \"containerSpec\": {\n",
    "                        \"name\": \"\",\n",
    "                        \"resources\": {}\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then insert this additional configuration into your original `mldeployment` specification which you defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldeployment['spec']['predictors'][0]['explainer'] = explainer_spec\n",
    "mldeployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then deploy the explainer to the Seldon Deploy cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then run the same predictions through your model as above and generate explanations on them:\n",
    "\n",
    "An example of a good review that we would expect to correspond to a higher rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "        \"names\": [\"Review\"],\n",
    "        \"ndarray\": [\"_product_ is excellent! I love it, it's great!\"]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "And an example of a negative review that we would expect to correspond to a lower rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "        \"names\": [\"Review\"],\n",
    "        \"ndarray\": [\"_product_ was terrible, I would not use it again, it was awful!\"]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-ratings-new] *",
   "language": "python",
   "name": "conda-env-nlp-ratings-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
