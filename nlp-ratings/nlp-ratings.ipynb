{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Workshop - *Predicting product ratings from reviews*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop is focused on the creation, deployment, monitoring and management of a machine learning model for predicting product ratings from reviews.\n",
    "\n",
    "### Tom - this isn't completely relevant anymore, will change it before workshop to talk about pretrained model being used etc, so just ignore for now\n",
    "\n",
    "In this notebook we will be exploring the data, and training the machine learning model itself; in the form of a Tensorflow Keras model. We will then deploy a second Tensorflow model with differing architecture as a Canary model to demonstrate the A/B testing functionality Seldon provides. Then we will begin to add the advanced monitoring and explainability that Seldon Alibi is famed for. \n",
    "\n",
    "-----------------------------------\n",
    "Firstly, we will install and import the relevant packages which we will use throughout the exploration, training, and deployment process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd import KSDrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try another LSTM method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/seldon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>_product_ provided me with a pretty good, sec...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>it protects our files and computer and very si...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>like most businesses, we are always looking fo...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>we believed _product_ was a great solution for...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>i formerly used _product_ and was relieved to ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rating                                             review  label\n",
       "0    5.0   _product_ provided me with a pretty good, sec...      8\n",
       "1    4.0  it protects our files and computer and very si...      6\n",
       "2    5.0  like most businesses, we are always looking fo...      8\n",
       "3    3.0  we believed _product_ was a great solution for...      4\n",
       "4    4.0  i formerly used _product_ and was relieved to ...      6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"review_data.csv\", delimiter=\";\")\n",
    "\n",
    "df['review'] = df['review'].astype(str)\n",
    "df['rating'] = df['rating'].astype(str)\n",
    "\n",
    "rating_mapping = {\n",
    "    '1.0': 0,\n",
    "    '1.5': 1,\n",
    "    '2.0': 2,\n",
    "    '2.5': 3,\n",
    "    '3.0': 4,\n",
    "    '3.5': 5,\n",
    "    '4.0': 6, \n",
    "    '4.5': 7,\n",
    "    '5.0': 8\n",
    "}\n",
    "\n",
    "df['label'] = df['rating'].apply(lambda x: rating_mapping[x])\n",
    "df.drop(columns=['product', 'user_id', 'date_created'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "for row in df.iterrows():\n",
    "    labels.append(row[1]['label'])\n",
    "    review = row[1]['review']\n",
    "    for word in STOPWORDS:\n",
    "        token = ' ' + word + ' '\n",
    "        review = review.replace(token, ' ')\n",
    "        review = review.replace(' ', ' ')\n",
    "    reviews.append(review)\n",
    "print(len(labels))\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 400\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "8    31721\n",
       "6    14017\n",
       "4     2645\n",
       "0      763\n",
       "2      625\n",
       "7      150\n",
       "3       27\n",
       "5       27\n",
       "1       25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(df['label']),\n",
    "                                                  y=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "\n",
    "for index,value in enumerate(class_weights):\n",
    "    weights[index] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 7.2811999417504,\n",
       " 1: 222.22222222222223,\n",
       " 2: 8.88888888888889,\n",
       " 3: 205.76131687242798,\n",
       " 4: 2.1003990758244067,\n",
       " 5: 205.76131687242798,\n",
       " 6: 0.3963441218203293,\n",
       " 7: 37.03703703703704,\n",
       " 8: 0.17513809638900274}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "40000\n",
      "40000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(reviews) * training_portion)\n",
    "\n",
    "train_articles = reviews[0: train_size]\n",
    "train_labels = labels[0: train_size]\n",
    "\n",
    "validation_articles = reviews[train_size:]\n",
    "validation_labels = labels[train_size:]\n",
    "\n",
    "print(train_size)\n",
    "print(len(train_articles))\n",
    "print(len(train_labels))\n",
    "print(len(validation_articles))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'product': 2,\n",
       " 'use': 3,\n",
       " 'i': 4,\n",
       " 'great': 5,\n",
       " 'software': 6,\n",
       " 'easy': 7,\n",
       " 'it': 8,\n",
       " 'using': 9,\n",
       " 'overall': 10}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index\n",
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 45, 430, 959, 2, 725, 11, 544, 1014, 44, 99, 128, 2, 2087]\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
    "print(train_sequences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "400\n",
      "7\n",
      "400\n",
      "14\n",
      "400\n",
      "10000\n",
      "(10000, 400)\n"
     ]
    }
   ],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(len(train_sequences[0]))\n",
    "print(len(train_padded[0]))\n",
    "\n",
    "print(len(train_sequences[1]))\n",
    "print(len(train_padded[1]))\n",
    "\n",
    "print(len(train_sequences[10]))\n",
    "print(len(train_padded[10]))\n",
    "\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(len(validation_sequences))\n",
    "print(validation_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_label_seq = np.array(train_labels)\n",
    "validation_label_seq = np.array(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          320000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 394,889\n",
      "Trainable params: 394,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    # Add a Dense layer with 9 units and softmax activation.\n",
    "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "    tf.keras.layers.Dense(9, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 26250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ab24ba7c120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "num_epochs = 10\n",
    "batch_num = 2048\n",
    "history = model.fit(encoded_dataset['train']['input_ids'], encoded_dataset['train']['labels'], epochs=num_epochs, batch_size= batch_num, validation_data=(encoded_dataset['val']['input_ids'], encoded_dataset['train']['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method using DistilBERT\n",
    "\n",
    "### First part using pytorch example \n",
    "### Second part using tensorflow example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seldon/anaconda3/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/home/seldon/anaconda3/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: alibi_detect==0.8.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: tensorflow-probability<0.13.0,>=0.8.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (0.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (2.28.0)\n",
      "Requirement already satisfied: pandas<2.0.0,>=0.23.3 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (1.3.5)\n",
      "Requirement already satisfied: dill<0.4.0,>=0.3.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (0.3.4)\n",
      "Requirement already satisfied: numba!=0.54.0,<0.56.0,>=0.50.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (0.55.2)\n",
      "Requirement already satisfied: scikit-image!=0.17.1,<0.19,>=0.14.2 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (0.18.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.0.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (4.19.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.28.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (4.64.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.2 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn<1.1.0,>=0.20.2 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (1.0.2)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (3.5.2)\n",
      "Requirement already satisfied: Pillow<9.0.0,>=5.4.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (8.4.0)\n",
      "Requirement already satisfied: opencv-python<5.0.0,>=3.2.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (4.6.0.66)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.3.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (1.7.3)\n",
      "Requirement already satisfied: tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from alibi_detect==0.8.1) (2.7.3)\n",
      "Requirement already satisfied: dm-tree in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi_detect==0.8.1) (0.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi_detect==0.8.1) (2.1.0)\n",
      "Requirement already satisfied: decorator in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi_detect==0.8.1) (5.1.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi_detect==0.8.1) (0.4.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi_detect==0.8.1) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/seldon/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.21.0->alibi_detect==0.8.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/seldon/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.21.0->alibi_detect==0.8.1) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.21.0->alibi_detect==0.8.1) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.21.0->alibi_detect==0.8.1) (2.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/seldon/anaconda3/lib/python3.7/site-packages (from pandas<2.0.0,>=0.23.3->alibi_detect==0.8.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/seldon/anaconda3/lib/python3.7/site-packages (from pandas<2.0.0,>=0.23.3->alibi_detect==0.8.1) (2022.1)\n",
      "Requirement already satisfied: setuptools in /home/seldon/anaconda3/lib/python3.7/site-packages (from numba!=0.54.0,<0.56.0,>=0.50.0->alibi_detect==0.8.1) (62.4.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from numba!=0.54.0,<0.56.0,>=0.50.0->alibi_detect==0.8.1) (0.38.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi_detect==0.8.1) (2.6.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/seldon/anaconda3/lib/python3.7/site-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi_detect==0.8.1) (2021.11.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi_detect==0.8.1) (2.19.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi_detect==0.8.1) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.0.0->alibi_detect==0.8.1) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.0.0->alibi_detect==0.8.1) (0.12.1)\n",
      "Requirement already satisfied: filelock in /home/seldon/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.0.0->alibi_detect==0.8.1) (3.7.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/seldon/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.0.0->alibi_detect==0.8.1) (2022.6.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.0.0->alibi_detect==0.8.1) (0.7.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/seldon/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.0.0->alibi_detect==0.8.1) (4.11.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.0.0->alibi_detect==0.8.1) (6.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/seldon/anaconda3/lib/python3.7/site-packages (from scikit-learn<1.1.0,>=0.20.2->alibi_detect==0.8.1) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from scikit-learn<1.1.0,>=0.20.2->alibi_detect==0.8.1) (3.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi_detect==0.8.1) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/seldon/anaconda3/lib/python3.7/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi_detect==0.8.1) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi_detect==0.8.1) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi_detect==0.8.1) (4.33.3)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (0.37.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (0.26.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (1.6.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (3.19.4)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (14.0.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (3.3.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (2.9.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (1.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wrapt>=1.11.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (4.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (1.1.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (2.7.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (3.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (1.46.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (2.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/seldon/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.0.0->alibi_detect==0.8.1) (3.8.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (2.8.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (2.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/seldon/anaconda3/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/seldon/anaconda3/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/seldon/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/seldon/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow!=2.6.0,!=2.6.1,<2.8.0,>=2.2.0->alibi_detect==0.8.1) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Install those not pre-installed on Google colab\n",
    "\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install seldon_deploy_sdk\n",
    "!pip install alibi==0.6.4\n",
    "!pip install alibi_detect==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from transformers import TFDistilBertForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TFAutoModelForSequenceClassification, create_optimizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "from seldon_deploy_sdk import Configuration, ApiClient, SeldonDeploymentsApi, ModelMetadataServiceApi, DriftDetectorApi, BatchJobsApi, BatchJobDefinition\n",
    "from seldon_deploy_sdk.auth import OIDCAuthenticator\n",
    "\n",
    "from alibi.explainers import AnchorTabular\n",
    "from alibi_detect.cd import MMDDrift\n",
    "from alibi_detect.utils.saving import save_detector, load_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://kelly-seldon/nlp-ratings/review_data.csv...\n",
      "/ [1 files][ 11.3 MiB/ 11.3 MiB]                                                \n",
      "Operation completed over 1 objects/11.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://kelly-seldon/nlp-ratings/review_data.csv review_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_product_ provided me with a pretty good, sec...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it protects our files and computer and very si...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like most businesses, we are always looking fo...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we believed _product_ was a great solution for...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i formerly used _product_ and was relieved to ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0   _product_ provided me with a pretty good, sec...      8\n",
       "1  it protects our files and computer and very si...      6\n",
       "2  like most businesses, we are always looking fo...      8\n",
       "3  we believed _product_ was a great solution for...      4\n",
       "4  i formerly used _product_ and was relieved to ...      6"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"review_data.csv\", delimiter=\";\")\n",
    "\n",
    "df['review'] = df['review'].astype(str)\n",
    "df['rating'] = df['rating'].astype(str)\n",
    "\n",
    "rating_mapping = {\n",
    "    '1.0': 0,\n",
    "    '1.5': 1,\n",
    "    '2.0': 2,\n",
    "    '2.5': 3,\n",
    "    '3.0': 4,\n",
    "    '3.5': 5,\n",
    "    '4.0': 6, \n",
    "    '4.5': 7,\n",
    "    '5.0': 8\n",
    "}\n",
    "\n",
    "df['label'] = df['rating'].apply(lambda x: rating_mapping[x])\n",
    "df.drop(columns=['product', 'user_id', 'date_created', 'rating'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = pd.get_dummies(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whole = df.join(df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_product_ provided me with a pretty good, sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it protects our files and computer and very si...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like most businesses, we are always looking fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we believed _product_ was a great solution for...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i formerly used _product_ and was relieved to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>this is the best form i have even come across ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>average, not bad but not amazing either. not s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>great</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i used act! for years until that became slow a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>_product_ is still used every single day by u...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  0  1  2  3  4  5  6  \\\n",
       "0       _product_ provided me with a pretty good, sec...  0  0  0  0  0  0  0   \n",
       "1      it protects our files and computer and very si...  0  0  0  0  0  0  1   \n",
       "2      like most businesses, we are always looking fo...  0  0  0  0  0  0  0   \n",
       "3      we believed _product_ was a great solution for...  0  0  0  0  1  0  0   \n",
       "4      i formerly used _product_ and was relieved to ...  0  0  0  0  0  0  1   \n",
       "...                                                  ... .. .. .. .. .. .. ..   \n",
       "49995  this is the best form i have even come across ...  0  0  0  0  0  0  0   \n",
       "49996  average, not bad but not amazing either. not s...  0  0  0  0  0  0  0   \n",
       "49997                                             great   0  0  0  0  0  0  0   \n",
       "49998  i used act! for years until that became slow a...  0  0  0  0  1  0  0   \n",
       "49999   _product_ is still used every single day by u...  0  0  0  0  0  0  1   \n",
       "\n",
       "       7  8  \n",
       "0      0  1  \n",
       "1      0  0  \n",
       "2      0  1  \n",
       "3      0  0  \n",
       "4      0  0  \n",
       "...   .. ..  \n",
       "49995  0  1  \n",
       "49996  0  1  \n",
       "49997  0  1  \n",
       "49998  0  0  \n",
       "49999  0  0  \n",
       "\n",
       "[50000 rows x 10 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tom - I'm worried that the labels are so skewed the model won't be able to predict the lower ratings/.5 ratings well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tom - have validation split in here too, but don't think I need that if not fine-tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_whole, test_size=0.3, random_state=42)\n",
    "\n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33553</th>\n",
       "      <td>certainly that it is a big tool to everybody t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>in base on my experience, a _product_ is a sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>great software for designing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>using _product_ allows to optimize industrial ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39489</th>\n",
       "      <td>solid cloud storage option with the ability to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15168</th>\n",
       "      <td>very good! customer support responded quickly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49241</th>\n",
       "      <td>i think that this is a pretty cool software! i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39317</th>\n",
       "      <td>definitely the standard statistical package in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42191</th>\n",
       "      <td>have been using it like during my school and c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15109</th>\n",
       "      <td>i would love to find a new _product_ for our f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  0  1  2  3  4  5  6  \\\n",
       "33553  certainly that it is a big tool to everybody t...  0  0  1  0  0  0  0   \n",
       "9427   in base on my experience, a _product_ is a sec...  0  0  0  0  0  0  1   \n",
       "199                        great software for designing   0  0  0  0  0  0  0   \n",
       "12447  using _product_ allows to optimize industrial ...  0  0  0  0  0  0  0   \n",
       "39489  solid cloud storage option with the ability to...  0  0  0  0  0  0  0   \n",
       "...                                                  ... .. .. .. .. .. .. ..   \n",
       "15168  very good! customer support responded quickly ...  0  0  0  0  0  0  0   \n",
       "49241  i think that this is a pretty cool software! i...  0  0  0  0  0  0  0   \n",
       "39317  definitely the standard statistical package in...  0  0  0  0  0  0  0   \n",
       "42191  have been using it like during my school and c...  0  0  0  0  0  0  0   \n",
       "15109  i would love to find a new _product_ for our f...  0  0  0  0  1  0  0   \n",
       "\n",
       "       7  8  \n",
       "33553  0  0  \n",
       "9427   0  0  \n",
       "199    0  1  \n",
       "12447  0  1  \n",
       "39489  0  1  \n",
       "...   .. ..  \n",
       "15168  0  1  \n",
       "49241  0  1  \n",
       "39317  0  1  \n",
       "42191  0  1  \n",
       "15109  0  0  \n",
       "\n",
       "[15000 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.Dataset.from_pandas(train, preserve_index=False)\n",
    "test_ds = datasets.Dataset.from_pandas(test, preserve_index=False)\n",
    "val_ds = datasets.Dataset.from_pandas(val, preserve_index=False)\n",
    "comp_ds = datasets.DatasetDict({\"train\":train_ds,\"test\":test_ds, \"val\":val_ds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', '0', '1', '2', '3', '4', '5', '6', '7', '8'],\n",
       "        num_rows: 26250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', '0', '1', '2', '3', '4', '5', '6', '7', '8'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['review', '0', '1', '2', '3', '4', '5', '6', '7', '8'],\n",
       "        num_rows: 8750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in comp_ds['train'].features.keys() if label not in ['review']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"review\"]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((1,len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding[\"labels\"] = labels_matrix[0].tolist()\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4ab397c8f14a9ca824ff48d2ec54c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26250 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573199a980304125896d3978caff5b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd119595e691479e8924e7437b905069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8750 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = comp_ds.map(preprocess_data, remove_columns=comp_ds['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 26250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][2]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_dataset.set_format(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_train_set = encoded_dataset[\"train\"].to_tf_dataset(\n",
    "#     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "#     shuffle=True,\n",
    "#     batch_size=16,\n",
    "#     collate_fn=data_collator,\n",
    "# )\n",
    "\n",
    "# tf_test_set = encoded_dataset[\"test\"].to_tf_dataset(\n",
    "#     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "#     shuffle=False,\n",
    "#     batch_size=16,\n",
    "#     collate_fn=data_collator,\n",
    "# )\n",
    "\n",
    "# tf_val_set = encoded_dataset[\"val\"].to_tf_dataset(\n",
    "#     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "#     shuffle=False,\n",
    "#     batch_size=16,\n",
    "#     collate_fn=data_collator,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/seldon/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\",\n",
      "    \"2\": \"2\",\n",
      "    \"3\": \"3\",\n",
      "    \"4\": \"4\",\n",
      "    \"5\": \"5\",\n",
      "    \"6\": \"6\",\n",
      "    \"7\": \"7\",\n",
      "    \"8\": \"8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"2\": 2,\n",
      "    \"3\": 3,\n",
      "    \"4\": 4,\n",
      "    \"5\": 5,\n",
      "    \"6\": 6,\n",
      "    \"7\": 7,\n",
      "    \"8\": 8\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/seldon/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"distil-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seldon/anaconda3/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/home/seldon/anaconda3/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: torch in /home/seldon/anaconda3/lib/python3.7/site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /home/seldon/anaconda3/lib/python3.7/site-packages (from torch) (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6851, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[ 0.0677, -0.1175,  0.0105,  0.0166, -0.0122, -0.0382,  0.0799, -0.0883,\n",
       "          0.0854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 26250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16410\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16410' max='16410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16410/16410 6:21:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.161395</td>\n",
       "      <td>0.649046</td>\n",
       "      <td>0.791929</td>\n",
       "      <td>0.620229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.160734</td>\n",
       "      <td>0.641696</td>\n",
       "      <td>0.783164</td>\n",
       "      <td>0.600114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.134400</td>\n",
       "      <td>0.176416</td>\n",
       "      <td>0.628970</td>\n",
       "      <td>0.783543</td>\n",
       "      <td>0.607543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.204808</td>\n",
       "      <td>0.613543</td>\n",
       "      <td>0.778907</td>\n",
       "      <td>0.600914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.234696</td>\n",
       "      <td>0.601756</td>\n",
       "      <td>0.772364</td>\n",
       "      <td>0.590629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distil-finetuned-sem_eval-english/checkpoint-3282\n",
      "Configuration saved in distil-finetuned-sem_eval-english/checkpoint-3282/config.json\n",
      "Model weights saved in distil-finetuned-sem_eval-english/checkpoint-3282/pytorch_model.bin\n",
      "tokenizer config file saved in distil-finetuned-sem_eval-english/checkpoint-3282/tokenizer_config.json\n",
      "Special tokens file saved in distil-finetuned-sem_eval-english/checkpoint-3282/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distil-finetuned-sem_eval-english/checkpoint-6564\n",
      "Configuration saved in distil-finetuned-sem_eval-english/checkpoint-6564/config.json\n",
      "Model weights saved in distil-finetuned-sem_eval-english/checkpoint-6564/pytorch_model.bin\n",
      "tokenizer config file saved in distil-finetuned-sem_eval-english/checkpoint-6564/tokenizer_config.json\n",
      "Special tokens file saved in distil-finetuned-sem_eval-english/checkpoint-6564/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distil-finetuned-sem_eval-english/checkpoint-9846\n",
      "Configuration saved in distil-finetuned-sem_eval-english/checkpoint-9846/config.json\n",
      "Model weights saved in distil-finetuned-sem_eval-english/checkpoint-9846/pytorch_model.bin\n",
      "tokenizer config file saved in distil-finetuned-sem_eval-english/checkpoint-9846/tokenizer_config.json\n",
      "Special tokens file saved in distil-finetuned-sem_eval-english/checkpoint-9846/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distil-finetuned-sem_eval-english/checkpoint-13128\n",
      "Configuration saved in distil-finetuned-sem_eval-english/checkpoint-13128/config.json\n",
      "Model weights saved in distil-finetuned-sem_eval-english/checkpoint-13128/pytorch_model.bin\n",
      "tokenizer config file saved in distil-finetuned-sem_eval-english/checkpoint-13128/tokenizer_config.json\n",
      "Special tokens file saved in distil-finetuned-sem_eval-english/checkpoint-13128/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8750\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distil-finetuned-sem_eval-english/checkpoint-16410\n",
      "Configuration saved in distil-finetuned-sem_eval-english/checkpoint-16410/config.json\n",
      "Model weights saved in distil-finetuned-sem_eval-english/checkpoint-16410/pytorch_model.bin\n",
      "tokenizer config file saved in distil-finetuned-sem_eval-english/checkpoint-16410/tokenizer_config.json\n",
      "Special tokens file saved in distil-finetuned-sem_eval-english/checkpoint-16410/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distil-finetuned-sem_eval-english/checkpoint-3282 (score: 0.6490462237636789).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16410, training_loss=0.13285895352825605, metrics={'train_runtime': 22914.8583, 'train_samples_per_second': 5.728, 'train_steps_per_second': 0.716, 'total_flos': 4347129124800000.0, 'train_loss': 0.13285895352825605, 'epoch': 5.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tom - all of this following now is from following the article https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERTâ€™s maximum input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['review'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-a4a18a9ca69d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_review'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocess_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test['token_review'] = test['review'].apply(lambda x: preprocess_function(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'label'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2005719bd3ee4b5fa96f7257b8ed475d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_revs = comp_ds['train'].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 35000\n",
       "})"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_revs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['review', 'label', 'input_ids', 'attention_mask']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-eb43c192d131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tf_train_set = tokenized_revs[\"train\"].to_tf_dataset(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2123\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m         return self._getitem(\n\u001b[0;32m-> 2125\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2126\u001b[0m         )\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   2106\u001b[0m         \u001b[0mformat_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2108\u001b[0;31m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2109\u001b[0m         formatted_output = format_table(\n\u001b[1;32m   2110\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0m_raise_bad_key_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0m_check_valid_column_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_valid_column_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column {key} not in the dataset. Current columns in the dataset: {columns}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['review', 'label', 'input_ids', 'attention_mask']\""
     ]
    }
   ],
   "source": [
    "tf_train_set = tokenized_revs[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = tokenized_revs[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tokenized_revs['train']) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tom - How does it know to map the labels to the correct labels in the reviews data? Without seeing the training data? There's surely a step missing here that needs adding in if not doing the fine tuning part? Because at the moment is it not unsupervised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test['token_review'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is input_ids the correct thing to be passing here for predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[ 0.04180664,  0.02156035,  0.01319884, -0.05503405,  0.00553667,\n",
       "         0.05655766,  0.00432652,  0.00907688,  0.00125564],\n",
       "       [ 0.00953853, -0.00879546,  0.01612893, -0.05012612,  0.00059584,\n",
       "         0.03864627,  0.08955497,  0.06449739, -0.11617592]],\n",
       "      dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04180664,  0.02156035,  0.01319884, -0.05503405,  0.00553667,\n",
       "         0.05655766,  0.00432652,  0.00907688,  0.00125564],\n",
       "       [ 0.00953853, -0.00879546,  0.01612893, -0.05012612,  0.00059584,\n",
       "         0.03864627,  0.08955497,  0.06449739, -0.11617592]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tom - Can ignore this - another example I found online of fine-tuning DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_39', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=9)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "# model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\", line 996, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"tf_distil_bert_for_sequence_classification_1\" (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\", line 730, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 742, in call  *\n            distilbert_output = self.distilbert(\n        File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        TypeError: Exception encountered when calling layer \"distilbert\" (type TFDistilBertMainLayer).\n        \n        in user code:\n        \n            File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\", line 730, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 400, in call  *\n                embedding_output = self.embeddings(input_ids, inputs_embeds=inputs_embeds)  # (bs, seq_length, dim)\n            File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n        \n            TypeError: Exception encountered when calling layer \"embeddings\" (type TFEmbeddings).\n            \n            in user code:\n            \n                File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 122, in call  *\n                    final_embeddings = self.LayerNorm(inputs=final_embeddings)\n                File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n            \n                TypeError: Exception encountered when calling layer \"LayerNorm\" (type LayerNormalization).\n                \n                Failed to convert elements of [1, 1, None, 1] to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n                \n                Call arguments received:\n                  â€¢ inputs=tf.Tensor(shape=(None, 16, None, 768), dtype=float32)\n            \n            \n            Call arguments received:\n              â€¢ input_ids=tf.Tensor(shape=(None, 16, None), dtype=int64)\n              â€¢ position_ids=None\n              â€¢ inputs_embeds=None\n              â€¢ training=True\n        \n        \n        Call arguments received:\n          â€¢ self=tf.Tensor(shape=(None, 16, None), dtype=int64)\n          â€¢ input_ids=None\n          â€¢ attention_mask=tf.Tensor(shape=(None, 16, None), dtype=int64)\n          â€¢ head_mask=None\n          â€¢ inputs_embeds=None\n          â€¢ output_attentions=False\n          â€¢ output_hidden_states=False\n          â€¢ return_dict=True\n          â€¢ training=True\n    \n    \n    Call arguments received:\n      â€¢ self={'input_ids': 'tf.Tensor(shape=(None, 16, None), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(None, 16, None), dtype=int64)', 'labels': 'tf.Tensor(shape=(None, 16), dtype=int64)'}\n      â€¢ input_ids=None\n      â€¢ attention_mask=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ output_attentions=None\n      â€¢ output_hidden_states=None\n      â€¢ return_dict=None\n      â€¢ labels=None\n      â€¢ training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-bd4aa36be170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(tf_train_set.shuffle(1000).batch(16), epochs=3, batch_size=16,\n\u001b[0;32m----> 2\u001b[0;31m           validation_data=tf_test_set.shuffle(1000).batch(16))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\", line 996, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"tf_distil_bert_for_sequence_classification_1\" (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\", line 730, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 742, in call  *\n            distilbert_output = self.distilbert(\n        File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        TypeError: Exception encountered when calling layer \"distilbert\" (type TFDistilBertMainLayer).\n        \n        in user code:\n        \n            File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\", line 730, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 400, in call  *\n                embedding_output = self.embeddings(input_ids, inputs_embeds=inputs_embeds)  # (bs, seq_length, dim)\n            File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n        \n            TypeError: Exception encountered when calling layer \"embeddings\" (type TFEmbeddings).\n            \n            in user code:\n            \n                File \"/home/seldon/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 122, in call  *\n                    final_embeddings = self.LayerNorm(inputs=final_embeddings)\n                File \"/home/seldon/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n            \n                TypeError: Exception encountered when calling layer \"LayerNorm\" (type LayerNormalization).\n                \n                Failed to convert elements of [1, 1, None, 1] to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n                \n                Call arguments received:\n                  â€¢ inputs=tf.Tensor(shape=(None, 16, None, 768), dtype=float32)\n            \n            \n            Call arguments received:\n              â€¢ input_ids=tf.Tensor(shape=(None, 16, None), dtype=int64)\n              â€¢ position_ids=None\n              â€¢ inputs_embeds=None\n              â€¢ training=True\n        \n        \n        Call arguments received:\n          â€¢ self=tf.Tensor(shape=(None, 16, None), dtype=int64)\n          â€¢ input_ids=None\n          â€¢ attention_mask=tf.Tensor(shape=(None, 16, None), dtype=int64)\n          â€¢ head_mask=None\n          â€¢ inputs_embeds=None\n          â€¢ output_attentions=False\n          â€¢ output_hidden_states=False\n          â€¢ return_dict=True\n          â€¢ training=True\n    \n    \n    Call arguments received:\n      â€¢ self={'input_ids': 'tf.Tensor(shape=(None, 16, None), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(None, 16, None), dtype=int64)', 'labels': 'tf.Tensor(shape=(None, 16), dtype=int64)'}\n      â€¢ input_ids=None\n      â€¢ attention_mask=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ output_attentions=None\n      â€¢ output_hidden_states=None\n      â€¢ return_dict=None\n      â€¢ labels=None\n      â€¢ training=True\n"
     ]
    }
   ],
   "source": [
    "# model.fit(tf_train_set.shuffle(1000).batch(16), epochs=3, batch_size=16,\n",
    "#           validation_data=tf_test_set.shuffle(1000).batch(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tom - Need to add accuracy metric in here too, but got error about using internal loss function meaning you can't specify additional metrics. At the moment is it unsupervised because it's using the pretrained labels? and this is why we don't see accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tom - tf_test_set - the reviews have been tokenised. Does the tokeniser need embedding in the model? Otherwise a preprocesser will be needed in order to pass raw text to the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703/938 [=====================>........] - ETA: 4:02 - loss: 2.1695"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6b55bb352e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mint_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    947\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(tf_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tom - from here down not used anymore - this was messing to build a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.17513809638900274,\n",
       " 1: 0.3963441218203293,\n",
       " 2: 2.1003990758244067,\n",
       " 3: 7.2811999417504,\n",
       " 4: 8.88888888888889,\n",
       " 5: 37.03703703703704,\n",
       " 6: 205.76131687242798,\n",
       " 7: 205.76131687242798,\n",
       " 8: 222.22222222222223}"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset into tensorflow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_tensor_slices((train['review'].values, train['rating'].values))\n",
    "ds_test = Dataset.from_tensor_slices((test['review'].values, test['rating'].values))\n",
    "ds_val = Dataset.from_tensor_slices((val['review'].values, val['rating'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert categorical target variable to numeric variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-25855b0a6604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# if using can pop this in a function, just experimenting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mct0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# if using can pop this in a function, just experimenting\n",
    "\n",
    "ct0, ct1, ct2, ct3, ct4, ct5, ct6, ct7, ct8 = np.bincount(y_train)\n",
    "total = len(df)\n",
    "\n",
    "weight_for_0 = (1 / ct0) * (total / 9.0)\n",
    "weight_for_1 = (1 / ct1) * (total / 9.0)\n",
    "weight_for_2 = (1 / ct2) * (total / 9.0)\n",
    "weight_for_3 = (1 / ct3) * (total / 9.0)\n",
    "weight_for_4 = (1 / ct4) * (total / 9.0)\n",
    "weight_for_5 = (1 / ct5) * (total / 9.0)\n",
    "weight_for_6 = (1 / ct6) * (total / 9.0)\n",
    "weight_for_7 = (1 / ct7) * (total / 9.0)\n",
    "weight_for_8 = (1 / ct8) * (total / 9.0)\n",
    "\n",
    "class_weights = {\n",
    "    0: weight_for_0, \n",
    "    1: weight_for_1, \n",
    "    2: weight_for_2, \n",
    "    3: weight_for_3,\n",
    "    4: weight_for_4,\n",
    "    5: weight_for_5,\n",
    "    6: weight_for_6,\n",
    "    7: weight_for_7,\n",
    "    8: weight_for_8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return int_vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_train_ds = int_vectorize_text(X_train, y_train)\n",
    "int_val_ds = int_vectorize_text(X_val, y_val)\n",
    "int_test_ds = int_vectorize_text(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(33750, 150), dtype=int64, numpy=\n",
       " array([[  6,  83,  35, ...,   0,   0,   0],\n",
       "        [733,   3,  31, ...,   0,   0,   0],\n",
       "        [ 40,   4,  45, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  6,  18,  19, ...,   0,   0,   0],\n",
       "        [ 40,  57,  11, ...,   0,   0,   0],\n",
       "        [  6, 183,   5, ...,   0,   0,   0]])>,\n",
       " array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]], dtype=float32))"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding layer**\n",
    "\n",
    "Input shape:\n",
    "2D tensor with shape: (batch_size, input_length).\n",
    "\n",
    "Output shape:\n",
    "3D tensor with shape: (batch_size, input_length, output_dim).\n",
    "\n",
    "*Do I have this right at the moment? I have input length as the vocab size... should output_dim be 9? What should it be?*\n",
    "\n",
    "input_length = Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "\n",
    "output_dim = Integer. Dimension of the dense embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conv1D layer**\n",
    "\n",
    "Should it not be 16 here?... filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "\n",
    "Should it not be 5 here?... kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n",
    "\n",
    "Do I need an input_shape argument? I don't know if I do as it's not the first layer? Embedding is the first layer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shape = int_train_ds[0].reshape(len(int_train_ds[0]), int_train_ds[0].shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "in_shape = (test_shape.shape[1],1)\n",
    "inputs = Input(shape=(in_shape), name='inputs_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 150, 1) dtype=float32 (created by layer 'inputs_cnn')>"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(33750, 150), dtype=int64, numpy=\n",
       "array([[  6,  83,  35, ...,   0,   0,   0],\n",
       "       [733,   3,  31, ...,   0,   0,   0],\n",
       "       [ 40,   4,  45, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  6,  18,  19, ...,   0,   0,   0],\n",
       "       [ 40,  57,  11, ...,   0,   0,   0],\n",
       "       [  6, 183,   5, ...,   0,   0,   0]])>"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, num_labels, input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "      layers.Embedding(vocab_size, 1024, mask_zero=True, input_shape=input_shape),\n",
    "      layers.Conv1D(1024, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv1d_34. Consider increasing the input size. Received input shape [None, 150, 1, 1024] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-356-4c407121faeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mint_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m int_model.compile(\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-355-34fe80b86dd0>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(vocab_size, num_labels, input_shape)\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     ])\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       raise ValueError(\n\u001b[0;32m--> 303\u001b[0;31m           \u001b[0;34mf'One of the dimensions in the output is <= 0 '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m           \u001b[0;34mf'due to downsampling in {self.name}. Consider '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m           \u001b[0;34mf'increasing the input size. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv1d_34. Consider increasing the input size. Received input shape [None, 150, 1, 1024] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\n",
    "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=9, input_shape=in_shape)\n",
    "int_model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "history = int_model.fit(\n",
    "    inputs, \n",
    "    int_train_ds[1], \n",
    "#     validation_data=(int_val_ds[0], int_val_ds[1]), \n",
    "    epochs=15,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    metrics = ['loss', 'accuracy']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.8,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "        plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.5388 - accuracy: 0.5240\n",
      "Int model accuracy: 52.40%\n"
     ]
    }
   ],
   "source": [
    "int_loss, int_accuracy = int_model.evaluate(int_test_ds[0], int_test_ds[1])\n",
    "\n",
    "print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-12.551326   -14.205595   -10.307593    -6.996902    -6.236613\n",
      "  -11.317929    14.247541     0.38065982  15.718541  ]]\n"
     ]
    }
   ],
   "source": [
    "predictions = int_model.predict(int_test_ds[0])\n",
    "print(predictions[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-12.551326   -14.205595   -10.307593   ...  14.247541     0.38065982\n",
      "   15.718541  ]\n",
      " [ -2.5539522  -11.087661     2.168796   ...   0.59642243  -8.97046\n",
      "    5.491527  ]\n",
      " [-14.290668   -13.789915    -4.7460256  ...  11.998764     1.4773175\n",
      "   13.5251255 ]\n",
      " ...\n",
      " [-16.912428   -18.866726   -10.535642   ...   7.2865367   -3.2117934\n",
      "   10.618746  ]\n",
      " [ -7.1668673  -10.601623    -6.650805   ...  -4.0072656   -5.804961\n",
      "   -0.5142288 ]\n",
      " [ -3.5702062   -6.9096284   -2.2240076  ...  -1.6571041   -4.138695\n",
      "    0.28596228]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_re = int_test_ds[1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_list = []\n",
    "pred_val_list = []\n",
    "\n",
    "for i in range(0,len(test_re)):\n",
    "    test_val = test_re[i]\n",
    "    test_val_list.append(test_val)\n",
    "    pred_val = list(predictions[i]).index(predictions[i].max())\n",
    "    pred_val_list.append(pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(\n",
    "        {'Test_val':test_val_list,\n",
    "            'Pred_val':pred_val_list}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_val</th>\n",
       "      <th>Pred_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Test_val  Pred_val\n",
       "0            2         8\n",
       "1            6         8\n",
       "2            8         8\n",
       "3            8         8\n",
       "4            8         6\n",
       "...        ...       ...\n",
       "4995         8         8\n",
       "4996         8         6\n",
       "4997         8         8\n",
       "4998         6         8\n",
       "4999         4         8\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['match']='False'\n",
    "df_results.loc[df_results['Test_val']==df_results['Pred_val'],'match'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_results.groupby(['Test_val','match']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    0.6066\n",
       "6    0.3024\n",
       "4    0.0510\n",
       "0    0.0196\n",
       "2    0.0166\n",
       "7    0.0024\n",
       "3    0.0008\n",
       "5    0.0004\n",
       "1    0.0002\n",
       "Name: Pred_val, dtype: float64"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results['Pred_val'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Pred_val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_val</th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>False</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>False</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>False</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>False</th>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>False</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>False</th>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">7</th>\n",
       "      <th>False</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>False</th>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Pred_val\n",
       "Test_val match          \n",
       "0        False        55\n",
       "         True         26\n",
       "1        False         1\n",
       "2        False        46\n",
       "         True          5\n",
       "4        False       246\n",
       "         True         37\n",
       "5        False         1\n",
       "         True          1\n",
       "6        False       901\n",
       "         True        472\n",
       "7        False        14\n",
       "         True          1\n",
       "8        False      1116\n",
       "         True       2078"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now deploy your model to the dedicated Seldon Deploy cluster which we have configured for this workshop. To do so you will interact with the Seldon Deploy SDK and deploy your model using that.\n",
    "\n",
    "First, setting up the configuration and authentication required to access the cluster. Make sure to fill in the SD_IP variable to be the same as the cluster you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do - speak to Tom around which cluster we will be using for the workshop/ask Alejandro for \n",
    "# a cluster to be manually spun up if needed\n",
    "\n",
    "SD_IP = \"\"\n",
    "\n",
    "config = Configuration()\n",
    "config.host = f\"http://{SD_IP}/seldon-deploy/api/v1alpha1\"\n",
    "config.oidc_client_id = \"sd-api\"\n",
    "config.oidc_server = f\"http://{SD_IP}/auth/realms/deploy-realm\"\n",
    "config.oidc_client_secret = \"sd-api-secret\"\n",
    "config.auth_method = \"client_credentials\"\n",
    "\n",
    "def auth():\n",
    "    auth = OIDCAuthenticator(config)\n",
    "    config.id_token = auth.authenticate()\n",
    "    api_client = ApiClient(configuration=config, authenticator=auth)\n",
    "    return api_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have configured the IP correctly as well as setup your authentication function you can describe the deployment you would like to create. \n",
    "\n",
    "You will need to fill in the `DEPLOYMENT_NAME`. The `NAMESPACE` and `MODEL_LOCATION`, along with the rest of the deployment description has been templated for you. \n",
    "\n",
    "## To do - check with Tom - is this true for Tensorflow? Are models always saved as a folder with assets, metadata, model.pb etc? Or can you just save the model as model.pb in this case?\n",
    "\n",
    "For the `MODEL_LOCATION`, it is worth nothing that you will not specify the path all the way up to `model.pb`, but instead the path should be to the model folder for your transformer model i.e. `MODEL_LOCATION` should be `gs://kelly-seldon/nlp-ratings/saved-model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_NAME = \n",
    "NAMESPACE = \"seldon-gitops\"\n",
    "MODEL_LOCATION = f\"gs://gs://kelly-seldon/nlp-ratings/models/{YOUR_NAME}\"\n",
    "\n",
    "PREPACKAGED_SERVER = \"TENSORFLOW_SERVER\"\n",
    "\n",
    "CPU_REQUESTS = \"1\"\n",
    "MEMORY_REQUESTS = \"1Gi\"\n",
    "\n",
    "CPU_LIMITS = \"1\"\n",
    "MEMORY_LIMITS = \"1Gi\"\n",
    "\n",
    "mldeployment = {\n",
    "    \"kind\": \"SeldonDeployment\",\n",
    "    \"metadata\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"namespace\": NAMESPACE,\n",
    "        \"labels\": {\n",
    "            \"fluentd\": \"true\"\n",
    "        }\n",
    "    },\n",
    "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "    \"spec\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"annotations\": {\n",
    "            \"seldon.io/engine-seldon-log-messages-externally\": \"true\"\n",
    "        },\n",
    "        \"protocol\": \"seldon\",\n",
    "        \"transport\": \"rest\",\n",
    "        \"predictors\": [\n",
    "            {\n",
    "                \"componentSpecs\": [\n",
    "                    {\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                                    \"resources\": {\n",
    "                                        \"requests\": {\n",
    "                                            \"cpu\": CPU_REQUESTS,\n",
    "                                            \"memory\": MEMORY_REQUESTS\n",
    "                                        },\n",
    "                                        \"limits\": {\n",
    "                                            \"cpu\": CPU_LIMITS,\n",
    "                                            \"memory\": MEMORY_LIMITS\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": \"default\",\n",
    "                \"replicas\": 1,\n",
    "                \"traffic\": 100,\n",
    "                \"graph\": {\n",
    "                    \"implementation\": PREPACKAGED_SERVER,\n",
    "                    \"modelUri\": MODEL_LOCATION,\n",
    "                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                    \"endpoint\": {\n",
    "                        \"type\": \"REST\"\n",
    "                    },\n",
    "                    \"parameters\": [],\n",
    "                    \"children\": [],\n",
    "                    \"logger\": {\n",
    "                        \"mode\": \"all\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"status\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now invoke the SeldonDeploymentsApi and create a new Seldon Deployment.\n",
    "\n",
    "Time for you to get your hands dirty. You will use the Seldon Deploy SDK to create a new Seldon deployment. You can find the reference documentation [here](https://github.com/SeldonIO/seldon-deploy-sdk/blob/master/python/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the Seldon Deploy cluster and view your freshly created deployment here:\n",
    "\n",
    "### To do - add cluster IP in here when cluster is decided on\n",
    "\n",
    "* URL: http:///seldon-deploy/\n",
    "* Username: admin@seldon.io\n",
    "* Password: 12341234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Prediction Schema and Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seldon Deploy has a model catalog where all deployed models are automatically registered. The model catalog can store custom metadata as well as prediction schemas for your models.\n",
    "\n",
    "Metadata promotes lineage from across different machine learning systems, aids knowledge transfer between teams, and allows for faster deployment. Meanwhile, prediction schemas allow Seldon Deploy to automatically profile tabular data into histograms, allowing for filtering on features to explore trends.\n",
    "\n",
    "In order to effectively construct a prediction schema Seldon has the ML Prediction Schema project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_schema = {\n",
    "    \"requests\": [\n",
    "        {\n",
    "            \"name\": \"Product Review\",\n",
    "            \"type\": \"TEXT\",\n",
    "        }\n",
    "    ],\n",
    "    \"responses\": [\n",
    "        {\n",
    "            \"name\": \"Rating\",\n",
    "            \"type\": \"CATEGORICAL\",\n",
    "            \"data_type\": \"INT\",\n",
    "            \"n_categories\": 9,\n",
    "            \"category_map\": {\n",
    "            \"0\": \"1.0\",\n",
    "            \"1\": \"1.5\",\n",
    "            \"2\": \"2.0\",\n",
    "            \"3\": \"2.5\",\n",
    "            \"4\": \"3.0\",\n",
    "            \"5\": \"3.5\",\n",
    "            \"6\": \"4.0\",\n",
    "            \"7\": \"4.5\",\n",
    "            \"8\": \"5.0\"\n",
    "        }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then add the prediction schema to the wider model catalog metadata. This includes information such as the model storage location, the name, who authored the model etc. The metadata tags and metrics which can be associated with a model are freeform and can therefore be determined based upon the use case which is being developed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_LOCATION' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a1df7d1d982c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model_catalog_metadata = {\n\u001b[0;32m----> 4\u001b[0;31m       \u001b[0;34m\"URI\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMODEL_LOCATION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m       \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{DEPLOYMENT_NAME}-model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0;34m\"version\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"v1.0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL_LOCATION' is not defined"
     ]
    }
   ],
   "source": [
    "# To do if time - add accuracy as a metric in here once have a working model\n",
    "\n",
    "model_catalog_metadata = {\n",
    "      \"URI\": MODEL_LOCATION,\n",
    "      \"name\": f\"{DEPLOYMENT_NAME}-model\",\n",
    "      \"version\": \"v1.0\",\n",
    "      \"artifactType\": \"Tensorflow\",\n",
    "      \"taskType\": \"Product review rating classification\",\n",
    "      \"tags\": {\n",
    "        \"auto_created\": \"true\",\n",
    "        \"author\": f\"{YOUR_NAME}\"\n",
    "      },\n",
    "      \"metrics\": {},\n",
    "      \"project\": \"default\",\n",
    "      \"prediction_schema\": prediction_schema\n",
    "    }\n",
    "\n",
    "# model_catalog_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using the metadata API you can add this to the model which you have just created in Seldon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_api = ModelMetadataServiceApi(auth())\n",
    "metadata_api.model_metadata_service_update_model_metadata(model_catalog_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then list the metadata via the API, or view it in the UI, to confirm that it has been successfully added to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_response = metadata_api.model_metadata_service_list_model_metadata(uri=MODEL_LOCATION)\n",
    "metadata_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can have a go at sending some requests to your model using the 'Predict' tab in the UI. \n",
    "\n",
    "An example of a good review that we would expect to correspond to a higher rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "        \"names\": [\"Review\"],\n",
    "        \"ndarray\": [\"_product_ is excellent! I love it, it's great!\"]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "And an example of a negative review that we would expect to correspond to a lower rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": {\n",
    "        \"names\": [\"Review\"],\n",
    "        \"ndarray\": [\"_product_ was terrible, I would not use it again, it was awful!\"]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although powerful, modern machine learning models can be sensitive. Seemingly subtle changes in a data distribution can destroy the performance of otherwise state-of-the art models, which can be especially problematic when ML models are deployed in production. Typically, ML models are tested on held out data in order to estimate their future performance. Crucially, this assumes that the process underlying the input data `X` and output data `Y` remains constant.\n",
    "\n",
    "Drift can be classified into the following types:\n",
    "* **Covariate drift**: Also referred to as input drift, this occurs when the distribution of the input data has shifted `P(X) != Pref(X)`, whilst `P(Y|X) = Pref(Y|X)`. This may result in the model giving unreliable predictions.\n",
    "\n",
    "* **Prior drift**: Also referred to as label drift, this occurs when the distribution of the outputs has shifted `P(Y) != Pref(Y)`, whilst `P(X|Y) = Pref(X|Y)`. This can affect the modelâ€™s decision boundary, as well as the modelâ€™s performance metrics.\n",
    "\n",
    "* **Concept drift**: This occurs when the process generating `Y` from `X` has changed, such that `P(Y|X) != Pref(Y|X)`. It is possible that the model might no longer give a suitable approximation of the true process.\n",
    "\n",
    "-----------------\n",
    "\n",
    "In this instance we will train a Kolmgorov-Smirnov drift detector to pick up on covariate drift. The KS Drift detector applies a two-sample KS test to compare the distance between the new probability distribution and the reference distribution. \n",
    "\n",
    "This is done on a feature by feature basis and the results are then aggregated using a correction, i.e. Bonferroni, to determine whether drift has occurred overall within the sample. \n",
    "\n",
    "We will use the training set as our reference distribution. Creating our drift detector is then as simple as writing a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_array = np.asarray(encoded_dataset['train']['input_ids'][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = KSDrift(dd_array, p_val=.05, correction='bonferroni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drift? No\n"
     ]
    }
   ],
   "source": [
    "preds = dd.predict(dd_array)\n",
    "labels = [\"No\", \"Yes\"]\n",
    "\n",
    "print(f'Drift? {labels[preds[\"data\"][\"is_drift\"]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'is_drift': 0,\n",
       "  'distance': array([0.    , 0.0736, 0.0778, 0.0618, 0.078 , 0.0686, 0.1028, 0.061 ,\n",
       "         0.1342, 0.128 , 0.0932, 0.1122, 0.1264, 0.1026, 0.0536, 0.0752,\n",
       "         0.1442, 0.1262, 0.104 , 0.0808, 0.0604, 0.101 , 0.0792, 0.1144,\n",
       "         0.0878, 0.093 , 0.0936, 0.1054, 0.1232, 0.1152, 0.1056, 0.158 ,\n",
       "         0.1144, 0.1206, 0.1206, 0.0956, 0.0736, 0.0842, 0.0758, 0.0686,\n",
       "         0.078 , 0.069 , 0.082 , 0.0672, 0.0502, 0.0584, 0.085 , 0.0592,\n",
       "         0.0702, 0.067 , 0.069 , 0.0512, 0.0548, 0.059 , 0.0584, 0.0746,\n",
       "         0.054 , 0.059 , 0.0678, 0.058 , 0.0454, 0.0544, 0.0496, 0.055 ,\n",
       "         0.046 , 0.0322, 0.0328, 0.044 , 0.0352, 0.0224, 0.0334, 0.027 ,\n",
       "         0.028 , 0.0508, 0.0172, 0.0264, 0.0436, 0.0216, 0.0338, 0.0144,\n",
       "         0.0254, 0.0364, 0.064 , 0.0314, 0.0406, 0.0226, 0.0318, 0.014 ,\n",
       "         0.0244, 0.0322, 0.0404, 0.0238, 0.0384, 0.0358, 0.036 , 0.022 ,\n",
       "         0.0344, 0.031 , 0.0192, 0.032 , 0.0296, 0.02  , 0.0214, 0.0172,\n",
       "         0.0134, 0.0208, 0.0266, 0.0174, 0.0148, 0.0176, 0.0104, 0.0086,\n",
       "         0.0176, 0.0236, 0.012 , 0.021 , 0.0284, 0.0186, 0.0144, 0.0102,\n",
       "         0.018 , 0.0182, 0.019 , 0.012 , 0.0194, 0.0104, 0.0152, 0.009 ],\n",
       "        dtype=float32),\n",
       "  'p_val': array([1.        , 0.6361109 , 0.5664959 , 0.82572395, 0.56322813,\n",
       "         0.7193911 , 0.23491362, 0.83718324, 0.05324746, 0.07366122,\n",
       "         0.34100214, 0.15707447, 0.07989575, 0.23683992, 0.9265221 ,\n",
       "         0.6094172 , 0.03053895, 0.08070546, 0.22360176, 0.51816106,\n",
       "         0.8455835 , 0.25267687, 0.54374874, 0.14221026, 0.41297403,\n",
       "         0.34351197, 0.33601907, 0.21093121, 0.09370344, 0.13709456,\n",
       "         0.20916684, 0.01327207, 0.14221026, 0.10633895, 0.10633895,\n",
       "         0.31183937, 0.6361109 , 0.46555644, 0.59945047, 0.7193911 ,\n",
       "         0.56322813, 0.71280104, 0.4993004 , 0.7422566 , 0.9553735 ,\n",
       "         0.8722614 , 0.4535764 , 0.86184597, 0.692916  , 0.7454935 ,\n",
       "         0.71280104, 0.9477553 , 0.91437614, 0.8644829 , 0.8722614 ,\n",
       "         0.61941093, 0.9225844 , 0.8644829 , 0.73249894, 0.8773338 ,\n",
       "         0.98212445, 0.91853535, 0.9595969 , 0.9122557 , 0.9796251 ,\n",
       "         0.9998847 , 0.99983335, 0.9871125 , 0.9993933 , 1.        ,\n",
       "         0.99976367, 0.9999984 , 0.99999565, 0.95088965, 1.        ,\n",
       "         0.99999917, 0.9883331 , 1.        , 0.9997047 , 1.        ,\n",
       "         0.99999976, 0.99894613, 0.79285365, 0.9999318 , 0.99498856,\n",
       "         1.        , 0.9999109 , 1.        , 0.99999994, 0.9998847 ,\n",
       "         0.9952958 , 1.        , 0.9976379 , 0.99919474, 0.99911785,\n",
       "         1.        , 0.9995936 , 0.9999483 , 1.        , 0.99989855,\n",
       "         0.9999821 , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 0.999999  , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 0.9999936 , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        ], dtype=float32),\n",
       "  'threshold': 0.000390625},\n",
       " 'meta': {'name': 'KSDrift',\n",
       "  'detector_type': 'offline',\n",
       "  'data_type': None,\n",
       "  'version': '0.8.1'}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dd.predict(\n",
    "    np.asarray(encoded_dataset['test']['input_ids'][6700:6800]), \n",
    "    return_p_val=True, \n",
    "    return_distance=True)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_detector(dd, \"reviews-drift-detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Will add this earlier when saving the model etc\n",
    "\n",
    "YOUR_NAME = \"kelly-spry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://reviews-drift-detector/KSDrift.dill [Content-Type=application/octet-stream]...\n",
      "Copying file://reviews-drift-detector/meta.dill [Content-Type=application/octet-stream]...\n",
      "| [2 files][  4.9 MiB/  4.9 MiB]                                                \n",
      "Operation completed over 2 objects/4.9 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r reviews-drift-detector gs://kelly-seldon/nlp-ratings/models/{YOUR_NAME}/reviews-drift-detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the drift detector you will use Seldon Deploy's user interface. Simply navigate to your deployment, and select the \"Create\" button for your drift detector.\n",
    "\n",
    "This will bring up a form. Add your detector name, the URI which will be gs://kelly-seldon/nlp-ratings/models/{YOUR_NAME}/reviews-drift-detector, and set a batch size of 10.\n",
    "\n",
    "The batch size configuration sets how many data points have to be sent to the endpoint before drift is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
