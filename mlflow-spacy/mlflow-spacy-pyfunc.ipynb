{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging, Deployment and Monitoring of spaCy Entity Extractor using MLFlow and Seldon Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop is focused on the packaging, deployment and monitoring of a machine learning model for extracting entities from text.\n",
    "\n",
    "In order to create an MLFlow `pyfunc` model, we will use the `mlflow.pyfunc` utilities. We will define a class for a custom `PythonModel` that creates the desired entity dictionary output, before saving this custom model.\n",
    "\n",
    "We will then deploy the model with Seldon Deploy, using the MLServer MLFlow runtime (MLServer is on open source inference server for ML models. MLServer has support for for the standard V2 Inference Protocol on both the gRPC and REST flavours, which has been standardised and adopted by various model serving frameworks. Full MLServer docs can be found [here](https://mlserver.readthedocs.io/en/latest/)). In order to do this, we will create a custom conda environment to ensure any extra dependencies not known in advance by MLServer are available. These dependencies are those that won't be include by default in the default `seldonio/mlserver` Docker image, so the `seldonio/mlserver` Docker image allows you to load custom environments before starting the server itself. We will serialise our created environment in the format expected by MLServer by using a tool called `conda-pack`. This tool will save a portable version of our environment as a `.tar.gz` file, also known as a *tarball*. We will deploy our model using the V2 Inference Protocol. \n",
    "\n",
    "We will then turn to the advanced monitoring capabilities that Seldon Alibi is famed for. We will configure and deploy a drift detector, before running a batch job in order to observe drift on the Seldon Deploy UI. \n",
    "\n",
    "Agenda:\n",
    "\n",
    "1) Set up environment\n",
    "2) Define Entity Extraction code\n",
    "3) Define custom MLFlow `PythonModel`\n",
    "4) Create custom conda environment\n",
    "5) *Optional* - Test model locally using `mlserver`\n",
    "6) Save MLFlow model to GCS\n",
    "7) Deploy MLFlow model with Seldon Deploy\n",
    "8) Add metadata and prediction schema to deployed MLFlow model \n",
    "9) Configure Maximum Mean Discrepancy (MMD) Drift Detector\n",
    "10) Deploy Drift Detector with Seldon Deploy\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up environment\n",
    "\n",
    "Firstly, we will import the relevant packages which we will use throughout the packaging and deployment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import spacy\n",
    "import mlflow\n",
    "import mlflow.spacy\n",
    "import mlflow.pyfunc\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from mlserver.codecs.string import StringRequestCodec\n",
    "from mlserver.logging import logger\n",
    "\n",
    "from seldon_deploy_sdk import Configuration, PredictApi, ApiClient, SeldonDeploymentsApi, ModelMetadataServiceApi, DriftDetectorApi, BatchJobsApi, BatchJobDefinition\n",
    "from seldon_deploy_sdk.auth import OIDCAuthenticator, SessionAuthenticator\n",
    "from seldon_deploy_sdk.rest import ApiException\n",
    "\n",
    "from alibi_detect.models.tensorflow import TransformerEmbedding\n",
    "from alibi_detect.cd import MMDDrift\n",
    "from functools import partial\n",
    "from alibi_detect.cd.tensorflow import preprocess_drift\n",
    "from alibi_detect.utils.saving import save_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get the saved spaCy artefact from where it has been stored in Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://andrew-seldon/sandbox/spacy_body_model ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**The following 5 sections (*Define Entity Extraction code, define custom MLFlow `PythonModel`, create custom conda environment, *Optional* - Test model locally using `mlserver`, and save MLFlow model to GCS*) concentrate on packaging up the spaCy Entity Extractor ready for Deployment with Seldon. They follow the assumption that a trained spaCy entity extractor is already available, as is the case here.**\n",
    "\n",
    "**These 4 sections would very typically form part of an automation pipeline (i.e. a CI/CD pipeline). For the sake of showing each of the steps in the workshop, we will run through them in a notebook, but it is worth emphasising that all of the following steps would be abstracted away and fully automated in a real world scenario.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Entity Extraction code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define text processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_geboortedatum(preprocessed_text):\n",
    "    \"\"\"\n",
    "    returns a list with all the valid geboortedatums in the db notation (%Y-%m-%d) in in the preprocessed text\n",
    "    @param preprocessed_text: processed text; entities are in database label notation format\n",
    "    @return: list with all the found birth dates. A birthdate is considered valid if the date is older than\n",
    "    12 years (currently hardcoded...)\n",
    "    \"\"\"\n",
    "    gebdat_pattern = re.compile(r'\\b[1-2][0-9]{3}-[0-1][0-9]-[0-3][0-9]\\b')\n",
    "    return [m.group() for m in re.finditer(gebdat_pattern, preprocessed_text) if valid_birthdate(m.group(), format='%Y-%m-%d')]\n",
    "\n",
    "\n",
    "def find_postcode(preprocessed_text):\n",
    "    \"\"\"\n",
    "    returns a list with all the postcodes in the db notation (1234AB)\n",
    "    @param preprocessed_text: processed text; entities are in database label notation format\n",
    "    @return: list with all the found postcodes in db notation (1234AB)\n",
    "    \"\"\"\n",
    "    postcode_pattern = re.compile(r'\\b[1-9][0-9]{3}[A-Z]{2}\\b')\n",
    "    return [m.group() for m in re.finditer(postcode_pattern, preprocessed_text)]\n",
    "\n",
    "\n",
    "def find_polis(preprocessed_text):\n",
    "    \"\"\"\n",
    "    returns a list with all the polisnummers in the db notation\n",
    "    @param preprocessed_text: processed text; entities are in database label notation format\n",
    "    @return: list with all the found polisnummers in db notation\n",
    "    \"\"\"\n",
    "    polis_pattern = re.compile(\"|\".join([\n",
    "        r\"\\b29[0-9]{6}\\b\", #PM29000000\tPM29599999 & PM29600000\tPM29999999\n",
    "        r\"\\b147[0-9]{5}\\b\", #PW14700000\tPW14799999\n",
    "        r\"\\b1[0-9]{7}\\b\",#SW10000000\tSW19999999\n",
    "        r\"\\b4[0-9]{7}\\b\",#PK40000000\tPK42999999 \n",
    "        r\"\\b9[0-9]{7}\\b\",#SL90000000\tSL92999999  & SL93000000\tSL93999999\n",
    "        r\"\\b95[0-9]{6}\\b\",#LP95000000\tLP95999999\n",
    "        r\"\\b147[0-9]{5}\\b\",#PW14700000\tPW14799999\n",
    "        r\"\\b81[0-9]{6}\\b\"]))#PW81000000\tPW81999999 \n",
    "    return [m.group() for m in re.finditer(polis_pattern, preprocessed_text)]\n",
    "\n",
    "    \n",
    "def find_nn_iban(preprocessed_text):\n",
    "    \"\"\"\n",
    "    returns a list with all NN iban numbers found in the input text\n",
    "    @param preprocessed_text: processed text; entities are in database label notation format\n",
    "    @return: list with all the found iban numbers\n",
    "    \"\"\"\n",
    "    return find_iban(preprocessed_text, nn_iban=True)\n",
    "\n",
    "\n",
    "def find_rekeningnummer(preprocessed_text):\n",
    "    \"\"\"\n",
    "    returns a list with all rekening nummers (length between 7 and 10) found in the input text\n",
    "    @param preprocessed_text: processed text; entities are in database label notation format\n",
    "    @return: list with all the found rekening numbers\n",
    "    \"\"\"\n",
    "    rekeningnummer_pattern = re.compile(r'\\b\\d{7,10}\\b')\n",
    "    return [m.group() for m in re.finditer(rekeningnummer_pattern, preprocessed_text)]\n",
    "\n",
    "\n",
    "def find_iban(preprocessed_text, nn_iban=False):\n",
    "    \"\"\"\n",
    "    returns a list with all iban numbers found in the input text\n",
    "    @param preprocessed_text: processed text; entities are in database label notation format\n",
    "    @param nn_iban: If true the regex only searches for iban numbers containing 'NNBA'\n",
    "    @return: list with all the found iban numbers\n",
    "    \"\"\"\n",
    "    if nn_iban:\n",
    "        iban_pattern = re.compile(r'\\bNL\\s*\\d\\d\\s*NNBA\\s*\\d{​​​​​​​​10}​​​​​​​​\\b')\n",
    "    else:\n",
    "        iban_pattern = re.compile(r'\\bNL\\s*\\d\\d\\s*\\w{​​​​​​​​4}​​​​​​​​\\s*\\d{​​​​​​​​10}​​​​​​​​\\b')\n",
    "\n",
    "    matches = [m.group() for m in re.finditer(iban_pattern, preprocessed_text)]\n",
    "    cleaned_matches = [match.replace(' ', '') for match in matches]\n",
    "    stripped_matches = [i[-10:] for i in cleaned_matches]\n",
    "    return stripped_matches\n",
    "\n",
    "\n",
    "def find_phonenumber(preprocessed_text):\n",
    "    \"\"\"\n",
    "    returns a list with all the phonenumbers in the db notation\n",
    "    @param preprocessed_text: processed text; entities are in database label notation format\n",
    "    @return: list with all the found phonenumbers in db notation\n",
    "    \"\"\"\n",
    "    polis_pattern = re.compile(\"|\".join([\n",
    "        r'\\(?([+]31|0031|0)-?6(\\s?|-)([0-9]\\s{0,3}){8}$',\n",
    "        r'(((0)[1-9]{2}[0-9](\\s?|-)?[1-9][0-9]{5})$',\n",
    "        r'(((0)[1-9]{2}(\\s?|-)?[1-9][0-9]{2}(\\s?|-)?[0-9]{2}(\\s?|-)?[0-9]{2}))$',# vast\n",
    "        r'(((0)[1-9]{2}[0-9](\\s?|-)?[1-9][0-9]{2}(\\s?|-)?[0-9]{2}(\\s?|-)?[0-9]{2}))$',\n",
    "        r'((\\\\+31|0|0031)[1-9][0-9](\\s?|-)?[1-9][0-9]{6}))$',\n",
    "        #vast\n",
    "        \n",
    "        ]))\n",
    "    found_numbers =  [m.group() for m in re.finditer(polis_pattern, preprocessed_text)]\n",
    "    return [x.replace('-','').replace(' ','')[-9:] for x in found_numbers]\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Removes the actual url and replaces it with 'URL'. Only works for\n",
    "    @param text: string containing the text that needs to be processed\n",
    "    @return: original string with the url replaced with 'URL'\n",
    "    \"\"\"\n",
    "    url_pattern = r'<?http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(url_pattern, 'URL', text)\n",
    "\n",
    "\n",
    "def zipcode_db_notation(text):\n",
    "    \"\"\"\n",
    "    DB notation for postal codes is 1234AB instead of 1234 AB -> the space is removed for postal codes.\n",
    "    @param text: string containing the text that needs to be processed\n",
    "    @return: original string with the optional space in postal codes removed (e.g. 1234 AB -> 1234AB)\n",
    "    \"\"\"\n",
    "    zipcode_pattern = r'\\b[1-9][0-9]{3}[ -]?[A-Z]{2}\\b|\\b[1-9][0-9]{3}-?[a-zA-Z]{2}\\b'\n",
    "\n",
    "    def db_notate(match):\n",
    "        return match.group().replace(' ', '').replace(\"-\", \"\").upper()\n",
    "    return re.sub(zipcode_pattern, db_notate, text)\n",
    "\n",
    "\n",
    "def valid_date(candidate_date, format='%d-%m-%Y'):\n",
    "    \"\"\"\n",
    "    Checks if the a date can be converted to a valid date(time)\n",
    "    @param candidate_date: string that needs to be converted to a date\n",
    "    @param format: date format\n",
    "    @return: bool (True/False) whether the string can be converted to a datetime with specified format\n",
    "    \"\"\"\n",
    "    try: \n",
    "        valid_date = pd.to_datetime(candidate_date, format=format)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def valid_birthdate(candidate_date, format='%d-%m-%Y', threshold=12*365.25):\n",
    "    \"\"\"\n",
    "    checks whether a date can be considered as a birthdate. The assumtion is that very\n",
    "    recent dates are unlikely to be actual birthdays.\n",
    "    @param candidate_date: string that needs to be validated for a proper birthday\n",
    "    @param format: date format\n",
    "    @param threshold: minimum age in days\n",
    "    @return: bool - whether the candidate is a proper birthdate or not\n",
    "    \"\"\"\n",
    "    if valid_date(candidate_date, format=format):\n",
    "        return pd.to_datetime(candidate_date, format=format) <= date.today() - timedelta(days=threshold)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def birthdate_db_notation(text):\n",
    "    \"\"\"\n",
    "    Converts potential birthdates to the proper db notation/date format: %d-%m-%Y. TODO: explain which cases should be\n",
    "    captured by this function.\n",
    "    @param text: string containing the text that needs to be processed\n",
    "    @return: original string with the potential birthdates in %d-%m-%Y format\n",
    "    \"\"\"\n",
    "    months = ['januari', 'jan', 'februari', 'feb', 'maart', 'mrt', 'april', 'apr', 'mei', 'juni', 'jun', 'juli',\n",
    "            'jul', 'augustus', 'aug', 'september', 'sep', 'oktober', 'okt', 'november', 'nov', 'december', 'dec']\n",
    "    gebdat_pattern = '[0-3]?[0-9](/|-|\\s)(' + '|'.join(months) + \\\n",
    "        '|[0-1]?[0-9])(/|-|\\s)(19|20)?[0-9]{2}'\n",
    "\n",
    "    def db_notate(match):\n",
    "        replace_dict = {\n",
    "            'januari': '1',\n",
    "            'jan': '1',\n",
    "            'februari': '2',\n",
    "            'feb': '2',\n",
    "            'maart': '3',\n",
    "            'mrt': '3',\n",
    "            'april': '4',\n",
    "            'apr': '4',\n",
    "            'mei': '5',\n",
    "            'juni': '6',\n",
    "            'jun': '6',\n",
    "            'juli': '7',\n",
    "            'jul': '7',\n",
    "            'augustus': '8',\n",
    "            'aug': '8',\n",
    "            'september': '9',\n",
    "            'sep': '9',\n",
    "            'sept': '9',\n",
    "            'oktober': '10',\n",
    "            'okt': '10',\n",
    "            'november': '11',\n",
    "            'nov': '11',\n",
    "            'december': '12',\n",
    "            'dec': '12'\n",
    "        }\n",
    "\n",
    "        candidate_date = re.sub('/|\\s', '-', match.group()).lower()\n",
    "        mon = re.search('[a-z]+', candidate_date)\n",
    "        if mon:\n",
    "            # string month to int month\n",
    "            candidate_date = re.sub(\n",
    "                mon.group(), replace_dict[mon.group()], candidate_date)\n",
    "        if re.fullmatch('[0-3]?[0-9]-[0-1]?[0-9]-[0-9]{2}', candidate_date):\n",
    "            # yy to yyyy\n",
    "            candidate_date = candidate_date[:-\n",
    "                                            2] + '19' + candidate_date[-2:]\n",
    "\n",
    "        if valid_birthdate(candidate_date):\n",
    "            return pd.to_datetime(candidate_date, format='%d-%m-%Y').strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            return match.group()\n",
    "\n",
    "    return re.sub(gebdat_pattern, db_notate, text, flags = re.IGNORECASE)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess text to match the database (label) notation\n",
    "    @param: text: Text that will be annotated\n",
    "    \"\"\"\n",
    "    preprocessed_text = remove_urls(text)\n",
    "    preprocessed_text = zipcode_db_notation(preprocessed_text)\n",
    "    preprocessed_text = birthdate_db_notation(preprocessed_text)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Entity Extractor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    \"\"\"\n",
    "    Extracts entities using the spacy model and regular expressions. Returns an entity dictionary with\n",
    "    entity_name: [value1, value2, ...] as output. For example: {\"postcode\": [\"1234AB\", \"5678CD\"], \"polis\": []}\n",
    "    \"\"\"\n",
    "    def __init__(self,  nlp_model, regex_extractors, prefix=None):\n",
    "        self.nlp_model = nlp_model\n",
    "        self.regex_extractors = regex_extractors\n",
    "        self.prefix = prefix\n",
    "\n",
    "        '''\n",
    "        Class which extracts entities with help of both regex extractors \n",
    "        and trained Named Entity Recognition model (spacy).\n",
    "        @param: nlp_model: trained spacy NER model \n",
    "        @param: regex_extractors: dictionary {type entity: regex function to extract this entity}\n",
    "        @param: prefix: prefix that needs to be applied for all found entity names (keys in the dictionary)\n",
    "\n",
    "        '''\n",
    "\n",
    "    def nlp_extract(self, text):\n",
    "        '''\n",
    "        Extracts entities from text with help of trained NER spacy model.\n",
    "        @param text: from which we want to extract entities with our trained spacy model\n",
    "        @return: dictionary {type entity: found entity/entities with help of spacy}\n",
    "        '''\n",
    "        d = {}\n",
    "        for label in self.nlp_model.pipe_labels['ner']:\n",
    "            d[label] = []\n",
    "        doc = self.nlp_model(text)\n",
    "        for ent in doc.ents:\n",
    "            if ent.text not in d[ent.label_]:\n",
    "                d[ent.label_].append(ent.text)\n",
    "        return d\n",
    "\n",
    "    def regex_extract(self, text):\n",
    "        '''\n",
    "        applies regex functions found in regex_extractors to text and maps them in dictionary.\n",
    "        @param text: from which we want to extract entities with regular expressions\n",
    "        @return: dictionary with {type entity: found entities with help of regex extractor}\n",
    "        '''\n",
    "        return {key: regextract_function(text) for key, regextract_function in self.regex_extractors.items()}\n",
    "\n",
    "    def create_ent_dict(self, text):\n",
    "        \"\"\"\n",
    "        Creates an entity dictionary for all entities. A prefix is applied to the keys to track which method was used\n",
    "        to extract the entity e.g. \"spacy_\" or \"_regex\"\n",
    "        @param text: text from which we want to extract entities with our spacy model and regular expressions\n",
    "        @return: dictionary containing all the found entities using the spacy model and regular expressions.\n",
    "        \"\"\"\n",
    "        preprocessed_text = preprocess(text)\n",
    "        spacy_ent_dict = self.nlp_extract(preprocessed_text)\n",
    "        spacy_ent_dict = self.apply_prefix(spacy_ent_dict, 'spacy_')\n",
    "        regex_ent_dict = self.regex_extract(preprocessed_text)\n",
    "        regex_ent_dict = self.apply_prefix(regex_ent_dict, 'regex_')\n",
    "        ent_dict = {**spacy_ent_dict, **regex_ent_dict}\n",
    "        if self.prefix:\n",
    "            ent_dict = self.apply_prefix(ent_dict, self.prefix)\n",
    "        return ent_dict\n",
    "\n",
    "    def apply_prefix(self, ent_dict, prefix):\n",
    "        \"\"\"\n",
    "        Returns the dictionary with found entities with the specified prefix. Used to track which method was used\n",
    "        to extract the entities\n",
    "        @param ent_dict: the dictionary with entities extracted\n",
    "        @param prefix: the prefix that needs to be applied for the keys e.g. \"spacy_\"\n",
    "        @return: dictionary with the prefix applied to the keys\n",
    "        \"\"\"\n",
    "        return {prefix + key: val for key, val in ent_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define regex extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_extractors = {\n",
    "    'polis': find_polis,\n",
    "    'postcode': find_postcode,\n",
    "    'rekeningnummer': find_iban,\n",
    "    'telefoon': find_phonenumber\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define custom MLFlow `PythonModel`\n",
    "\n",
    "As mentioned at the top of the notebook, we will use the `mlflow.pyfunc` utilities to define a class for a custom `PythonModel` that creates the desired entity dictionary output, before saving this custom model.\n",
    "\n",
    "We are doing this so that our MLFlow model will have the `pyfunc` flavour with the custom `predict` method as defined below. This flavour is expected by Seldon's MLFlow server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./spacy_body_model/\"\n",
    "\n",
    "# Define the model class\n",
    "class EntityDictionary(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.extractor = Extractor(\n",
    "            nlp_model=spacy.load(model_path),\n",
    "            regex_extractors=regex_extractors,\n",
    "            prefix='subject_')\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        logger.info(f\"Model input before indexing: {model_input}\")\n",
    "        str_payload = model_input[0]\n",
    "        logger.info(f\"Model input: {str_payload}\")\n",
    "        logger.info(f\"Model input type: {type(str_payload)}\")\n",
    "        ent_dict = self.extractor.create_ent_dict(str_payload)\n",
    "        logger.info(f\"Entity dict {ent_dict}\")\n",
    "        logger.info(f\"Entity dict type: {type(ent_dict)}\")\n",
    "\n",
    "        return [json.dumps(ent_dict)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create the custom model and use the `mlflow.pyfunc` utility `save_model()` to save our custom MLFlow `PythonModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = EntityDictionary()\n",
    "mlflow.pyfunc.save_model(path=\"spacy_pyfunc\", python_model=custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create custom conda environment\n",
    "\n",
    "Now we will create our custom conda environment and install the required packages into it for any extra dependencies required that are not know in advance by MLServer.\n",
    "\n",
    "Then we will run `conda-pack` our environment as a *tarball*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we can copy and paste the following into the `requirements.txt` file within the `spacy_pyfunc` model folder: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "mlserver==1.2.0.dev12\n",
    "mlserver-mlflow==1.2.0.dev12\n",
    "datetime\n",
    "pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to save the `requirements.txt` file afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a new conda environment, install the requirements and then `conda-pack` our environment as a *tarball*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create --name model-pack python=3.8.13 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "source ~/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate model-pack\n",
    "pip install -r ./spacy_pyfunc/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda pack -n model-pack -o ./spacy_pyfunc/environment.tar.gz -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. *Optional* - Test model locally using `mlserver`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our custom model and custom conda environment ready, we can test our model locally. \n",
    "\n",
    "In order to do this, we first need to create a `model-settings.json` file. This holds the configuration of our model (e.g. input type, runtime to use, etc):\n",
    "\n",
    "  ```\n",
    "  {\n",
    "      \"name\": \"nn-ee\",\n",
    "      \"implementation\": \"mlserver_mlflow.MLflowRuntime\",\n",
    "      \"parameters\": {\n",
    "          \"uri\": \"./spacy_pyfunc/\"\n",
    "      }\n",
    "  }\n",
    "  ```\n",
    "This should already be present in the folder containing this notebook.\n",
    "\n",
    "Then we can start the server by running:\n",
    "\n",
    "`mlserver start .`\n",
    "\n",
    "Note: This needs to either be run from the same directory where our `model-settings.json` file is or pointing to the folder where they are.\n",
    "\n",
    "Since this command will start the server and block the terminal, waiting for requests, this will need to be run in the background on a separate terminal.\n",
    "\n",
    "We can then test sending requests by running the following python script:\n",
    "\n",
    "```\n",
    "  import requests\n",
    "\n",
    "  inference_request = {\n",
    "      \"parameters\": {\n",
    "        \"content_type\": \"str\"\n",
    "      },\n",
    "      \"inputs\": [\n",
    "          {\n",
    "            \"name\": \"test\",\n",
    "            \"shape\": [1, 1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"Roeselare verkocht gemotiveerd student gevaarlijke eindeloze Koerdische luchthavens getal CERA uiteindelijke gesprekken overbodig losse achterop ongeval grafische belanden vergezeld witloof 1013ZK. steen ein VU buitenstaander alleen ongeval gepaard begeven file regelmaat Kennedy schuld vertrok dozen Ford Landen Nick voorbereiden vult benoemd 1013ZK. avec deel conservatief loonnorm ken huisvesting oceaan westen onvermijdelijk Wil schrijven mogelijke stamt muren gesprekken vult vorming Ludwig bezorgd bestemmingen 1013ZK. felle Elizabeth Express evenmin Buffett vult herhaald ontwerpers vertegenwoordigers strand Mobutu bedenken rekenen gÃ©Ã©n terechtgekomen hoort volumes authentieke wilden voorbereiden 1013ZK. westen afstand tweeduizend besliste Planet Hendrik onderschreven Finance menselijke conservatief nauwe verklaart constructeur Sam boeiende stamt CERA financier verwachte integratie 2596 CX\"]\n",
    "          }\n",
    "      ]\n",
    "  }\n",
    "\n",
    "  endpoint = \"http://localhost:8080/v2/models/nn-ee/infer\"\n",
    "  response = requests.post(endpoint, json=inference_request)\n",
    "\n",
    "  print(response.json())\n",
    "\n",
    "```\n",
    "\n",
    "Again, this should already be present in the folder containing this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save MLFlow model to GCS\n",
    "\n",
    "Finally we can save our custom spaCy MLFlow `PythonModel` to GCS ready for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r ./spacy_pyfunc/ gs://andrew-seldon/sandbox/spacy_pyfunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy MLFlow Model with Seldon Deploy\n",
    "\n",
    "We can now deploy our models to the dedicated Seldon Deploy cluster that we have configured for this workshop. To do this, we will interact with the Seldon Deploy SDK. You can find the reference documentation [here](https://github.com/SeldonIO/seldon-deploy-sdk/blob/master/python/README.md).\n",
    "\n",
    "Firstly, we need to set up the configuration and authentication required to access the cluster. \n",
    "\n",
    "⚠️ Make sure to fill in the following in the below cell if not already filled in: \n",
    "\n",
    "1) `YOUR_NAME` variable - NO capitals or underscores, but lower case characters and hyphens are fine. \n",
    "2) `SD_DOM` variable - Ensure to change this to the domain name of the cluster you are using. \n",
    "3) `CLIENT_SECRET` variable - this will either be `sd-api-secret` or an alternative secret provided during the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = \"\"\n",
    "SD_DOM = \"\"\n",
    "CLIENT_SECRET = \"\"\n",
    "\n",
    "config = Configuration()\n",
    "config.host = f\"https://{SD_DOM}/seldon-deploy/api/v1alpha1\"\n",
    "config.oidc_client_id = \"sd-api\"\n",
    "config.oidc_server = f\"https://{SD_DOM}/auth/realms/deploy-realm\"\n",
    "config.oidc_client_secret = f\"{CLIENT_SECRET}\"\n",
    "config.auth_method = \"auth_code\"\n",
    "\n",
    "def auth():\n",
    "    auth = OIDCAuthenticator(config)\n",
    "    config.id_token = auth.authenticate()\n",
    "    api_client = ApiClient(configuration=config, authenticator=auth)\n",
    "    return api_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have configured the domain name correctly, as well as setup the authentication function, we can create the manifest for the deployment we would like to create. This is defined as a `SeldonDeployment` custom resource.\n",
    "\n",
    "Notice in the deployment manifest, we have specificed `protocol: v2`. MLServer will then be the inference server and requests and responses will be compliant with the V2 Inference Protocol spec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_NAME = f\"{YOUR_NAME}-spacy\"\n",
    "NAMESPACE = \"seldon-gitops\"\n",
    "MODEL_LOCATION = f\"gs://andrew-seldon/sandbox/spacy_pyfunc\"\n",
    "\n",
    "\n",
    "mldeployment = {\n",
    "  \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "  \"kind\": \"SeldonDeployment\",\n",
    "  \"metadata\": {\n",
    "    \"name\": f\"{DEPLOYMENT_NAME}\",\n",
    "    \"namespace\": f\"{NAMESPACE}\"\n",
    "  },\n",
    "  \"spec\": {\n",
    "    \"protocol\": \"v2\",\n",
    "    \"name\": f\"{DEPLOYMENT_NAME}\",\n",
    "    \"predictors\": [\n",
    "      {\n",
    "        \"graph\": {\n",
    "          \"children\": [],\n",
    "          \"implementation\": \"MLFLOW_SERVER\",\n",
    "          \"modelUri\": f\"{MODEL_LOCATION}\",\n",
    "          \"name\": f\"{DEPLOYMENT_NAME}-container\"\n",
    "        },\n",
    "        \"name\": \"default\",\n",
    "        \"replicas\": 1\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now invoke the `SeldonDeploymentsApi` and create a new `SeldonDeployment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access the Seldon Deploy cluster and view our freshly created deployments using the link below.\n",
    "\n",
    "⚠️ Make sure to replace the `XXXXX` in the URL below with the cluster domain name. \n",
    "\n",
    "http://XXXXX/seldon-deploy/\n",
    "\n",
    "The username and password for accessing the cluster will be shared during the session. \n",
    "\n",
    "We can test sending a request to our model running in production. We can do this through the `Predict` pane in the Seldon Deploy UI by pasting in the following json: \n",
    "\n",
    "```\n",
    "{\n",
    "      \"parameters\": {\n",
    "        \"content_type\": \"str\"\n",
    "      },\n",
    "      \"inputs\": [\n",
    "          {\n",
    "            \"name\": \"test\",\n",
    "            \"shape\": [1, 1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"Roeselare verkocht gemotiveerd student gevaarlijke eindeloze Koerdische luchthavens getal CERA uiteindelijke gesprekken overbodig losse achterop ongeval grafische belanden vergezeld witloof 1013ZK. steen ein VU buitenstaander alleen ongeval gepaard begeven file regelmaat Kennedy schuld vertrok dozen Ford Landen Nick voorbereiden vult benoemd 1013ZK. avec deel conservatief loonnorm ken huisvesting oceaan westen onvermijdelijk Wil schrijven mogelijke stamt muren gesprekken vult vorming Ludwig bezorgd bestemmingen 1013ZK. felle Elizabeth Express evenmin Buffett vult herhaald ontwerpers vertegenwoordigers strand Mobutu bedenken rekenen gÃ©Ã©n terechtgekomen hoort volumes authentieke wilden voorbereiden 1013ZK. westen afstand tweeduizend besliste Planet Hendrik onderschreven Finance menselijke conservatief nauwe verklaart constructeur Sam boeiende stamt CERA financier verwachte integratie 2596 CX\"]\n",
    "          }\n",
    "      ]\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Add metadata and prediction schema to deployed MLFlow model \n",
    "\n",
    "Seldon Deploy has a model catalogue where all deployed models are automatically registered. The model catalogue can store custom metadata as well as prediction schemas for your models. \n",
    "\n",
    "Metadata promotes lineage from across different machine learning systems, aids kmowledge transfer between teams, and allows for faster deployment. Meanwhile, prediction schemas allow Seldon Deploy to automatically profile tabular data into histograms, allowing for filtering on features to explore trends. \n",
    "\n",
    "In order to effectively construct a prediciton schema, Seldon has the [ML Prediction Schema project](https://github.com/SeldonIO/ml-prediction-schema). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_schema = {\n",
    "  \"requests\": [\n",
    "    {\n",
    "      \"name\": \"Input text\",\n",
    "      \"type\": \"TEXT\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"nCategories\": \"0\",\n",
    "      \"categoryMap\": {},\n",
    "      \"schema\": [],\n",
    "      \"shape\": []\n",
    "    }\n",
    "  ],\n",
    "  \"responses\": [\n",
    "    {\n",
    "      \"name\": \"Entity Dictionary\",\n",
    "      \"type\": \"TEXT\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"nCategories\": \"0\",\n",
    "      \"categoryMap\": {},\n",
    "      \"schema\": [],\n",
    "      \"shape\": []\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then add the prediction schema to the wider model catalogue metadata. This includes information such as the model storage locaton, the name, the version, the artefact type etc. The metadata tags and metrics that can be associated with a model are freeform and can therefore be determined based upon the use case which is being developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_catalog_metadata = {\n",
    "      \"URI\": MODEL_LOCATION,\n",
    "      \"name\": f\"{DEPLOYMENT_NAME}-model\",\n",
    "      \"version\": \"v1.0\",\n",
    "      \"artifactType\": \"MLFLOW\",\n",
    "      \"taskType\": \"Entity Extraction\",\n",
    "      \"tags\": {\n",
    "        \"auto_created\": \"true\",\n",
    "        \"author\": f\"{YOUR_NAME}\"\n",
    "      },\n",
    "      \"project\": \"default\",\n",
    "      \"prediction_schema\": prediction_schema\n",
    "    }\n",
    "\n",
    "model_catalog_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using the `ModelMetadataServiceApi`, we can add this to the model which we have just created in Seldon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_api = ModelMetadataServiceApi(auth())\n",
    "metadata_api.model_metadata_service_update_model_metadata(model_catalog_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configure Maximum Mean Discrepancy (MMD) Drift Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although powerful, modern machine learning models can be sensitive. Seemingly subtle changes in a data distribution can destroy the performance of otherwise state-of-the art models, which can be especially problematic when ML models are deployed in production. \n",
    "\n",
    "Drift can be classified into the following types:\n",
    "\n",
    "- **Covariate drift**: Also referred to as input drift, this occurs when the distribution of the input data has shifted `P(X) != Pref(X)`, whilst `P(Y|X) = Pref(Y|X)`. This may result in the model giving unreliable predictions.\n",
    "\n",
    "- **Prior drift**: Also referred to as label drift, this occurs when the distribution of the outputs has shifted `P(Y) != Pref(Y)`, whilst `P(X|Y) = Pref(X|Y)`. This can affect the model’s decision boundary, as well as the model’s performance metrics.\n",
    "\n",
    "- **Concept drift**: This occurs when the process generating `Y` from `X` has changed, such that `P(Y|X) != Pref(Y|X)`. It is possible that the model might no longer give a suitable approximation of the true process.\n",
    "\n",
    "---\n",
    "\n",
    "In this example we will use the Maximum Mean Discrepancy (MMD) method for Drift Detection. Covariate or input drift detection relies on creating a distance measure between two distributions; a reference distribution and a new distribution. The MMD drift detector is no different; the mean embeddings of your features are used to generate the distributions and then the distance between them is measured. The training data is used to calculate the reference distribution, while the new distribution comes from your inference data. More information on the Maximum Mean Discrepancy (MMD) detector can be found in the alibi detect documentation [here](https://docs.seldon.io/projects/alibi-detect/en/stable/cd/methods/mmddrift.html).\n",
    "\n",
    "Firstly we will define and load a tokenizer. We will use `BERTje` (`GroNLP/bert-base-dutch-cased` as our hugging face transformer here. More information can be found [here](https://huggingface.co/GroNLP/bert-base-dutch-cased#benchmarks). `BERTje` is a Ducth pre-trained BERT model developed at the University of Groningen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"GroNLP/bert-base-dutch-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crucially, the pre-processing which we perform when feeding data to our drift detector does not necessarily have to match the pre-processing being used by the model. This means that the embedding method which generates the best results for the model and the drift detector can be controlled independently of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_type = 'hidden_state'\n",
    "n_layers = 8\n",
    "layers = [-_ for _ in range(1, n_layers + 1)]\n",
    "\n",
    "embedding = TransformerEmbedding(model_name, emb_type, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(list(x_ref), padding=True, return_tensors='tf')\n",
    "x_emb = embedding(tokens)\n",
    "print(x_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alibi Detect then allows us to construct a preprocessing function by bringing together each of these different components into a single function which can be readily serialised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_fn = partial(preprocess_drift, model=embedding, tokenizer=tokenizer, max_len=512, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finally we can fit the MMD detector on our reference data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = MMDDrift(x_ref, p_val=.05, preprocess_fn=preprocess_fn, input_shape=(512,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then observe whether drift if flagged on 2 different batches of data: \n",
    "\n",
    "- `batch_0` containing the first 100 data points from the reference data set. \n",
    "- `batch_1` containing a single example repeated 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_0 = x_ref[:100]\n",
    "batch_1 = [x_ref[0]] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cd = cd.predict(batch_0)\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_cd['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_cd['data']['p_val']))\n",
    "print('Drift Distance: {}'.format(preds_cd['data']['distance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds_cd = cd.predict(batch_1)\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_cd['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_cd['data']['p_val']))\n",
    "print('Drift Distance: {}'.format(preds_cd['data']['distance']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will save our drift detectors to a Google Storage bucket in subfolders corresponding to the `YOUR_NAME` variable. We can then deploy our drift detectors direcly from object storage with Seldon Deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_detector(cd, \"ee-dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r ee-dd gs://kelly-seldon/entity-extractor/dd/{YOUR_NAME}/ee-dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Deploy Drift Detector with Seldon Deploy\n",
    "\n",
    "Finally, we can use the Seldon Deploy SDK to deploy our newly configured drift detector. We can define the config for the drift detector and then call the `DriftDetectorApi` to create the drift detector seldon deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_URI = f\"gs://kelly-seldon/entity-extractor/dd/{YOUR_NAME}/ee-dd/\"\n",
    "DD_NAME = \"ee-dd\"\n",
    "\n",
    "dd_config = {'config': {'basic': \n",
    "                        {'drift_batch_size': '5',\n",
    "                         'storage_uri': DD_URI},\n",
    "                        'deployment': {'protocol': 'kfserving.http'}\n",
    "                        },\n",
    "             'deployment_name': DEPLOYMENT_NAME,\n",
    "             'detector_type': 'drift',\n",
    "             'name': DD_NAME,\n",
    "             'namespace': NAMESPACE\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_api = DriftDetectorApi(auth())\n",
    "dd_api.create_drift_detector_seldon_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE, detector_data=dd_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f5fe526981bc5449ca7eb30ee5d12e6bfee641ca2b5c801227fa92bac28d7b43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
