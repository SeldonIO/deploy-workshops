{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20eec117",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-15T07:48:52.44714Z",
     "iopub.status.busy": "2021-11-15T07:48:52.446721Z",
     "iopub.status.idle": "2021-11-15T07:48:52.478971Z",
     "shell.execute_reply": "2021-11-15T07:48:52.4775Z",
     "shell.execute_reply.started": "2021-11-15T07:48:52.447038Z"
    },
    "papermill": {
     "duration": 0.032152,
     "end_time": "2021-11-15T08:54:16.800827",
     "exception": false,
     "start_time": "2021-11-15T08:54:16.768675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hands-On NLP Workshop: *Detecting Sentiment Using Tweets to US Airlines*\n",
    "\n",
    "Within this hands-on workshop you will analyse tweets from customers of airlines about their performance. These were scraped from Twitter in 2015, and will be categorised in positive, neutral or negative sentiment. \n",
    "  \n",
    "The steps which have been carried out\n",
    "1. Load The Data \n",
    "2. Data Visualization \n",
    "3. Text Preprocessing and Cleaning  \n",
    "4. Handling Imbalance         \n",
    "5. Model Building  \n",
    "\n",
    "The code in this notebook has been adapted from the brilliant work done by [Meisam Raz](https://www.kaggle.com/meisamraz/sentiment-analysis-96-acc-eda-text-preprocessing/notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974c403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Colab has a load of packages pre-loaded into the environment. Installing the additional ones we require here.\n",
    "!pip install seldon-deploy-sdk==1.4.1\n",
    "!pip install alibi-detect==0.6.2\n",
    "!pip install alibi==0.6.2\n",
    "!pip install wordcloud\n",
    "!pip install seaborn\n",
    "!pip install dill\n",
    "!pip install imblearn\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9522f",
   "metadata": {
    "papermill": {
     "duration": 2.027968,
     "end_time": "2021-11-15T08:54:18.918059",
     "exception": false,
     "start_time": "2021-11-15T08:54:16.890091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic Operation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text Preprocessing & Cleaning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split # Split Data \n",
    "from imblearn.over_sampling import SMOTE # Handling Imbalanced\n",
    "\n",
    "# Model Building\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report , confusion_matrix , accuracy_score # Performance Metrics  \n",
    "\n",
    "# Drift Detection\n",
    "from alibi_detect.cd import KSDrift\n",
    "from alibi_detect.utils.saving import save_detector, load_detector\n",
    "\n",
    "# Explainabilty\n",
    "from alibi.explainers import AnchorText\n",
    "import spacy\n",
    "from alibi.utils.download import spacy_model\n",
    "\n",
    "# Data Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from termcolor import cprint\n",
    "import seaborn as sns\n",
    "import warnings   \n",
    "\n",
    "# Serialising artefacts\n",
    "import joblib\n",
    "import dill\n",
    "\n",
    "# Deploy SDK \n",
    "from seldon_deploy_sdk import Configuration, ApiClient, SeldonDeploymentsApi, OutlierDetectorApi, DriftDetectorApi\n",
    "from seldon_deploy_sdk.auth import OIDCAuthenticator\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bb6a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/seldon-koz-data/airline/airline.zip\n",
    "!unzip -o 'airline.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070e25d",
   "metadata": {
    "papermill": {
     "duration": 0.029596,
     "end_time": "2021-11-15T08:54:18.978151",
     "exception": false,
     "start_time": "2021-11-15T08:54:18.948555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "Load the data into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb0084",
   "metadata": {
    "papermill": {
     "duration": 0.158843,
     "end_time": "2021-11-15T08:54:19.166751",
     "exception": false,
     "start_time": "2021-11-15T08:54:19.007908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a17210",
   "metadata": {},
   "source": [
    "Inspecting the columns within the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c127e19",
   "metadata": {
    "papermill": {
     "duration": 0.038976,
     "end_time": "2021-11-15T08:54:19.322909",
     "exception": false,
     "start_time": "2021-11-15T08:54:19.283933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798bf82",
   "metadata": {},
   "source": [
    "The dataset is 14,460 separate tweets to various US airlines which have been labelled according to their percevied sentiment. There is additional metadata associated with each tweet such as a location of where the tweet was created, the retweet count and the user name of the tweeter. \n",
    "\n",
    "Next we can begin to explore and visualise the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edf684",
   "metadata": {
    "papermill": {
     "duration": 0.229076,
     "end_time": "2021-11-15T08:54:20.451714",
     "exception": false,
     "start_time": "2021-11-15T08:54:20.222638",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cprint(\"Number of tweets in each category:\",'green')\n",
    "print(df[\"airline_sentiment\"].value_counts())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.countplot(x='airline_sentiment', data=df, palette='pastel')\n",
    "ax.set_title(label='Number of tweets in each category', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4dfda2",
   "metadata": {},
   "source": [
    "Who would have thought? People turn to Twitter when they're annoyed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105da75",
   "metadata": {
    "papermill": {
     "duration": 0.288513,
     "end_time": "2021-11-15T08:54:20.773727",
     "exception": false,
     "start_time": "2021-11-15T08:54:20.485214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cprint(\"Total number of tweets for each airline:\",'green')\n",
    "print(df.groupby('airline')['airline_sentiment'].count())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.countplot(x='airline', data=df, palette='pastel')\n",
    "ax.set_title(label='Total number of tweets for each airline', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b1d74",
   "metadata": {
    "papermill": {
     "duration": 0.323079,
     "end_time": "2021-11-15T08:54:21.131089",
     "exception": false,
     "start_time": "2021-11-15T08:54:20.808010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 10))\n",
    "sns.countplot(x='negativereason', data=df, palette='hls')\n",
    "plt.title('Reasons for Negative Tweets', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a6825",
   "metadata": {},
   "source": [
    "So at first glance it appears that customer service issues are the main driver of negatively sentimented tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaca8b8",
   "metadata": {
    "papermill": {
     "duration": 1.224283,
     "end_time": "2021-11-15T08:54:22.392408",
     "exception": false,
     "start_time": "2021-11-15T08:54:21.168125",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "NR_Count=df['negativereason'].value_counts()\n",
    "def NCount(Airline):\n",
    "    airlineName =df[df['airline']==Airline]\n",
    "    count= airlineName['negativereason'].value_counts()\n",
    "    Unique_reason= df['negativereason'].unique()\n",
    "    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n",
    "    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n",
    "    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n",
    "    return Reason_frame\n",
    "\n",
    "def plot_reason(airline):\n",
    "    a= NCount(airline)\n",
    "    count=a['count']\n",
    "    Id = range(1,(len(a)+1))\n",
    "    plt.bar(Id,count, color=['darkviolet','yellow','blue','lime','pink','crimson','gold','cyan','orange','purple'])\n",
    "    plt.xticks(Id,a['Reasons'],rotation=90)\n",
    "    plt.title(airline)\n",
    "\n",
    "airlines= ['US Airways','United','American','Southwest','Delta','Virgin America']\n",
    "    \n",
    "plt.figure(2,figsize=(13, 13))\n",
    "for i in airlines:\n",
    "    indices= airlines.index(i)\n",
    "    plt.subplot(2,3,indices+1)\n",
    "    plt.subplots_adjust(hspace=0.9)\n",
    "    plot_reason(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da2d3a",
   "metadata": {},
   "source": [
    "Breaking that down by airline, we can see that Delta is the only airline whose most frequent issue is late flights. All of the other providers suffer from customer service issues as their leading cause of negative tweets. \n",
    "\n",
    "Finally, we can generate word clouds to observe the most common words within each sentiment category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee504b6",
   "metadata": {
    "papermill": {
     "duration": 0.053641,
     "end_time": "2021-11-15T08:54:22.484599",
     "exception": false,
     "start_time": "2021-11-15T08:54:22.430958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split text of Sentiments    \n",
    "positive = df[df['airline_sentiment'] == 'positive'].text\n",
    "neutral = df[df['airline_sentiment'] == 'neutral'].text\n",
    "negative = df[df['airline_sentiment'] == 'negative'].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce926d",
   "metadata": {
    "papermill": {
     "duration": 14.596571,
     "end_time": "2021-11-15T08:54:37.118795",
     "exception": false,
     "start_time": "2021-11-15T08:54:22.522224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word cloud of positive sentiments\n",
    "plt.figure(figsize = (20,20)) \n",
    "worldcould_pos = WordCloud(min_font_size=3,\n",
    "                           max_words=1000, \n",
    "                           width=1600,\n",
    "                           height=680).generate(\" \".join(positive))\n",
    "\n",
    "plt.imshow(worldcould_pos,interpolation='bilinear')\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb8f3d",
   "metadata": {
    "papermill": {
     "duration": 18.741769,
     "end_time": "2021-11-15T08:54:55.916434",
     "exception": false,
     "start_time": "2021-11-15T08:54:37.174665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word cloud of neutral sentiments\n",
    "plt.figure(figsize = (20,20)) \n",
    "worldcould_neutral = WordCloud(min_font_size=3,  \n",
    "                               max_words=1000,\n",
    "                               width=1600,\n",
    "                               height=680).generate(\" \".join(neutral))\n",
    "\n",
    "plt.imshow(worldcould_neutral,interpolation='bilinear')\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aa493c",
   "metadata": {
    "papermill": {
     "duration": 25.446856,
     "end_time": "2021-11-15T08:55:21.441573",
     "exception": false,
     "start_time": "2021-11-15T08:54:55.994717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word cloud of negative sentiments\n",
    "plt.figure(figsize = (20,20)) \n",
    "worldcould_neg = WordCloud(min_font_size=3,\n",
    "                           max_words=1000, \n",
    "                           width=1600, \n",
    "                           height=680).generate(\" \".join(negative))\n",
    "\n",
    "plt.imshow(worldcould_neg,interpolation = 'bilinear')\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55fe4a",
   "metadata": {
    "papermill": {
     "duration": 0.103814,
     "end_time": "2021-11-15T08:55:21.649180",
     "exception": false,
     "start_time": "2021-11-15T08:55:21.545366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Tweets are inherently messy for ML models to interpret; emojis, punctuation, usernames all need to be removed and prior to being embedded and used for training. \n",
    "\n",
    "This section of the notebook tackles the data preparation and cleaning required to wrangle the dataset into a training-ready state. \n",
    "\n",
    "Firstly, we prepare the labels by converting the categories to integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0cab57",
   "metadata": {
    "papermill": {
     "duration": 0.11074,
     "end_time": "2021-11-15T08:55:21.864153",
     "exception": false,
     "start_time": "2021-11-15T08:55:21.753413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert Sentiments to 0,1,2\n",
    "def convert_sentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 2\n",
    "    elif sentiment == \"neutral\":\n",
    "        return 1\n",
    "    elif sentiment == \"negative\":\n",
    "        return 0\n",
    "    \n",
    "# Apply convert_sentiment function\n",
    "df[\"airline_sentiment\"] = df[\"airline_sentiment\"].apply(lambda x : convert_sentiment(x))\n",
    "df[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4edfea",
   "metadata": {},
   "source": [
    "Next we then perform a number of data cleaning and standardisation steps. This ensures that there are no quirky characters or usernames fed to our model later down the line which would cause issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4b8b7",
   "metadata": {
    "papermill": {
     "duration": 29.790366,
     "end_time": "2021-11-15T08:55:52.201738",
     "exception": false,
     "start_time": "2021-11-15T08:55:22.411372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in (stopwords.words('english'))])\n",
    "    return text\n",
    "\n",
    "# Remove url  \n",
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "# Remove punct\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "# Remove html \n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "# Remove @username\n",
    "def remove_username(text):\n",
    "    return re.sub('@[^\\s]+','',text)\n",
    "\n",
    "# Remove emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "# Decontraction text\n",
    "def decontraction(text):\n",
    "    text = re.sub(r\"won\\'t\", \" will not\", text)\n",
    "    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n",
    "    text = re.sub(r\"can\\'t\", \" can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \" do not\", text)\n",
    "    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n",
    "    text = re.sub(r\"ma\\'am\", \" madam\", text)\n",
    "    text = re.sub(r\"let\\'s\", \" let us\", text)\n",
    "    text = re.sub(r\"ain\\'t\", \" am not\", text)\n",
    "    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n",
    "    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n",
    "    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n",
    "    text = re.sub(r\"y\\'all\", \" you all\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"n\\'t've\", \" not have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'d've\", \" would have\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ll've\", \" will have\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    return text  \n",
    "\n",
    "# Seperate alphanumeric\n",
    "def seperate_alphanumeric(text):\n",
    "    words = text\n",
    "    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def cont_rep_char(text):\n",
    "    tchr = text.group(0) \n",
    "    \n",
    "    if len(tchr) > 1:\n",
    "        return tchr[0:2] \n",
    "\n",
    "def unique_char(rep, text):\n",
    "    substitute = re.sub(r'(\\w)\\1+', rep, text)\n",
    "    return substitute\n",
    "\n",
    "def char(text):\n",
    "    substitute = re.sub(r'[^a-zA-Z]',' ',text)\n",
    "    return substitute\n",
    "\n",
    "# Combine negative reason with tweet (if it exists)\n",
    "df['final_text'] = df['negativereason'].fillna('') + ' ' + df['text'] \n",
    "\n",
    "# Apply functions to tweets\n",
    "df['final_text'] = df['final_text'].apply(lambda x: remove_username(x))\n",
    "df['final_text'] = df['final_text'].apply(lambda x: remove_url(x))\n",
    "df['final_text'] = df['final_text'].apply(lambda x: remove_emoji(x))\n",
    "df['final_text'] = df['final_text'].apply(lambda x: decontraction(x))\n",
    "df['final_text'] = df['final_text'].apply(lambda x: seperate_alphanumeric(x))\n",
    "df['final_text'] = df['final_text'].apply(lambda x: unique_char(cont_rep_char,x))\n",
    "df['final_text'] = df['final_text'].apply(lambda x: char(x))\n",
    "df['final_text'] = df['final_text'].apply(lambda x: x.lower())\n",
    "df['final_text'] = df['final_text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# End result\n",
    "df['final_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2cf4c2",
   "metadata": {},
   "source": [
    "Setting the X and y datasets ready for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d98b5dd",
   "metadata": {
    "papermill": {
     "duration": 0.113587,
     "end_time": "2021-11-15T08:55:52.637907",
     "exception": false,
     "start_time": "2021-11-15T08:55:52.524320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df['final_text']\n",
    "y = df['airline_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e08ead",
   "metadata": {},
   "source": [
    "We then embed the tweets using the TF-IDF vectorizer. This is an industry standard embedding method and provides us with an array for each tweet which can then be used for training a range of ML algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9fe6e",
   "metadata": {
    "papermill": {
     "duration": 0.286932,
     "end_time": "2021-11-15T08:55:53.028524",
     "exception": false,
     "start_time": "2021-11-15T08:55:52.741592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_final =  tfidf.fit_transform(X)\n",
    "X_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0e7eb",
   "metadata": {
    "papermill": {
     "duration": 0.103698,
     "end_time": "2021-11-15T08:55:53.237132",
     "exception": false,
     "start_time": "2021-11-15T08:55:53.133434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Due to the imbalance in the dataset, with there being 3 times as many negatively sentimented tweets as any other category we use SMOTE to resample the minority classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074603b0",
   "metadata": {
    "papermill": {
     "duration": 0.596374,
     "end_time": "2021-11-15T08:55:53.937794",
     "exception": false,
     "start_time": "2021-11-15T08:55:53.341420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "x_sm,y_sm = smote.fit_resample(X_final,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f07668",
   "metadata": {},
   "source": [
    "Finally, we split the data into training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc981371",
   "metadata": {
    "papermill": {
     "duration": 0.119667,
     "end_time": "2021-11-15T08:55:54.162350",
     "exception": false,
     "start_time": "2021-11-15T08:55:54.042683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(x_sm , y_sm , test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136c037",
   "metadata": {
    "papermill": {
     "duration": 0.104987,
     "end_time": "2021-11-15T08:55:54.371587",
     "exception": false,
     "start_time": "2021-11-15T08:55:54.266600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training\n",
    "\n",
    "Once the data has been prepared we are ready to train our ML model. \n",
    "\n",
    "In this case we will use a RandomForestClassifier from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5864054",
   "metadata": {
    "papermill": {
     "duration": 35.283075,
     "end_time": "2021-11-15T08:56:29.760402",
     "exception": false,
     "start_time": "2021-11-15T08:55:54.477327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed28e7",
   "metadata": {
    "papermill": {
     "duration": 0.523926,
     "end_time": "2021-11-15T08:56:30.388872",
     "exception": false,
     "start_time": "2021-11-15T08:56:29.864946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_prediction =  rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165c266",
   "metadata": {
    "papermill": {
     "duration": 0.122419,
     "end_time": "2021-11-15T08:56:30.622007",
     "exception": false,
     "start_time": "2021-11-15T08:56:30.499588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score(rf_prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c6bc9",
   "metadata": {
    "papermill": {
     "duration": 0.108862,
     "end_time": "2021-11-15T08:58:47.165457",
     "exception": false,
     "start_time": "2021-11-15T08:58:47.056595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that we have successfully generated our RandomForestClassifier we can generate a classification report, and visualise a confusion matrix to gain greater insights into our model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12107597",
   "metadata": {
    "papermill": {
     "duration": 0.187788,
     "end_time": "2021-11-15T08:58:47.463191",
     "exception": false,
     "start_time": "2021-11-15T08:58:47.275403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cr = classification_report(y_test, rf_prediction)\n",
    "\n",
    "print(\"Classification Report:\\n----------------------\\n\", cr)\n",
    "cm = confusion_matrix(y_test,rf_prediction)\n",
    "\n",
    "# plot confusion matrix \n",
    "plt.figure(figsize=(8,6))\n",
    "sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n",
    "            xticklabels=sentiment_classes,\n",
    "            yticklabels=sentiment_classes)\n",
    "plt.title('Confusion matrix', fontsize=16)\n",
    "plt.xlabel('Actual label', fontsize=12)\n",
    "plt.ylabel('Predicted label', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f045f80",
   "metadata": {},
   "source": [
    "## Deploying the Model\n",
    "As we have seen in the previous sections the tweets are pre-processed using a variety of techniques. In order to account for this we have 2 options for how to account for the pre-processing logic in production:\n",
    "1. **Custom Model:** Incorporate the pre-processing directly in the `predict` method of a custom model. This provides simplicity when creating the deployment as there is only a single code base to worry about and a single component to be deployed.\n",
    "2. **Input Transformer:** Make use of a separate container to perform all of the input transformation and then pass the vectors to the model for prediction. The schematic below outlines how this would work. \n",
    "```\n",
    "            ________________________________________\n",
    "            |            SeldonDeployment          |\n",
    "            |                                      |\n",
    "Request -->  Input transformer   -->     Model --> Response\n",
    "            |  (Pre-processing)          (SKLearn) |\n",
    "            |______________________________________|\n",
    "```\n",
    "The use of an input transformer allows us to separate the pre-processing logic from the prediction logic. This means we can leverage the pre-packaged SKLearn server provided by Seldon to serve our model, and each of the components can be upgraded independently of one another. However, it does introduce additional complexity in the deployment which is generated, and how that then interacts with advanced monitoring components such as outlier and drift detectors. \n",
    "\n",
    "This workshop will focus on the generation of a **custom model for this case**, therefore we need to define an `__init__` and `predict` method which shall load and perform inference respectively in our new deployment. \n",
    "\n",
    "---------\n",
    "\n",
    "First we will save our model using joblib- SKLearn's recommended serialisation technology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154dbf39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(rf, 'model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842aa11",
   "metadata": {},
   "source": [
    "We also need to serialise our TF-IDF vectorizer in order to make use of it at runtime. This will ensure that we are using the same embeddings our model relyed upon at training time when inferring against new unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a687828",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tfidf, 'tfidf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b0325",
   "metadata": {},
   "source": [
    "We can now copy the models over to GCS where they can be downloaded from later when we come to deploy. \n",
    "\n",
    "!!! Remember to replace your `<YOUR NAME>` with your name. !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf19411",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp model.joblib gs://tom-seldon-examples/nlp-workshop/<YOUR NAME>/model.joblib\n",
    "!gsutil cp tfidf.joblib gs://tom-seldon-examples/nlp-workshop/<YOUR NAME>/tfidf.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa02e77",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We then define our Seldon custom model. The component parts required to build the custom model are outlined below. Each of the files play a key part in building the eventual Seldon docker container.\n",
    "\n",
    "---\n",
    "### TweetSentiment.py\n",
    "This is the critical file as it contains the logic associated with the deployment wrapped as part of a class by the same name as the Python file. \n",
    "\n",
    "A key thing to note about the way this has been structured is that we have focused on making this deployment reusable. The `__init__` method accepts two custom predictor parameters; one for the saved model (`model_path`), and the other for the TF-IDF vectorizer (`tfidf_path`). \n",
    "\n",
    "The advantage of this is that it allows us to upgrade the model or vectorizer without having to re-build the container image. Additionally, if the logic was more general it could be used to accept a wider variety of objects for greater reusability. \n",
    "\n",
    "```\n",
    "from joblib import load\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seldon_core\n",
    "import os\n",
    "\n",
    "# For downloading the model and OHE encoder from GCS\n",
    "from io import BytesIO\n",
    "from google.cloud import storage\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\", download_dir=\"./nltk\")\n",
    "nltk.data.path.append(\"./nltk\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TweetSentiment(object):\n",
    "\n",
    "    def __init__(self, model_path, tfidf_path):\n",
    "        logger.info(f\"Connecting to GCS\")\n",
    "        self.client = storage.Client.create_anonymous_client()\n",
    "        self.bucket = self.client.bucket('tom-seldon-examples')\n",
    "\n",
    "        logger.info(f\"Model name: {model_path}\")\n",
    "        self.model_path = model_path\n",
    "\n",
    "        logger.info(f\"TF-IDF Name: {tfidf_path}\")\n",
    "        self.tfidf_path = tfidf_path\n",
    "\n",
    "        logger.info(\"Loading model file and TF-IDF vectorizer.\")\n",
    "        self.load_deployment_artefacts()\n",
    "        self.ready = False\n",
    "\n",
    "    def load_deployment_artefacts(self):\n",
    "        logger.info(\"Loading model\")\n",
    "        model_file = BytesIO()\n",
    "        model_blob = self.bucket.get_blob(f'{self.model_path}')\n",
    "        model_blob.download_to_file(model_file)\n",
    "        self.model = load(model_file)\n",
    "\n",
    "        logger.info(\"Loading TF-IDF vectorizer\")\n",
    "        tfidf_file = BytesIO()\n",
    "        tfidf_blob = self.bucket.get_blob(f'{self.tfidf_path}')\n",
    "        tfidf_blob.download_to_file(tfidf_file)\n",
    "        self.tfidf = load(tfidf_file)\n",
    "        \n",
    "        self.ready = True\n",
    "\n",
    "    # Remove stop words\n",
    "    def remove_stopwords(self, text):\n",
    "        text = ' '.join([word for word in text.split() if word not in (stopwords.words('english'))])\n",
    "        return text\n",
    "\n",
    "    ...\n",
    "    \n",
    "    def char(self, text):\n",
    "        substitute = re.sub(r'[^a-zA-Z]',' ',text)\n",
    "        return substitute\n",
    "\n",
    "    def predict(self, tweets, names=[], meta={}):\n",
    "        try:\n",
    "            if not self.ready:\n",
    "                self.load_deployment_artefacts()\n",
    "            else:\n",
    "                final_text = []\n",
    "\n",
    "                for text in tweets:\n",
    "                    # Apply functions to tweets\n",
    "                    text = self.remove_username(text)\n",
    "                    text = self.remove_url(text)\n",
    "                    text = self.remove_emoji(text)\n",
    "                    text = self.decontraction(text)\n",
    "                    text = self.seperate_alphanumeric(text)\n",
    "                    text = self.unique_char(self.cont_rep_char,text)\n",
    "                    text = self.char(text)\n",
    "                    text = text.lower()\n",
    "                    text = self.remove_stopwords(text)\n",
    "                    final_text.append(text)\n",
    "\n",
    "                logger.info(f\"Final text to be embedded: {final_text}\")\n",
    "                embeddings = self.tfidf.transform(final_text)\n",
    "                sentiment = self.model.predict(embeddings)\n",
    "                return sentiment\n",
    "\n",
    "        except Exception as ex:\n",
    "            logging.exception(f\"Failed during predict: {ex}\")\n",
    "```\n",
    "---\n",
    "### .s2i/environment\n",
    "In order for the Seldon base image to correctly convert your source code to an image it requires certain environment variables. In this case it is only 3 variables. \n",
    "* `MODEL_NAME`: The model name matches the name of the Python file and class which is created. \n",
    "* `SERVICE_TYPE`: Seldon allows you to create many different components each specialised for a different purpose e.g. `TRANSFORMER` for performing pre or post-processing steps. \n",
    "* `PERSISTENCE`: In some cases you would like to save the state of your deployments to Redis e.g. when scaling up multi-armed bandits\n",
    "\n",
    "This is our environment file:\n",
    "```\n",
    "MODEL_NAME=TweetSentiment\n",
    "SERVICE_TYPE=MODEL\n",
    "PERSISTENCE=0\n",
    "```\n",
    "---\n",
    "### requirements.txt\n",
    "List of Python packages which the deployment requires to run.\n",
    "```\n",
    "joblib\n",
    "pandas\n",
    "numpy\n",
    "seldon_core\n",
    "google-cloud-storage\n",
    "scikit-learn\n",
    "nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f48e6",
   "metadata": {},
   "source": [
    "## Testing Locally\n",
    "In order to ensure that we have gotten the `TweetClassifier.py` working correctly we can use the `seldon_core` Python package to run our model locally and test the endpoint. \n",
    "```\n",
    "seldon-core-microservice TweetSentiment --service-type MODEL\n",
    "                                        --parameters='[{ \n",
    "                                                        \"name\": \"model_path\",\n",
    "                                                        \"value\": \"nlp-workshop/<YOUR NAME>/model.joblib\",\n",
    "                                                        \"type\": \"STRING\"\n",
    "                                                       }, {\n",
    "                                                        \"name\": \"tfidf_path\",\n",
    "                                                        \"value\": \"nlp-workshop/<YOUR NAME>/tfidf.joblib\",\n",
    "                                                        \"type\": \"STRING\"\n",
    "                                                       }]'\n",
    "```\n",
    "This endpoint can then be tested by posting cURL commands to the local endpoint: \n",
    "```\n",
    "curl -H 'Content-Type: application/json' -d '{\"data\": {\"ndarray\": [\"@united how can you not put my bag on plane to Seattle. Flight 1212. Waiting  in line to talk to someone about my bag. Status should matter.\"]}}' http://localhost:9000/api/v1.0/predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988862c0",
   "metadata": {},
   "source": [
    "## Building the Image\n",
    "\n",
    "We can then build the custom model using source 2 image technology. Firstly, installing it locally as per [the documentation](https://github.com/openshift/source-to-image), and then by running this command: \n",
    "```\n",
    "s2i build . seldonio/seldon-core-s2i-python3:1.12.0-dev tweet-sentiment:0.3\n",
    "```\n",
    "\n",
    "The built image is then pushed to Dockerhub where it can be pulled ready for deployment. In an enterprise setting, the container registry would be customised to the client's needs. \n",
    "```\n",
    "docker tag tweet-sentiment:0.3 tomfarrand/tweet-sentiment:0.3\n",
    "docker push tomfarrand/tweet-sentiment:0.3\n",
    "```\n",
    "\n",
    "In this case we will use my pre-built container image for speed and simplicity, which we can now deploy using the SDK. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf376647",
   "metadata": {},
   "source": [
    "## Using the SDK\n",
    "Now that we have our trained model artefact and preprocessor, alongside our custom built container we can now use the Seldon Deploy SDK to deploy our model! \n",
    "\n",
    "Initially we setup some authentication: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SD_IP = \"34.125.69.167\"\n",
    "\n",
    "config = Configuration()\n",
    "config.host = f\"http://{SD_IP}/seldon-deploy/api/v1alpha1\"\n",
    "config.oidc_client_id = \"sd-api\"\n",
    "config.oidc_server = f\"http://{SD_IP}/auth/realms/deploy-realm\"\n",
    "config.oidc_client_secret = \"sd-api-secret\"\n",
    "config.auth_method = 'client_credentials'\n",
    "\n",
    "def auth():\n",
    "    auth = OIDCAuthenticator(config)\n",
    "    config.id_token = auth.authenticate()\n",
    "    api_client = ApiClient(configuration=config, authenticator=auth)\n",
    "    return api_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc14b53",
   "metadata": {},
   "source": [
    "Next, we define the parameters which we shall feed to the SDK.\n",
    "\n",
    "!!! Again, remember to fill in the `YOUR_NAME` parameter !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = <YOUR NAME>\n",
    "MODEL_NAME = \"tweet-sentiment\"\n",
    "\n",
    "DEPLOYMENT_NAME = f\"{YOUR_NAME}-{MODEL_NAME}\"\n",
    "CONTAINER_NAME = f\"tomfarrand/tweet-sentiment:0.3\"\n",
    "\n",
    "NAMESPACE = \"default\"\n",
    "\n",
    "CPU_REQUESTS = \"0.1\"\n",
    "MEMORY_REQUESTS = \"1Gi\"\n",
    "\n",
    "CPU_LIMITS = \"0.1\"\n",
    "MEMORY_LIMITS = \"1Gi\"\n",
    "\n",
    "MODEL_PATH = f\"nlp-workshop/{YOUR_NAME}/model.joblib\"\n",
    "TFIDF_PATH = f\"nlp-workshop/{YOUR_NAME}/tfidf.joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1194566",
   "metadata": {},
   "source": [
    "The deployment specification is then defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67912bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mldeployment = {\n",
    "    \"kind\": \"SeldonDeployment\",\n",
    "    \"metadata\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"namespace\": NAMESPACE,\n",
    "        \"labels\": {\n",
    "            \"fluentd\": \"true\"\n",
    "        }\n",
    "    },\n",
    "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "    \"spec\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"annotations\": {\n",
    "            \"seldon.io/engine-seldon-log-messages-externally\": \"true\"\n",
    "        },\n",
    "        \"protocol\": \"seldon\",\n",
    "        \"predictors\": [\n",
    "            {\n",
    "                \"componentSpecs\": [\n",
    "                    {\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                                    \"image\": CONTAINER_NAME,\n",
    "                                    \"resources\": {\n",
    "                                        \"requests\": {\n",
    "                                            \"cpu\": CPU_REQUESTS,\n",
    "                                            \"memory\": MEMORY_REQUESTS\n",
    "                                        },\n",
    "                                        \"limits\": {\n",
    "                                            \"cpu\": CPU_LIMITS,\n",
    "                                            \"memory\": MEMORY_LIMITS\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": \"default\",\n",
    "                \"replicas\": 1,\n",
    "                \"traffic\": 100,\n",
    "                \"graph\": {\n",
    "                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                    \"parameters\": [\n",
    "                        {\n",
    "                            \"name\":\"model_path\",\n",
    "                            \"value\":MODEL_PATH,\n",
    "                            \"type\":\"STRING\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\":\"tfidf_path\",\n",
    "                            \"value\":TFIDF_PATH,\n",
    "                            \"type\":\"STRING\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"children\": [],\n",
    "                    \"logger\": {\n",
    "                        \"mode\": \"all\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"status\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae9630a",
   "metadata": {},
   "source": [
    "Finally, we deploy the model using a few simple API calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61951499",
   "metadata": {},
   "source": [
    "Once our endpoint becomes available we can then test the deployment using the following request: \n",
    "\n",
    "```\n",
    "{\"data\": {\"ndarray\": [\"@united how can you not put my bag on plane to Seattle. Flight 1212. Waiting  in line to talk to someone about my bag. Status should matter.\"]}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91106b",
   "metadata": {},
   "source": [
    "# Adding an Explainer\n",
    "Model explainers can be used to generate deeper understanding and insights into why models respond in the manner which they do. \n",
    "\n",
    "In this case we will be making use of the Anchors algorithm. The algorithm provides model-agnostic (black box) and human interpretable explanations suitable for classification models applied to images, text and tabular data. The idea behind anchors is to explain the behaviour of complex models with high-precision rules called anchors. These anchors are locally sufficient conditions to ensure a certain prediction with a high degree of confidence.\n",
    "\n",
    "Initially we provide a function which the `AnchorText` algorithm can call in order to generate a classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainer_predict(x):\n",
    "    final_x = []\n",
    "    for text in x:\n",
    "        text = remove_username(text)\n",
    "        text = remove_url(text)\n",
    "        text = remove_emoji(text)\n",
    "        text = decontraction(text)\n",
    "        text = seperate_alphanumeric(text)\n",
    "        text = unique_char(cont_rep_char,text)\n",
    "        text = char(text)\n",
    "        text = text.lower()\n",
    "        text = remove_stopwords(text)\n",
    "        final_x.append(text)\n",
    "\n",
    "    embeddings = tfidf.transform(final_x)\n",
    "    sentiment = rf.predict(embeddings)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27c859",
   "metadata": {},
   "source": [
    "We use spaCy to perform text tokenization, pos-tagging, compute word similarity, etc. The spaCy model can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e9910",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5085520",
   "metadata": {},
   "source": [
    "We then define our explainer itself. The AnchorText explainer acts to perturb a given request in order to generate variations of that request. This can either be done by replacing tokens with an `UNK` token, or by replacing words with similar ones from a corpus. \n",
    "\n",
    "In both cases we then observe how the model responds to determine the most important tokens within the request. In this instance we will generate an explainer using the `similarity` strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(predictor=explainer_predict, sampling_strategy='similarity', nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prediction(explanation):\n",
    "    pred = explanation.raw['prediction'][0]\n",
    "    \n",
    "    if pred == 0:\n",
    "        return \"neutral\"\n",
    "    elif pred == 1:\n",
    "        return \"negative\"\n",
    "    elif pred == 2:\n",
    "        return \"positive\"\n",
    "\n",
    "def provide_alternatives(prediction):\n",
    "    classes = [\"neutral\", \"negative\", \"positive\"]\n",
    "    \n",
    "    matched_indexes= []\n",
    "    i = 0\n",
    "    while i < len(classes):\n",
    "        if prediction == classes[i]:\n",
    "            matched_indexes.append(i)\n",
    "        i += 1\n",
    "    \n",
    "    classes.remove(classes[matched_indexes[0]])\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain(\"not sure how to feel about that flight\", threshold=0.9)\n",
    "\n",
    "str_pred = convert_prediction(explanation)\n",
    "\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('Coverage: %.2f' % explanation.coverage)\n",
    "cprint('\\nExamples where anchor applies and model predicts %s:' % str_pred, \"green\")\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "cprint('\\nExamples where anchor applies and model predicts %s:' % provide_alternatives(str_pred), \"red\")\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90455d6",
   "metadata": {},
   "source": [
    "### Default Text Explainer\n",
    "\n",
    "Seldon Deploy has a trick up it's sleeve. When deploying Alibi Text explainers we can try to use the default explainer. This is generated on the fly by the Alibi Explain server using the following code: \n",
    "```\n",
    "logging.info(\"Loading Spacy Language model for %s\", spacy_language_model)\n",
    "spacy_model(model=spacy_language_model)\n",
    "self.nlp = spacy.load(spacy_language_model)\n",
    "logging.info(\"Language model loaded\")\n",
    "\n",
    "self.anchors_text = alibi.explainers.AnchorText(\n",
    "    predictor=predict_fn, sampling_strategy=\"unknown\", nlp=self.nlp\n",
    ")\n",
    "```\n",
    "Let's see how effective this default explainer is in our case!\n",
    "\n",
    "We can use this sample request to test our explainer once it is ready: \n",
    "```\n",
    "{\"data\": {\"ndarray\": [\"not sure how to feel about that flight\"]}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601cd68c",
   "metadata": {},
   "source": [
    "## Drift Detection\n",
    "\n",
    "Although powerful, modern machine learning models can be sensitive. Seemingly subtle changes in a data distribution can destroy the performance of otherwise state-of-the art models, which can be especially problematic when ML models are deployed in production. Typically, ML models are tested on held out data in order to estimate their future performance. Crucially, this assumes that the process underlying the input data `X` and output data `Y` remains constant.\n",
    "\n",
    "Drift can be classified into the following types:\n",
    "* **Covariate drift**: Also referred to as input drift, this occurs when the distribution of the input data has shifted `P(X) != Pref(X)`, whilst `P(Y|X) = Pref(Y|X)`. This may result in the model giving unreliable predictions.\n",
    "\n",
    "* **Prior drift**: Also referred to as label drift, this occurs when the distribution of the outputs has shifted `P(Y) != Pref(Y)`, whilst `P(X|Y) = Pref(X|Y)`. This can affect the model’s decision boundary, as well as the model’s performance metrics.\n",
    "\n",
    "* **Concept drift**: This occurs when the process generating `Y` from `X` has changed, such that `P(Y|X) != Pref(Y|X)`. It is possible that the model might no longer give a suitable approximation of the true process.\n",
    "\n",
    "-----------------\n",
    "\n",
    "In this instance we will train a Kolmgorov-Smirnov drift detector to pick up on covariate drift. The KS Drift detector applies a two-sample KS test to compare the distance between the new probability distribution and the reference distribution. \n",
    "\n",
    "This is done on a feature by feature basis and the results are then aggregated using a correction, i.e. Bonferroni, to determine whether drift has occurred overall within the sample. \n",
    "\n",
    "We will use the training set as our reference distribution. Creating our drift detector is then as simple as writing a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aee5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(x_ref):\n",
    "    final_x = []\n",
    "    for text in x_ref:\n",
    "        text = remove_username(text)\n",
    "        text = remove_url(text)\n",
    "        text = remove_emoji(text)\n",
    "        text = decontraction(text)\n",
    "        text = seperate_alphanumeric(text)\n",
    "        text = unique_char(cont_rep_char,text)\n",
    "        text = char(text)\n",
    "        text = text.lower()\n",
    "        text = remove_stopwords(text)\n",
    "        final_x.append(text)\n",
    "\n",
    "    embeddings = tfidf.transform(final_x).toarray() # Alibi works with np.arrays, be careful about using sparse arrays\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ref = df['text'].to_numpy()\n",
    "x_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a412d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from alibi_detect.cd import KSDrift\n",
    "\n",
    "dd = KSDrift(x_ref, p_val=.05, preprocess_fn=preprocess_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f17d79",
   "metadata": {},
   "source": [
    "We can now run predictions against our drift detector, and use it to answer questions about our dataset. For example, do the tweets aimed at Delta differ significantly from the overall tweet corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873811a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_tweets = df[df[\"airline\"] == \"Delta\"][\"text\"].to_numpy()\n",
    "\n",
    "preds = dd.predict(delta_tweets)\n",
    "predspreds = dd.predict(x_ref)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dfeedf",
   "metadata": {},
   "source": [
    "### WARNING: This code is currently not supported with the current Alibi Detect server. \n",
    "\n",
    "The use of `preprocess_fn` in this manner is not supported within Alibi v0.6.2, due to the use of `pickle` to serialise objects. However, in later versions of Alibi this is corrected, and therefore should behave correctly.\n",
    "\n",
    "This will be corrected in the next release of Seldon Deploy in Q1 2022, where we can revisit this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59822abf",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "That draws us to the end of the NLP workshop! \n",
    "\n",
    "You have seen how to generate and deploy a custom model for classifying the sentiment of airline tweets. We then created and worked with some of the advanced monitoring and explainability features which are available within the Seldon toolset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 279.948311,
   "end_time": "2021-11-15T08:58:48.814352",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-15T08:54:08.866041",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
