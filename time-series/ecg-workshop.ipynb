{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNsQppRJZGLQ"
   },
   "source": [
    "# Hands-On Time Series Workshop: *Identifying Arrhythmia Using Electrocardiogram Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcvNa1LCZ2e_"
   },
   "source": [
    "In this workshop we will be using time series electrocardiogram (ECG) data to classify Arrhythmia. \n",
    "\n",
    "The workshop is split into the following parts: \n",
    "1. Load and explore the dataset\n",
    "2. Pre-processing\n",
    "3. Training and saving the model \n",
    "4. Deploying the model\n",
    "5. Creating and adding a drift detector! \n",
    "\n",
    "The data pre-processing and model building are based upon the hardwork of Kaggle user Gregoire DC. You can see the original notebook [here](https://www.kaggle.com/gregoiredc/arrhythmia-on-ecg-classification-using-cnn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Colab has a load of packages pre-loaded into the environment. Installing the additional ones we require here.\n",
    "!pip install seldon-deploy-sdk==1.4.1\n",
    "!pip install alibi-detect==0.6.1\n",
    "!pip install seaborn\n",
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "9fJyFQQswDDw"
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn for scoring models\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Keras acts as the model training library\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Drift detection\n",
    "from alibi_detect.cd import MMDDrift, KSDrift\n",
    "from alibi_detect.cd.base import BaseUnivariateDrift\n",
    "from alibi_detect.utils.saving import save_detector, load_detector\n",
    "\n",
    "# Seldon Deploy SDK \n",
    "from seldon_deploy_sdk import Configuration, ApiClient, SeldonDeploymentsApi, DriftDetectorApi\n",
    "from seldon_deploy_sdk.auth import OIDCAuthenticator\n",
    "\n",
    "# For loading serialised objects\n",
    "import dill \n",
    "\n",
    "# Utility library\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS46AwV7SLM8"
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "Download and unpack training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_1YBFJ-UJu-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/seldon-koz-ecg/data/mitbih_test.csv.zip\n",
    "!wget https://storage.googleapis.com/seldon-koz-ecg/data/mitbih_train.csv.zip\n",
    "!unzip 'mitbih_test.csv.zip'\n",
    "!unzip 'mitbih_train.csv.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop uses ECG data collected from Beth Israel Hospital Arrhythmia Laboratory between 1975 and 1979. The subjects were 25 men aged 32 to 89 years, and 22 women aged 23 to 89 years. Data was collected using a Holter monitor. You can find out more about the dataset [here](https://physionet.org/content/mitdb/1.0.0/).\n",
    "\n",
    "**About the dataset**\n",
    "\n",
    "* Number of Samples: 109446\n",
    "* Number of Categories: 5\n",
    "* Sampling Frequency: 125Hz\n",
    "* Data Source: Physionet's MIT-BIH Arrhythmia Dataset\n",
    "* Classes: ['N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4]\n",
    "\n",
    "**Labels**\n",
    "\n",
    "* N : Non-ecotic beats (normal beat) \n",
    "* S : Supraventricular ectopic beats \n",
    "* V : Ventricular ectopic beats \n",
    "* F : Fusion Beats \n",
    "* Q : Unknown Beats\n",
    "\n",
    "More understanding around the labels: an ectopic beat is an extra beat which occurs prior to a normal beat. Fusion beats are a hybrid complex when a supraventricular (top section of your heart) and a ventricular (bottom section of the heart) impulse coincide. \n",
    "\n",
    "Next, we read the training and test data into Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OXXmt7VwDDx"
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('mitbih_train.csv',header=None)\n",
    "test_df=pd.read_csv('mitbih_test.csv',header=None)\n",
    "\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RK6JdZl-wDDy"
   },
   "source": [
    "Each row is a single beat cycle, with each of the features a reading of the electrical activity across that cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the labels to integers, and counting each of their values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7Gu18OLwDDy"
   },
   "outputs": [],
   "source": [
    "train_df[187]=train_df[187].astype(int)\n",
    "label_counts=train_df[187].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the label imbalance with a doughnut chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jZlk7R_wDDy"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "my_circle=plt.Circle((0,0), 0.7, color='white')\n",
    "plt.pie(label_counts, labels=['n','q','v','s','f'], colors=['red','green','blue','skyblue','orange'],autopct='%1.1f%%')\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSzQxmAlwDDz"
   },
   "source": [
    "Clearly, there is a large imbalance in the labels with over 80% of them representing normal heartbeats. This imbalance needs to be rectified in order to create a model which can accurately pick out different types of arrhythmia. \n",
    "\n",
    "Therefore, we select 20,000 normal heartbeat samples. We then resample the remaining classes with replacement to have a resultant dataset with an equal proporition of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "label_dfs = {}\n",
    "\n",
    "for i in label_counts.index:\n",
    "    label_dfs[i] = train_df[train_df[187]==i]\n",
    "\n",
    "# The first class has 72k observations, so we sample down to 20000\n",
    "label_dfs[0] = label_dfs[0].sample(n=20000, random_state=random_state)\n",
    "\n",
    "# The remaining classes have less than 20000 observations and therefore we resample with replacement. \n",
    "for i in [1, 2, 3, 4]:\n",
    "    label_dfs[i] = resample(label_dfs[i], replace=True, n_samples=20000, random_state=random_state)\n",
    "    \n",
    "rebalanced_df = pd.concat([i for i in label_dfs.values()])\n",
    "rebalanced_label_counts = rebalanced_df[187].value_counts()\n",
    "rebalanced_label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nzn2p6ewDD0"
   },
   "source": [
    "Next we visualise a single instance of each class to see the difference in the signatures which they present.\n",
    "\n",
    "To do so, we create a DataFrame with a single obeservation of each of the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mRnvNZ7wDD0"
   },
   "outputs": [],
   "source": [
    "sample_df = rebalanced_df.groupby(187, group_keys=False).apply(lambda train_df: train_df.sample(n=1))\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "plt.plot(sample_df.iloc[0,:186], label=\"Cat. N\")\n",
    "plt.plot(sample_df.iloc[1,:186], label=\"Cat. S\")\n",
    "plt.plot(sample_df.iloc[2,:186], label=\"Cat. V\")\n",
    "plt.plot(sample_df.iloc[3,:186], label=\"Cat. F\")\n",
    "plt.plot(sample_df.iloc[4,:186], label=\"Cat. Q\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"1-beat ECG for every category\", fontsize=20)\n",
    "plt.ylabel(\"Amplitude\", fontsize=15)\n",
    "plt.xlabel(\"Time (ms)\", fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlRhMOpqwDD4"
   },
   "source": [
    "The final piece of pre-processing we do is to add Gaussian noise to our data. This will help us to generalise to unseen data better, by forcing our model to work harder to detect meaningful signal from random noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4iFCJORwDD4"
   },
   "outputs": [],
   "source": [
    "def add_gaussian_noise(signal):\n",
    "    noise = np.random.normal(0, 0.3, 186)\n",
    "    return (signal + noise)\n",
    "\n",
    "single_sample = sample_df.iloc[0,:186]\n",
    "noisy_sample=add_gaussian_noise(single_sample)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(sample_df.iloc[0,:186])\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(noisy_sample)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the Gaussian noise to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pieTfZXMwDD4"
   },
   "outputs": [],
   "source": [
    "target_train=train_df[187]\n",
    "target_test=test_df[187]\n",
    "\n",
    "y_train=to_categorical(target_train)\n",
    "y_test=to_categorical(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtPdefhxwDD4"
   },
   "outputs": [],
   "source": [
    "X_train=train_df.iloc[:,:186].values\n",
    "X_test=test_df.iloc[:,:186].values\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i,:186]= add_gaussian_noise(X_train[i,:186])\n",
    "    \n",
    "X_train = X_train.reshape(len(X_train), X_train.shape[1],1)\n",
    "X_test = X_test.reshape(len(X_test), X_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "For the sake of time we will not train our CNN live and use a pre-trained model. \n",
    "\n",
    "This pre-trained model was trained with the following architecture: \n",
    "```\n",
    "# Input\n",
    "in_shape = (X_train.shape[1],1)\n",
    "inputs_cnn = Input(shape=(in_shape), name='inputs_cnn')\n",
    "\n",
    "# 1st convolutional layer\n",
    "conv1_1 = Convolution1D(64, (6), activation='relu', input_shape=in_shape)(inputs_cnn)\n",
    "conv1_1 = BatchNormalization()(conv1_1)\n",
    "pool1 = MaxPool1D(pool_size=(3), strides=(2), padding=\"same\")(conv1_1)\n",
    "\n",
    "# 2nd convolutional layer\n",
    "conv2_1 = Convolution1D(64, (3), activation='relu', input_shape=in_shape)(pool1)\n",
    "conv2_1 = BatchNormalization()(conv2_1)\n",
    "pool2 = MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv2_1)\n",
    "\n",
    "# 3rd convolutional layer\n",
    "conv3_1 = Convolution1D(64, (3), activation='relu', input_shape=in_shape)(pool2)\n",
    "conv3_1 = BatchNormalization()(conv3_1)\n",
    "pool3 = MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv3_1)\n",
    "flatten = Flatten()(pool3)\n",
    "\n",
    "# Output\n",
    "dense_end1 = Dense(64, activation='relu')(flatten)\n",
    "dense_end2 = Dense(32, activation='relu')(dense_end1)\n",
    "main_output = Dense(5, activation='softmax', name='main_output')(dense_end2)\n",
    "\n",
    "```\n",
    "The best model was saved using early stopping at epoch 25 of 40. We load back this pretrained model as well as the training history to explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pretrained_model\n",
    "!gsutil cp -r gs://tom-seldon-examples/time-series-workshop/1 pretrained_model\n",
    "!gsutil cp gs://tom-seldon-examples/time-series-workshop/history.dill history.dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"pretrained_model/1\")\n",
    "\n",
    "with open(\"history.dill\", \"rb\") as file:\n",
    "    history = dill.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define a function to evaluate our model training efforts. This plots the training and validation accuracy and losses of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paZkZN3ewDD5"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(history, X_test, y_test, model):\n",
    "    \"\"\"\n",
    "    Plots line graphs for the training/validation accuracy and losses. \n",
    "    \"\"\"\n",
    "    scores = model.evaluate((X_test), y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    \n",
    "    fig1, ax_acc = plt.subplots()\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model - Accuracy')\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    fig2, ax_loss = plt.subplots()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model- Loss')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.show()\n",
    "    target_names=['0','1','2','3','4']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc1mFFyycAhb"
   },
   "source": [
    "Evaluate the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoV3FPc4wDD5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(history, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot a normalised confusion matrix to understand better how our model performs across the multiple classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzGfpG0dwDD5"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, \n",
    "                          classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the normalised confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the Model\n",
    "We now want to serve our model on the Seldon infrastructure.\n",
    "\n",
    "For this we can use the pre-built Tensorflow container image, which allows us to rapidly deploy our newly trained model using TFServing. \n",
    "\n",
    "Firstly we save our model locally i.e. `model.save(\"pretrained_model\")` \n",
    "\n",
    "We can then push it a cloud storage bucket ready for deployment. This has been done ahead of time and the model can be accessed at the following URI `gs://tom-seldon-examples/time-series-workshop/`\n",
    "\n",
    "-----------\n",
    "\n",
    "A couple of considerations: \n",
    "1. The TF pre-built image uses the SavedModel format, not Kera's older HDF5 format.\n",
    "2. TFServing expects a subdirectory referring to the model version which then contains the model artifacts. You can create this manually when pushing to cloud storage e.g. `gs://tom-seldon-examples/time-series-workshop/1/saved_model.pb`\n",
    "\n",
    "----------\n",
    "\n",
    "We are now ready to deploy to Seldon. We will do this via the Python SDK provided to interact with Seldon Deploy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SD_IP = \"34.141.146.222\"\n",
    "\n",
    "config = Configuration()\n",
    "config.host = f\"http://{SD_IP}/seldon-deploy/api/v1alpha1\"\n",
    "config.oidc_client_id = \"sd-api\"\n",
    "config.oidc_server = f\"http://{SD_IP}/auth/realms/deploy-realm\"\n",
    "config.oidc_client_secret = \"sd-api-secret\"\n",
    "config.auth_method = 'client_credentials'\n",
    "\n",
    "def auth():\n",
    "    auth = OIDCAuthenticator(config)\n",
    "    config.id_token = auth.authenticate()\n",
    "    api_client = ApiClient(configuration=config, authenticator=auth)\n",
    "    return api_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = \"YOUR-NAME\" # Replace with your name\n",
    "MODEL_NAME = \"MODEL-NAME\" # Replace with your model name\n",
    "\n",
    "DEPLOYMENT_NAME = f\"{YOUR_NAME}-{MODEL_NAME}\"\n",
    "MODEL_LOCATION = f\"gs://tom-seldon-examples/time-series-workshop/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMESPACE = \"seldon-gitops\"\n",
    "\n",
    "PREPACKAGED_SERVER = \"TENSORFLOW_SERVER\"\n",
    "\n",
    "CPU_REQUESTS = \"0.1\"\n",
    "MEMORY_REQUESTS = \"1Gi\"\n",
    "\n",
    "CPU_LIMITS = \"0.1\"\n",
    "MEMORY_LIMITS = \"1Gi\"\n",
    "\n",
    "mldeployment = {\n",
    "    \"kind\": \"SeldonDeployment\",\n",
    "    \"metadata\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"namespace\": NAMESPACE,\n",
    "        \"labels\": {\n",
    "            \"fluentd\": \"true\"\n",
    "        }\n",
    "    },\n",
    "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "    \"spec\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"annotations\": {\n",
    "            \"seldon.io/engine-seldon-log-messages-externally\": \"true\"\n",
    "        },\n",
    "        \"protocol\": \"seldon\",\n",
    "        \"predictors\": [\n",
    "            {\n",
    "                \"componentSpecs\": [\n",
    "                    {\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                                    \"resources\": {\n",
    "                                        \"requests\": {\n",
    "                                            \"cpu\": CPU_REQUESTS,\n",
    "                                            \"memory\": MEMORY_REQUESTS\n",
    "                                        },\n",
    "                                        \"limits\": {\n",
    "                                            \"cpu\": CPU_LIMITS,\n",
    "                                            \"memory\": MEMORY_LIMITS\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": \"default\",\n",
    "                \"replicas\": 1,\n",
    "                \"traffic\": 100,\n",
    "                \"graph\": {\n",
    "                    \"implementation\": PREPACKAGED_SERVER,\n",
    "                    \"modelUri\": MODEL_LOCATION,\n",
    "                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                    \"endpoint\": {\n",
    "                        \"type\": \"REST\"\n",
    "                    },\n",
    "                    \"parameters\": [],\n",
    "                    \"children\": [],\n",
    "                    \"logger\": {\n",
    "                        \"mode\": \"all\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"status\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model should now be running as a fully fledged microservice. We can now log into Seldon Deploy:\n",
    "\n",
    "* URL: http://34.141.146.222/seldon-deploy/\n",
    "* Username: admin@seldon.io\n",
    "* Password: 12341234\n",
    "\n",
    "We can then test our new deployment using the following request: \n",
    "```\n",
    "{\n",
    "\t\"data\": {\n",
    "\t\t\"ndarray\": [\n",
    "\t\t\t[\n",
    "\t\t\t\t[1.0],\n",
    "\t\t\t\t[0.76],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.08],\n",
    "\t\t\t\t[0.08],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.03],\n",
    "\t\t\t\t[0.03],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.03],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.08],\n",
    "\t\t\t\t[0.1],\n",
    "\t\t\t\t[0.12],\n",
    "\t\t\t\t[0.13],\n",
    "\t\t\t\t[0.17],\n",
    "\t\t\t\t[0.2],\n",
    "\t\t\t\t[0.21],\n",
    "\t\t\t\t[0.24],\n",
    "\t\t\t\t[0.25],\n",
    "\t\t\t\t[0.26],\n",
    "\t\t\t\t[0.29],\n",
    "\t\t\t\t[0.27],\n",
    "\t\t\t\t[0.27],\n",
    "\t\t\t\t[0.24],\n",
    "\t\t\t\t[0.21],\n",
    "\t\t\t\t[0.17],\n",
    "\t\t\t\t[0.16],\n",
    "\t\t\t\t[0.12],\n",
    "\t\t\t\t[0.12],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.1],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.1],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.1],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.12],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.12],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.12],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.12],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.1],\n",
    "\t\t\t\t[0.1],\n",
    "\t\t\t\t[0.09],\n",
    "\t\t\t\t[0.09],\n",
    "\t\t\t\t[0.08],\n",
    "\t\t\t\t[0.08],\n",
    "\t\t\t\t[0.08],\n",
    "\t\t\t\t[0.08],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.08],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.08],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.09],\n",
    "\t\t\t\t[0.1],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.12],\n",
    "\t\t\t\t[0.12],\n",
    "\t\t\t\t[0.11],\n",
    "\t\t\t\t[0.1],\n",
    "\t\t\t\t[0.1],\n",
    "\t\t\t\t[0.1],\n",
    "\t\t\t\t[0.09],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.06],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.06],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.06],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.01],\n",
    "\t\t\t\t[0.19],\n",
    "\t\t\t\t[0.68],\n",
    "\t\t\t\t[0.98],\n",
    "\t\t\t\t[0.62],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.01],\n",
    "\t\t\t\t[0.09],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.03],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.04],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.05],\n",
    "\t\t\t\t[0.06],\n",
    "\t\t\t\t[0.07],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0],\n",
    "\t\t\t\t[0.0]\n",
    "\t\t\t]\n",
    "\t\t]\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift Detection\n",
    "\n",
    "Although powerful, modern machine learning models can be sensitive. Seemingly subtle changes in a data distribution can destroy the performance of otherwise state-of-the art models, which can be especially problematic when ML models are deployed in production. Typically, ML models are tested on held out data in order to estimate their future performance. Crucially, this assumes that the process underlying the input data `X` and output data `Y` remains constant.\n",
    "\n",
    "Drift can be classified into the following types:\n",
    "* **Covariate drift**: Also referred to as input drift, this occurs when the distribution of the input data has shifted `P(X) != Pref(X)`, whilst `P(Y|X) = Pref(Y|X)`. This may result in the model giving unreliable predictions.\n",
    "\n",
    "* **Prior drift**: Also referred to as label drift, this occurs when the distribution of the outputs has shifted `P(Y) != Pref(Y)`, whilst `P(X|Y) = Pref(X|Y)`. This can affect the model’s decision boundary, as well as the model’s performance metrics.\n",
    "\n",
    "* **Concept drift**: This occurs when the process generating `Y` from `X` has changed, such that `P(Y|X) != Pref(Y|X)`. It is possible that the model might no longer give a suitable approximation of the true process.\n",
    "\n",
    "-----------------\n",
    "\n",
    "In this instance we will train a Kolmgorov-Smirnov drift detector to pick up on covariate drift. The KS Drift detector applies a two-sample KS test to compare the distance between the new probability distribution and the reference distribution. \n",
    "\n",
    "This is done on a feature by feature basis and the results are then aggregated using a correction, i.e. Bonferroni, to determine whether drift has occurred overall within the sample. \n",
    "\n",
    "We will use the training set as our reference distribution. Creating our drift detector is then as simple as writing a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = KSDrift(X_train, p_val=.05, correction='bonferroni')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then easily get a prediction against a given batch of data and determine whether drift has occurred or not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dd.predict(X_train)\n",
    "labels = [\"No\", \"Yes\"]\n",
    "\n",
    "print(f'Drift? {labels[preds[\"data\"][\"is_drift\"]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, when using our training data as the new distribution we do not detect any drift. \n",
    "\n",
    "However, if we passed the instances of solely the ventricular ecoptic beats (class 2) we can see drift is now detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds = dd.predict(label_dfs[2].values)\n",
    "\n",
    "print(f'Drift? {labels[new_preds[\"data\"][\"is_drift\"]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the `new_preds` object we can see that it contains the `is_drift` binary flag indicating whether drift has been detected after applying the Bonferroni correction to the feature wise distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YUfhpOhbiDH"
   },
   "source": [
    "We can easily save the artifacts related to our drift detector using Alibi Detect's `save_detector` utility function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_detector(dd, \"drift_detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then push this to our GCS bucket ready for deployment alongside our model. \n",
    "\n",
    "Remember to replace `<YOUR NAME>` with the same name as before! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r drift_detector gs://tom-seldon-examples/time-series-workshop/<YOUR NAME>/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the Seldon Deploy SDK to configure and deploy our shiny new drift detector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Add code to deploy the drift detector!\n",
    "DD_URI = f'gs://tom-seldon-examples/time-series-workshop/{YOUR_NAME}/drift_detector/'\n",
    "DD_NAME = \"ks-drift\"\n",
    "\n",
    "dd_config = {'config': {'basic': \n",
    "                        {'drift_batch_size': '5',\n",
    "                         'storage_uri': DD_URI},\n",
    "                        'deployment': {'protocol': 'seldon.http'}\n",
    "                        },\n",
    "             'deployment_name': DEPLOYMENT_NAME,\n",
    "             'detector_type': 'drift',\n",
    "             'name': DD_NAME,\n",
    "             'namespace': NAMESPACE\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_api = DriftDetectorApi(auth())\n",
    "\n",
    "dd_api.create_drift_detector_seldon_deployment(name=DEPLOYMENT_NAME, \n",
    "                                               namespace=NAMESPACE, \n",
    "                                               detector_data=dd_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "01-ecg-workshop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
