{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqU0bqjtRKwu"
   },
   "source": [
    "# Customer Segmentation \n",
    "\n",
    "This notebook will focus on the creation, deployment, \n",
    "monitoring and management of a machine learning model for performing \n",
    "customer segmentation. \n",
    "\n",
    "We will be using an e-commerce dataset detailing actual purchases made by  ∼ 4000 customers over a period of one year (from 2010/12/01 to 2011/12/09). \n",
    "\n",
    "In this notebook we will: \n",
    "\n",
    "- Explore a subset of the dataset\n",
    "- Train several models on the pre-processed dataset\n",
    "- Deploy trained models to Seldon \n",
    "- Train an anchor tabular explainer and update Seldon deployment with explainer\n",
    "- Train an outlier detector (variational autoencoder) and update deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCnrhNBSkOZW"
   },
   "source": [
    "### Prerequisites\n",
    "Initially, we install some additional packages which do not come out of the box with our Colab environment, and then import all of the relevant packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNI8xSiiBd24",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install seldon-deploy-sdk==1.4.1\n",
    "!pip install alibi==0.6.0\n",
    "!pip install alibi-detect==0.6.1\n",
    "!pip install fsspec\n",
    "!pip install gcsfs\n",
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9RurRkiZljs"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, model_selection, metrics, feature_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import neighbors, linear_model, svm, tree, ensemble\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pickle \n",
    "import joblib\n",
    "import dill\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from alibi.explainers import AnchorTabular\n",
    "from alibi_detect.datasets import fetch_kdd\n",
    "from alibi_detect.models.tensorflow.losses import elbo\n",
    "from alibi_detect.od import OutlierVAE\n",
    "from alibi_detect.cd import TabularDrift\n",
    "from alibi_detect.utils.data import create_outlier_batch\n",
    "from alibi_detect.utils.fetching import fetch_detector\n",
    "from alibi_detect.utils.saving import save_detector, load_detector\n",
    "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_tabular, plot_roc\n",
    "from seldon_deploy_sdk import Configuration, ApiClient, SeldonDeploymentsApi, OutlierDetectorApi, DriftDetectorApi\n",
    "from seldon_deploy_sdk.auth import OIDCAuthenticator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cr6yO693qzfH"
   },
   "source": [
    "We then download our pre-processed dataset to save time in getting to the exciting parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBYt8NSjqxTl"
   },
   "outputs": [],
   "source": [
    "!gsutil -m cp -r gs://tom-seldon-examples/retail-workshop/data ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX6yZr4FRc3t"
   },
   "source": [
    "### Data Exploration\n",
    "\n",
    "Typically e-commerce datasets are proprietary and consequently hard to find among publicly available data. However, The UCI Machine Learning Repository has curated a retail dataset containing actual transactions from 2010 and 2011. The dataset is maintained on their site, where it can be found by the title \"Online Retail\". The following description is provided via Kaggle: \n",
    "\n",
    "\"This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\"\n",
    "\n",
    "We will first explore the dataset and then train models on a pre-processed version of the dataset that was transformed through following the fantastic work of Fabien Daniel whose methods are detailed in [this notebook](https://www.kaggle.com/fabiendaniel/customer-segmentation).\n",
    "\n",
    "Lets dive in and explore a subset of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOYDKkTIcej4"
   },
   "outputs": [],
   "source": [
    "df_initial = pd.read_csv('data/data.csv', encoding=\"ISO-8859-1\", nrows=3000,\n",
    "                         dtype={'CustomerID': str,'InvoiceID': str})\n",
    "\n",
    "print('Dataframe dimensions:', df_initial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aI8vVdPwSY0C"
   },
   "outputs": [],
   "source": [
    "df_initial['InvoiceDate'] = pd.to_datetime(df_initial['InvoiceDate'])\n",
    "\n",
    "# gives some infos on columns types and numer of null values\n",
    "tab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\n",
    "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
    "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n",
    "                         rename(index={0:'null values (%)'}))\n",
    "display(tab_info)\n",
    "\n",
    "# show first lines\n",
    "display(df_initial[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lD7KaiT0SBx"
   },
   "source": [
    "The dataset contains the following features: \n",
    "\n",
    "**InvoiceNo**: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "\n",
    "**StockCode**: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "\n",
    "**Description**: Product (item) name. Nominal.\n",
    "\n",
    "**Quantity**: The quantities of each product (item) per transaction. Numeric.\n",
    "\n",
    "**InvoiceDate**: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "\n",
    "**UnitPrice**: Unit price. Numeric, Product price per unit in sterling.\n",
    "\n",
    "**CustomerID**: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "\n",
    "**Country**: Country name. Nominal, the name of the country where each customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2nAnRGxkOZZ"
   },
   "source": [
    "We will be training models using a transformed version of this dataset. Rather than detailing each individual item bought, the transformed dataset details the spending behavoir of each individual customer across different categories of products. We don't have any product categories in the original dataset so we create this feature through analysing keywords used within product descriptions and applying k-means clustering to create a finite number of product clusters. We find in our analysis that 5 clusters is a suitable number to seperate products without creating clusters containing too few products (calculated using silhouette score). The following wordclouds demonstrate the splits we generate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXd0w4jAkOZa"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ribenamaplesyrup/seldon-retail-workshop/main/assets/download.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7HQJpFekOZa"
   },
   "source": [
    "We see that cluster no1 contains words we might associate more with gifts ('christmas', 'card', 'wrap', 'decoration') and no3 more with luxury goods ('lace', 'necklace'), although some of the other clusters are less differentiated.\n",
    "\n",
    "Based on customer spending across each of these categories of product, we can similarly use k-means to cluster customers into different segments. Performing similar analysis to how we clustered product categories, we find 11 clusters is the most suitable for segmenting customers within this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pOElOiWkOZa"
   },
   "outputs": [],
   "source": [
    "columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ] # change to feature_names?\n",
    "class_names = ['0','1','2','3','4','5','6','7','8','9','10']\n",
    "X_train = np.load('data/X_train.npy')\n",
    "Y_train = np.load('data/Y_train.npy')\n",
    "X_test = np.load('data/X_test.npy')\n",
    "Y_test = np.load('data/Y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXrmKzJq0SB2"
   },
   "source": [
    "The transformed dataset contains the following features specific to each unique customer:\n",
    "\n",
    "**mean**: mean spend across all transactions\n",
    "\n",
    "**categ_0**: percentage of total spend on products within category 0\n",
    "\n",
    "**categ_1**: percentage of total spend on products within category 1\n",
    "\n",
    "**categ_2**: percentage of total spend on products within category 2\n",
    "\n",
    "**categ_3**: percentage of total spend on products within category 3\n",
    "\n",
    "**categ_4**: percentage of total spend on products within category 4\n",
    "\n",
    "**customer_category**: integer signifying which of the 11 categories the customer aligns most with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoczMYGIkOZb"
   },
   "outputs": [],
   "source": [
    "customers = pd.DataFrame(Y_train, columns=['customer_category']).astype(dtype=int)\n",
    "spending = pd.DataFrame(X_train, columns = columns)\n",
    "transformed_dataset = pd.concat([spending, customers], axis=1)\n",
    "transformed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEWdnLO80SB3"
   },
   "source": [
    "We can explore the range of values across each of the features within our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93o62rkE0SB3"
   },
   "outputs": [],
   "source": [
    "transformed_dataset.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXPspr0X0SB4"
   },
   "outputs": [],
   "source": [
    "transformed_dataset.min(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ei4WoQXTZtjq"
   },
   "source": [
    "## Model Training\n",
    "\n",
    "In this section we will adjust a classifier that will classify consumers in the different client categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrM0vFLtJBnn"
   },
   "outputs": [],
   "source": [
    "# Standardise the dataset\n",
    "mean, stdev = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean) / stdev\n",
    "X_test = (X_test - mean) / stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZXr3-bfcrZr"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se7kBpFlcUm1"
   },
   "outputs": [],
   "source": [
    "lr = linear_model.LogisticRegression(max_iter=4000)\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4mLxfzSvvAd"
   },
   "outputs": [],
   "source": [
    "accuracy_score(Y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSDZtLDJ0SB5"
   },
   "source": [
    "### Gradient Boosting Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ck-H1eU0SB6"
   },
   "outputs": [],
   "source": [
    "gb=ensemble.GradientBoostingClassifier()\n",
    "gb.fit(X = X_train, y = Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLEZabim10iX"
   },
   "outputs": [],
   "source": [
    "accuracy_score(Y_test, gb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvFuSdmp28xt"
   },
   "outputs": [],
   "source": [
    "models = ['lr', 'gb']\n",
    "\n",
    "for model in models:\n",
    "    if not os.path.exists('models/' + model):\n",
    "        os.makedirs('models/' + model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB-Y8GEP3PND"
   },
   "outputs": [],
   "source": [
    "filename = 'models/lr/model.joblib'\n",
    "joblib.dump(lr, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7kIM8Tz3QCU"
   },
   "outputs": [],
   "source": [
    "filename = 'models/gb/model.joblib'\n",
    "joblib.dump(gb, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zZjwLAlAoXg"
   },
   "source": [
    "### Push model artefacts to GCP\n",
    "\n",
    "To push models to GCP you will create a unique folder with your name. Uncomment the command and replace \"YOUR NAME\" with your name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlU3AfYG_BtN"
   },
   "outputs": [],
   "source": [
    "!gsutil cp models/lr/model.joblib gs://tom-seldon-examples/retail-workshop/models/<YOUR NAME>/lr/model.joblib\n",
    "!gsutil cp models/gb/model.joblib gs://tom-seldon-examples/retail-workshop/models/<YOUR NAME>/gb/model.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlrAixGMA_Hs"
   },
   "source": [
    "### Deploy models to Seldon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bS8XO97eBMGF"
   },
   "source": [
    "We can now deploy our model to the dedicated Seldon Deploy cluster which we have configured for this workshop. To do so we will interact with the Seldon Deploy SDK.\n",
    "\n",
    "First, setting up the configuration and authentication required to access the cluster. Make sure to fill in the SD_IP variable to be the same as the cluster you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOCjkhaIBtVF"
   },
   "outputs": [],
   "source": [
    "SD_IP = \"34.141.246.254\"\n",
    "\n",
    "config = Configuration()\n",
    "config.host = f\"http://{SD_IP}/seldon-deploy/api/v1alpha1\"\n",
    "config.oidc_client_id = \"sd-api\"\n",
    "config.oidc_server = f\"http://{SD_IP}/auth/realms/deploy-realm\"\n",
    "config.oidc_client_secret = \"sd-api-secret\"\n",
    "config.auth_method = 'client_credentials'\n",
    "\n",
    "def auth():\n",
    "    auth = OIDCAuthenticator(config)\n",
    "    config.id_token = auth.authenticate()\n",
    "    api_client = ApiClient(configuration=config, authenticator=auth)\n",
    "    return api_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V37sraULkOZf"
   },
   "source": [
    "Now we have configured the IP correctly as well as setup our authentication function we can desrcibe the deployment we would like to create.\n",
    "\n",
    "You will need to fill in the DEPLOYMENT_NAME, NAMESPACE, and the MODEL_LOCATION, the rest of the deployment description has been templated for you.\n",
    "\n",
    "You will need to create a unique deployment name. A good example format would be \"YOUR NAME\" + \"MODEL\" so \"sgreaves-lr\" in my case (be careful not to use any upper-case letters or other characters like \"_\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktWrWseb0SB8"
   },
   "outputs": [],
   "source": [
    "YOUR_NAME = \"<YOUR NAME>\"\n",
    "MODEL_NAME = \"lr\"\n",
    "\n",
    "DEPLOYMENT_NAME = f\"{YOUR_NAME}-{MODEL_NAME}\"\n",
    "MODEL_LOCATION = f\"gs://tom-seldon-examples/retail-workshop/models/{YOUR_NAME}/lr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7gHzz9JBwLZ"
   },
   "outputs": [],
   "source": [
    "NAMESPACE = \"default\"\n",
    "\n",
    "PREPACKAGED_SERVER = \"SKLEARN_SERVER\"\n",
    "\n",
    "CPU_REQUESTS = \"0.1\"\n",
    "MEMORY_REQUESTS = \"1Gi\"\n",
    "\n",
    "CPU_LIMITS = \"0.1\"\n",
    "MEMORY_LIMITS = \"1Gi\"\n",
    "\n",
    "mldeployment = {\n",
    "    \"kind\": \"SeldonDeployment\",\n",
    "    \"metadata\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"namespace\": NAMESPACE,\n",
    "        \"labels\": {\n",
    "            \"fluentd\": \"true\"\n",
    "        }\n",
    "    },\n",
    "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "    \"spec\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"annotations\": {\n",
    "            \"seldon.io/engine-seldon-log-messages-externally\": \"true\"\n",
    "        },\n",
    "        \"protocol\": \"seldon\",\n",
    "        \"transport\": \"rest\",\n",
    "        \"predictors\": [\n",
    "            {\n",
    "                \"componentSpecs\": [\n",
    "                    {\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                                    \"resources\": {\n",
    "                                        \"requests\": {\n",
    "                                            \"cpu\": CPU_REQUESTS,\n",
    "                                            \"memory\": MEMORY_REQUESTS\n",
    "                                        },\n",
    "                                        \"limits\": {\n",
    "                                            \"cpu\": CPU_LIMITS,\n",
    "                                            \"memory\": MEMORY_LIMITS\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": \"default\",\n",
    "                \"replicas\": 1,\n",
    "                \"traffic\": 100,\n",
    "                \"graph\": {\n",
    "                    \"implementation\": PREPACKAGED_SERVER,\n",
    "                    \"modelUri\": MODEL_LOCATION,\n",
    "                    \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                    \"endpoint\": {\n",
    "                        \"type\": \"REST\"\n",
    "                    },\n",
    "                    \"parameters\": [],\n",
    "                    \"children\": [],\n",
    "                    \"logger\": {\n",
    "                        \"mode\": \"all\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"status\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SXGONbMbSFW"
   },
   "source": [
    "We can now invoke the `SeldonDeploymentsApi` and create a new Seldon Deployment. \n",
    "\n",
    "Time for you to get your hands dirty. You will use the Seldon Deploy SDK to create a new Seldon deployment. You can find the reference documentation [here](https://github.com/SeldonIO/seldon-deploy-sdk/blob/master/python/README.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeKpuObJCHA3"
   },
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOmBKqU_0SB9"
   },
   "source": [
    "Our model should now be running as a fully fledged microservice. We can now log into Seldon Deploy and test our deployment: \n",
    "\n",
    "* URL: http://34.141.246.254/seldon-deploy/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4wwbCTT0SB9"
   },
   "outputs": [],
   "source": [
    "# test model with this request: \n",
    "{\"data\": {\"ndarray\": [[-0.43831067,  0.23457404, -0.17366155,  0.26935668,  0.13463672, -0.55234561]]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model wrapping (aside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy our models we didn't need to build a server as there already exists containers for serving scikit-learn and xgboost models that we could use when defining `implementation` within our inference graph. This will not always be the case and we will often need to build our own server. Seldon Core can be used to server models based in Python, Java, R, Node and Go. We can use Seldon Core to build two kinds of server, reusable and non-reusable: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/ribenamaplesyrup/seldon-retail-workshop/main/assets/servers.png \"Reusable and non-reusable servers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusable servers allow us to provide a `model uri` parameter so we can serve different models every time we deploy without needing to build a new server. Non-reusable servers wrap models within the container and so if we want to deploy a new model, we need to build a new server.\n",
    "\n",
    "The process for wrapping python models in a non-reusable server is covered within the [Seldon Core documentation](https://docs.seldon.io/projects/seldon-core/en/latest/python/index.html) and within this [simple guide](https://docs.google.com/presentation/d/1wzs_Xfw0iGXYvPkas9VWO4cvW6xvtwHeNclw0m9ej0M/edit?usp=sharing) for wrapping a scikit-learn model trained on the iris dataset. Included within the `wrapper` directory in this repo are the files required to wrap an example model: \n",
    "\n",
    "- model class (CustomSegmenter.py)\n",
    "- model artefact (models/lr/model.joblib)\n",
    "- dependencies (requirements.txt)\n",
    "- environment (.s2i/environment)\n",
    "- optional testing scripts (test.py & container_test.py)\n",
    "\n",
    "You can have a go at wrapping the model stored within the `wrapper` directory by installing the [Source-2-Image tool](https://github.com/openshift/source-to-image) and running the following command within the wrapper directory to build a container: \n",
    "\n",
    "`s2i build . seldonio/seldon-core-s2i-python3:1.12.0-dev customer-segmenter:0.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjspFW0AFlHS"
   },
   "source": [
    "## Explainer\n",
    "\n",
    "Next, we shall train an explainer to glean deeper insights into the decisions being made by our model. We will make use of the Anchors algorithm, which has a [production grade implementation available](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html) using the Seldon Alibi Explain library. \n",
    "\n",
    "The algorithm provides model-agnostic (black box) and human interpretable explanations suitable for classification models applied to images, text and tabular data. The idea behind anchors is to explain the behaviour of complex models with high-precision rules called anchors. These anchors are locally sufficient conditions to ensure a certain prediction with a high degree of confidence. Anchor algorithms incorporate coverage, which is the region the explanation applies within and are optimised to maximize coverage.\n",
    "\n",
    "As an example of anchors for tabular data, if we want to predict whether a person makes less or more than £50,000 per year based on the person’s characteristics including age (continuous variable) and marital status (categorical variable), then the following would be a potential anchor: Hugo makes more than £50,000 because he is married and his age is between 35 and 45 years.\n",
    "\n",
    "The first step will be to write a simple prediction function which the explainer can call in order to query our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMowappmHKz1"
   },
   "outputs": [],
   "source": [
    "predict_fn = lambda x: lr.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NcaGOy50SB-"
   },
   "source": [
    "We then initialise our Anchor explainer, using the AnchorTabular flavour provided by Alibi due to our data modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNLvR53vG7Yb"
   },
   "outputs": [],
   "source": [
    "explainer = AnchorTabular(predict_fn, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZsZQIRlaHtc"
   },
   "source": [
    "We will now train our explainer. We need to specify percentages that our model will use to discretize continuous numerical data into intervals. Generally we observe that if we choose smaller percentages (finer discretization), this will lead to longer runtime and potentially higher precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQc3nvlIHofS"
   },
   "outputs": [],
   "source": [
    "explainer.fit(X_train, disc_perc=(25, 50, 75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-rTSwPD0SB_"
   },
   "source": [
    "We can now test our explainer by generating a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AgHZXu3Hr52"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print('Prediction: ', class_names[explainer.predictor(X_test[idx].reshape(1, -1))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FD98jlOI15c"
   },
   "outputs": [],
   "source": [
    "explanation = explainer.explain(X_test[idx], threshold=0.9)\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('Coverage: %.2f' % explanation.coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--T6Hu96_W6A"
   },
   "source": [
    "We use dill to serialise our explainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6LUjpMSI-BP"
   },
   "outputs": [],
   "source": [
    "with open(\"models/lr/explainer.dill\", \"wb\") as model_f:\n",
    "        dill.dump(explainer, model_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFX97ejagbyO"
   },
   "source": [
    "We will not push our explainer to GCP as explainer files can be quite large (100MB+) so instead we will deploy a pretrained explainer!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG6kWCMDOHsR"
   },
   "source": [
    "## Deployment\n",
    "\n",
    "We can now deploy an explainer alongside our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTq-3i5XMZDh"
   },
   "outputs": [],
   "source": [
    "EXPLAINER_TYPE = \"AnchorTabular\"\n",
    "EXPLAINER_URI = \"gs://tom-seldon-examples/retail-workshop/models/tom-farrand/lr\"\n",
    "\n",
    "explainer_spec = {\n",
    "                    \"type\": EXPLAINER_TYPE,\n",
    "                    \"modelUri\": EXPLAINER_URI,\n",
    "                    \"containerSpec\": {\n",
    "                        \"name\": \"\",\n",
    "                        \"resources\": {}\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gr7cIsDPOZ0T"
   },
   "outputs": [],
   "source": [
    "mldeployment['spec']['predictors'][0]['explainer'] = explainer_spec\n",
    "mldeployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEE1Ng-IOeR_"
   },
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmI8brYLieU-"
   },
   "source": [
    "## Outlier Detection\n",
    "You will now setup your outlier detector. This will pick up anomalous data points in an automated fashion. You will use the Seldon Alibi Detect library to configure a Variational Auto Encoder (VAE) outlier detector.\n",
    "\n",
    "In this case you are going to assume that any instance which is labelled as customer segmentation cluster 1 is an outlier. Clearly, this is a trivial example, as we are using our model to predict the customer segmen. However it demonstrates the flow of outlier detector creation nicely, and provides a set of easy to identify outliers. \n",
    "\n",
    "You now generate a dataset of inliers (all other clusters) and outliers (customer cluster 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90njHc1nDaf1"
   },
   "outputs": [],
   "source": [
    "df_total = pd.concat([pd.DataFrame(X_train), pd.DataFrame(Y_train, columns=[6])], axis=1)\n",
    "\n",
    "inliers = df_total[(df_total[6] != 1)]\n",
    "inliers.drop(labels=6, axis=1, inplace=True)\n",
    "inliers_np = inliers.to_numpy()\n",
    "\n",
    "outliers = df_total[df_total[6] == 1]\n",
    "outliers.drop(labels=6, axis=1, inplace=True)\n",
    "outliers_np = outliers.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNiXwKDK0SCC"
   },
   "source": [
    "You will then define the architecture of your VAE. The VAE works by attempting to reconstruct the input data which it receives. The VAE first encodes the data in some latent space (in our case a 2 dimensional vector), and then uses a decoder to reconstruct the original input data from the encoding. This forces the VAE to learn a mapping of input data to the latent space, and vice versa. If input data maps poorly to the latent space, and/or maps poorly from latent space to output then it is likely out of the distribution which the VAE was trained upon. Therefore, we can classify it as an outlier.  \n",
    "\n",
    "![](https://raw.githubusercontent.com/ribenamaplesyrup/seldon-retail-workshop/main/assets/vae_architecture.png \"VAE Architecture\")\n",
    "\n",
    "The first step is to define our VAE architecture. You will use TensorFlow Keras to setup the architecture for your encoder and decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZewHIB-EBkz"
   },
   "outputs": [],
   "source": [
    "# define model, initialize, train and save outlier detector\n",
    "    \n",
    "n_features = X_train.shape[1]\n",
    "latent_dim = 2\n",
    "    \n",
    "encoder_net = tf.keras.Sequential(\n",
    "    [\n",
    "     InputLayer(input_shape=(n_features,)),\n",
    "     Dense(20, activation=tf.nn.relu),\n",
    "     Dense(15, activation=tf.nn.relu),\n",
    "     Dense(7, activation=tf.nn.relu)\n",
    "     ])\n",
    "\n",
    "decoder_net = tf.keras.Sequential(\n",
    "    [\n",
    "     InputLayer(input_shape=(latent_dim,)),\n",
    "     Dense(7, activation=tf.nn.relu),\n",
    "     Dense(15, activation=tf.nn.relu),\n",
    "     Dense(20, activation=tf.nn.relu),\n",
    "     Dense(n_features, activation=None)\n",
    "     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uza6yh-x0SCC"
   },
   "source": [
    "Next you will make use of Alibi Detect's OutlierVAE class and instantiate it using the encoder and decoder architecture you defined above. \n",
    "\n",
    "You will then call the `fit` method on your outlier detector. To learn how to correctly reconstruct normal data the VAE is fit on only inlier examples initially. In this case you will use your `inliers` set as this does not contain any outlying data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LR7tSi920SCC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize outlier detector\n",
    "od = OutlierVAE(threshold=None,  # threshold for outlier score\n",
    "                score_type='mse',  # use MSE of reconstruction error for outlier detection\n",
    "                encoder_net=encoder_net,  # can also pass VAE model instead\n",
    "                decoder_net=decoder_net,  # of separate encoder and decoder\n",
    "                latent_dim=latent_dim,\n",
    "                samples=5) # number of samples drawn during detection for each instance to detect\n",
    "# train\n",
    "od.fit(inliers,\n",
    "       loss_fn=elbo, # Loss function used for training\n",
    "       cov_elbo=dict(sim=.01), # If using the elbo loss, this is the covariance matrix\n",
    "       epochs=50,\n",
    "       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGL9gc6UGMXo"
   },
   "source": [
    "You now need to set the threshold for your outlier detector. This is the score above which any instance will be considered an outlier. \n",
    "\n",
    "To do this you can make use of the `infer_threshold` function which Alibi provides. This will take a batch of data with a specified percentage of outliers, you therefore create a function to generate a dataset with a specified percentage of outliers*.\n",
    "\n",
    "\\*Alibi has a comparable function for generating batches of outlier data called `create_outlier_batch` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8GbfajFGCZV"
   },
   "outputs": [],
   "source": [
    "def generate_outlier_batch(perc_outlier=5, inliers=inliers, outliers=outliers, n_samples=1000):\n",
    "    outlier_idx = int(np.round(n_samples * (perc_outlier / 100), 0))\n",
    "    inlier_idx = int(np.round(n_samples * (1 - perc_outlier / 100), 0))\n",
    "\n",
    "    outlier_batch = outliers[:outlier_idx]\n",
    "    inlier_batch = inliers[:inlier_idx]\n",
    "\n",
    "    y_combined = np.concatenate([np.ones(len(outlier_batch)), np.zeros(len(inlier_batch))]).astype('int32')\n",
    "\n",
    "    X_combined = np.concatenate((outlier_batch, inlier_batch))\n",
    "    return X_combined, y_combined\n",
    "\n",
    "X_threshold, y_threshold = generate_outlier_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr6dIm1h0SCD"
   },
   "source": [
    "Now inferring the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWKoCcojGaEI"
   },
   "outputs": [],
   "source": [
    "perc_outlier = 5\n",
    "od.infer_threshold(X_threshold, threshold_perc=100-perc_outlier)\n",
    "print('New threshold: {}'.format(od.threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsnoR0DZ0SCD"
   },
   "source": [
    "You can now test your threshold by generating a second batch of outlying data, this time with a higher proportion of outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MtuBEz8Ggqp"
   },
   "outputs": [],
   "source": [
    "X_outlier, y_outlier = generate_outlier_batch(perc_outlier=10)\n",
    "print(X_outlier.shape, y_outlier.shape)\n",
    "print('{}% outliers'.format(100 * y_outlier.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6CeSGaf0SCE"
   },
   "source": [
    "Generating outlier predictions from the new detector using the freshly created outlier batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05ss0fUU0SCE"
   },
   "outputs": [],
   "source": [
    "od_preds = od.predict(X_outlier, return_instance_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVACx0nm0SCE"
   },
   "source": [
    "Visualising the effectiveness of our outlier detector using a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOzm_Z0u0SCE"
   },
   "outputs": [],
   "source": [
    "labels = ['normal', 'outlier']\n",
    "\n",
    "y_pred = od_preds['data']['is_outlier']\n",
    "f1 = f1_score(y_outlier, y_pred)\n",
    "print('F1 score: {}'.format(f1))\n",
    "\n",
    "cm = confusion_matrix(y_outlier, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "sns.heatmap(df_cm, annot=True, cbar=True, linewidths=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3JyDLsD0SCF"
   },
   "source": [
    "And then using a scatter plot and ROC curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKHtqIwL0SCF"
   },
   "outputs": [],
   "source": [
    "plot_instance_score(od_preds, y_outlier, labels, od.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ik3N2BLDaf4"
   },
   "outputs": [],
   "source": [
    "roc_data = {'VAE': {'scores': od_preds['data']['instance_score'], 'labels': y_outlier}}\n",
    "plot_roc(roc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYRFODPA0SCF"
   },
   "source": [
    "You can now save your outlier detector locally, and subsequently push to remote storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iw60AftDGcih"
   },
   "outputs": [],
   "source": [
    "save_detector(od, \"outlier_detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnO8tWSrDaf5"
   },
   "source": [
    "NOTE: To push models to GCP you will create a unique folder with your name. Uncomment the command and replace \"YOUR NAME\" with your name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDC-ziyr0SCF"
   },
   "outputs": [],
   "source": [
    "# Recursive copy this time as the OD is saved as a directory containing all the relevant binaries and parameters. \n",
    "!gsutil cp -r outlier_detector gs://tom-seldon-examples/retail-workshop/models/<YOUR NAME>/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJEzSqxcDaf6"
   },
   "source": [
    "Using your freshly uploaded outlier detector you can deploy this alongside your model into Seldon Deploy. Once again you will need to replace 'YOUR NAME' so we're saving our file to correct place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwvrL2k8iQxt"
   },
   "outputs": [],
   "source": [
    "OD_URI = f'gs://tom-seldon-examples/retail-workshop/models/{YOUR_NAME}/outlier_detector/'\n",
    "OD_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS9Df_3MDaf6"
   },
   "source": [
    "You can now test the outlier detector with the following inlier request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLcQK3IhDaf7"
   },
   "outputs": [],
   "source": [
    "{\"data\": {\"ndarray\": [[-0.43831067,  0.23457404, -0.17366155,  0.26935668,  0.13463672, -0.55234561]]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbBUl4IcDaf7"
   },
   "source": [
    "And then with a massive outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODxMCudDDaf8"
   },
   "outputs": [],
   "source": [
    "{\"data\": {\"ndarray\": [[10,  10, 10,  10,  10, 10]]}}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hmI8brYLieU-"
   ],
   "name": "retail_workshop_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
