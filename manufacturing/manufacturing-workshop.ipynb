{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Image Workshop: *Detecting Defects in Steel Strips Using Computer Vision* \n",
    "\n",
    "Within this hands-on workshop you will use a two different convolutional neural networks (CNNs) to classify different types of defects commonly found in steel strips. \n",
    "\n",
    "(And, if you're anything like the author you'll what on Earth defects in steel strips even look like!)\n",
    "\n",
    "The steps which you will carry out are: \n",
    "1. Load the data\n",
    "2. Perform some data exploration and visualisation\n",
    "3. Build a couple of different CNNs and deploy these to Seldon\n",
    "4. Add metadata to your newly created models\n",
    "5. Create a drift detector and once again create this on Seldon\n",
    "\n",
    "The model training code has been adapted from this blog post by `franky` on Medium: [Deep Learning, Computer Vision, and Automated Optical Inspection](https://towardsdatascience.com/deep-learning-computer-vision-and-automated-optical-inspection-774e8ca529d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin you will install/import the necessary dependencies to interact with Seldon Deplay and create your models and monitoring components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Colab has a load of packages pre-loaded into the environment. \n",
    "# Installing the additional ones we require.\n",
    "!pip install seldon_deploy_sdk\n",
    "!pip install alibi_detect==0.6.1\n",
    "!pip install dill\n",
    "!pip install tensorflow==2.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Packages\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import dill\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "# Model Building\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Layer, Reshape, InputLayer, GlobalAveragePooling2D, Flatten\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Drift Detection\n",
    "from alibi_detect.cd import MMDDrift\n",
    "from alibi_detect.cd.tensorflow import preprocess_drift\n",
    "from alibi_detect.utils.saving import save_detector, load_detector\n",
    "\n",
    "# Seldon Deploy SDK\n",
    "from seldon_deploy_sdk import Configuration, ApiClient, SeldonDeploymentsApi, OutlierDetectorApi, DriftDetectorApi, ModelMetadataServiceApi\n",
    "from seldon_deploy_sdk.auth import OIDCAuthenticator\n",
    "\n",
    "# Logging and Clearing Session\n",
    "tf.keras.backend.clear_session()\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Python dependencies have been installed and imported you can download the training and testing data. This will take approximately 2 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/tom-seldon-examples/workshops/manufactoring/data.zip\n",
    "!mkdir data\n",
    "!unzip -o 'data.zip' -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Loading the data into memory, as Keras `ImageDataGenerator` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "data_dir='data/train/images/'\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    directory = data_dir,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1/255)\n",
    "data_dir='data/validation/images/'\n",
    "val_ds = train_datagen.flow_from_directory(\n",
    "    directory = data_dir,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 6 classes which you can view below by loading them into the `categories` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = (train_ds.class_indices)\n",
    "categories = dict((v,k) for k,v in categories.items())\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can readily visualise a sample image to start to gain an understanding of the data you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = train_ds.next()[0][0]\n",
    "print(example_image.shape)\n",
    "plt.imshow(example_image[:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above; the images you'll be working with are 3 things: \n",
    "1. Relatively small, only 224 x 224 pixels. \n",
    "2. In colour as they have 3 channels, but appear greyscale. \n",
    "3. Pretty boring to look at!\n",
    "\n",
    "The image below shows a wider variety of the types of image which are available within the dataset. \n",
    "\n",
    "![steel photo examples](https://raw.githubusercontent.com/SeldonIO/deploy-workshops/master/manufacturing/assets/steel_images.jpeg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Models\n",
    "\n",
    "Now that the data is loaded in and you have had the chance to explore and get to grips with it, you can turn your attention to model building. \n",
    "\n",
    "You will build and work on 2 separate models: \n",
    "1. Training from scratch a simple hand crafted architecture, using 3 convolutional layers. \n",
    "2. Fine tuning an InceptionV3 model which has been pre-trained on the ImageNet dataset. \n",
    "\n",
    "Once these models have been created you are going to deploy them alongside one another as a canary deployment via the Seldon Deploy SDK. \n",
    "\n",
    "Prior to beginning the building efforts create a simple callback which will stop training early if the validation accuracy breaks 90%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('val_accuracy') > 0.90 ):\n",
    "            print(\"\\nReached 90% validation accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True \n",
    "            \n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: *Simple CNN*\n",
    "\n",
    "Using Keras it is straightforward to define the CNN architecture, and this should be familiar if you have worked with CNNs before. \n",
    "\n",
    "Your neural network has three convolutional layers, each with 32 channels and a 3\\*3 convolution. \n",
    "\n",
    "Looking at the model summary you can see the network has a total of approximately 11 million parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)), # First Convolution\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'), # Second Convolution\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'), # Third Convolution\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(6, activation='softmax'),\n",
    "])\n",
    "\n",
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network is then straightforward using standard methods; `categorical_crossentropy` for the loss function, and `adam` as the optimisation technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_cnn.compile(loss = 'categorical_crossentropy',\n",
    "                   optimizer = 'adam',\n",
    "                   metrics = ['accuracy'])\n",
    "\n",
    "simple_cnn_history = simple_cnn.fit(train_ds,\n",
    "                                    batch_size = 32,\n",
    "                                    epochs = 20,\n",
    "                                    validation_data = val_ds,\n",
    "                                    callbacks = [callbacks],\n",
    "                                    verbose = 1,\n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watching a neural network train is the modern equivalent of watching paint dry, therefore to speed things up feel free to interrupt training and grab the pre-trained example available below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://tom-seldon-examples/workshops/manufacturing/pretrained/simple-cnn/1 simple-cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = load_model(\"simple-cnn/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained neural network you can grab a test image and have your network generate a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scratches = load_img('data/validation/images/scratches/scratches_241.jpg',\n",
    "                          target_size=(224, 224, 3))\n",
    "\n",
    "test_scratches = img_to_array(test_scratches)\n",
    "test_scratches = test_scratches / 255\n",
    "test_scratches  = test_scratches.reshape((-1,) + test_scratches.shape)\n",
    "\n",
    "simple_preds = simple_cnn.predict(test_scratches)[0]\n",
    "\n",
    "for v, i in enumerate(simple_preds):\n",
    "    print(f\"{categories[v]}: {i:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the model is... **totally** wrong!\n",
    "\n",
    "Even worse than that, it's confidentally wrong- being fully convinced that the test scratch image has in fact been rolled in scale. \n",
    "\n",
    "The checkpointed model you're relying on here only acheived a validation accuracy of around 78% so this might be expected.\n",
    "\n",
    "Let's see if this can be improved upon by using the InceptionV3 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: *InceptionV3*\n",
    "\n",
    "For your second model you will make use of the pre-trained InceptionV3 architecture. InceptionV3 is a CNN built for object classification by [researchers at Google in 2015](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf). Back then it held the state of the art crown for a while, however more recently has been overtaken by more modern EfficientNet or Transformer architectures. \n",
    "\n",
    "It has been selected here because it is reasonable to train on a CPU, it gets great results and Keras has a built-in version of the model making it super easy to get going with!\n",
    "\n",
    "The key information to note around how you are preparing the InceptionV3 model is that when loaded you do not include the final layers (`include_top=False`). \n",
    "\n",
    "Three extra layers are then built on top of the InceptionV3 model; a pooling layer (`GlobalAveragePooling2D`), and two fully connected layers (`Dense`).\n",
    "\n",
    "These new layers are glued onto InceptionV3 and compiled using the same evaluation methods as you used with the `simple-cnn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for architecture\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 6\n",
    "conv_size = 32\n",
    "\n",
    "# parameters for training\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "\n",
    "# load InceptionV3 from Keras\n",
    "InceptionV3_model = InceptionV3(include_top=False, input_shape=input_shape)\n",
    "\n",
    "# add custom Layers\n",
    "x = InceptionV3_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "custom_output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# define the input and output of the model\n",
    "inception = Model(inputs=InceptionV3_model.input, outputs=custom_output)\n",
    "        \n",
    "# compile the model\n",
    "inception.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_history = inception.fit(train_ds,\n",
    "                                  batch_size=32,\n",
    "                                  epochs=20,\n",
    "                                  validation_data=val_ds,\n",
    "                                  callbacks=[callbacks],\n",
    "                                  verbose=1, \n",
    "                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watching an even larger neural network train is simply like watching an even larger wall dry, so once more you can take advantage of a pre-trained artefact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://tom-seldon-examples/workshops/manufacturing/pretrained/inception/1 inception/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = load_model(\"inception/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the `simple_cnn` built earlier you can test your `inception` model on the same `test_scratch` image to get an anecdotal feel for how it's going to perform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_preds = inception.predict(test_scratches)[0]\n",
    "\n",
    "for v, i in enumerate(inception_preds):\n",
    "    print(f\"{categories[v]}: {i:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hurrah!** \n",
    "\n",
    "This result looks better, and you can also take some reassurance that this pre-trained `inception` model achieved a much more respectable validation accuracy of 94%. \n",
    "\n",
    "Time to start thinking about how you might want to serve your newly created models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Your Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have trained and saved your model artefacts you can begin consider how to deploy them to Seldon. \n",
    "\n",
    "This process will involve uploading the relevant artefacts to blob storage, where they can be pulled from at deployment time. \n",
    "\n",
    "You will then configure connection to a Seldon Deploy cluster, and leverage the pre-built Tensorflow Serving runtime to create your deployment. \n",
    "\n",
    "---\n",
    "\n",
    "To begin with, you will push the `simple-cnn` and `inception` models to a Google storage bucket. It's worth noting that Seldon Deploy reads from a wide range of storage back ends and so all of the popular blob storage tools are covered as well.\n",
    "\n",
    "You will create a unique folder with your name. Uncomment the commands and replace \"YOUR NAME\" with your name.\n",
    "\n",
    "Tensorflow Serving, similarly to Nvidia's Triton, when pulling model artefacts from a directory expects that there are sub-directories representing the version of the model artefacts. TF Serving can then be configured to serve a given model version. \n",
    "\n",
    "Given that you only have the single version for both of your models currently you will simply save version 1. You can see this represented in the `gs` URI; a  `/1` is appended to show this is the first version of your model being deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !gsutil cp -r simple-cnn gs://tom-seldon-examples/workshops/manufacturing/<YOUR NAME>/simple-cnn/1\n",
    "# !gsutil cp -r inception gs://tom-seldon-examples/workshops/manufacturing/<YOUR NAME>/inception/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now deploy your models to the dedicated Seldon Deploy cluster which has been configured for this workshop. To do so you will interact with the Seldon Deploy SDK and deploy your model using that.\n",
    "\n",
    "First, setting up the configuration and authentication required to access the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD_IP = \"34.141.246.254\"\n",
    "config = Configuration()\n",
    "config.host = f\"http://{SD_IP}/seldon-deploy/api/v1alpha1\"\n",
    "config.oidc_client_id = \"sd-api\"\n",
    "config.oidc_server = f\"http://{SD_IP}/auth/realms/deploy-realm\"\n",
    "config.oidc_client_secret = \"sd-api-secret\"\n",
    "config.auth_method = \"client_credentials\"\n",
    "\n",
    "def auth():\n",
    "    auth = OIDCAuthenticator(config)\n",
    "    config.id_token = auth.authenticate()\n",
    "    api_client = ApiClient(configuration=config, authenticator=auth)\n",
    "    return api_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have configured the IP correctly as well as setup the authentication function you can describe the deployment you would like to create. \n",
    "\n",
    "As you have uncovered earlier the `simple-cnn` is not great, and the `inception` model improves dramatically on that performance. This has been demonstrated on your validation data, but neither model has seen any live data. \n",
    "\n",
    "In this scenario you may wish to test both models on the live data, to achieve this Seldon Deploy provides you with two options; a shadow deployment or a canary deployment. \n",
    "\n",
    "These techniques are usually applied to the rollout or upgrade of a new model version. A shadow deployment deploys two models behind a single endpoint, both of the models receive 100% of the traffic- there is no split. However, only one of the models' responses are returned to users- usually the original model you are upgrading. The others responses (new version) are simply logged to Elasticsearch where you can see how it responded compared to the original model. This allows you to roll out new versions of a model, test them on live data, without the risk of poor performance impacting user experience. \n",
    "\n",
    "The other option is a canary deployment; this involves a traffic split between the two models behind a single endpoint. The percentage split is determined by you, and allows you to control how much of the live data goes to each of the models. New versions of models will typically be sent a smaller percentage of traffic (10% or 20%) to validate they are behaving as expected before they are rolled out more fully.\n",
    "\n",
    "In this example you will create a canary deployment with a 70/30 split between the `inception` model and `simple-cnn` respectively. \n",
    "\n",
    "---\n",
    "\n",
    "The [Seldon Deploy SDK](https://github.com/SeldonIO/seldon-deploy-sdk) offers two approaches to creating deployments: \n",
    "1. Working directly with dictionaries, which are equivalent to the underlying Kubernetes manifests. \n",
    "2. Using the higher level Python classes as abstractions. \n",
    "\n",
    "In this example you will use the more direct approach of working with the dictionaries. While this requires some more configuration it does create the opportunity to make reusable templates which are custom to your needs and can be readily shared and understood by others. \n",
    "\n",
    "You will start by defining some variables for the deployment which will be passed to the `mldeployment` manifest object in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = <YOUR NAME>\n",
    "DEPLOYMENT_NAME = f\"{YOUR_NAME}-test\"\n",
    "NAMESPACE = \"default\"\n",
    "\n",
    "MODEL_NAME = \"inception\"\n",
    "MODEL_LOCATION = f\"gs://tom-seldon-examples/workshops/manufacturing/{YOUR_NAME}/{CANARY_NAME}/\"\n",
    "\n",
    "CANARY_NAME = \"simple-cnn\"\n",
    "CANARY_LOCATION = f\"gs://tom-seldon-examples/workshops/manufacturing/{YOUR_NAME}/{MODEL_NAME}/\"\n",
    "\n",
    "CPU_REQUESTS = \"1\"\n",
    "MEMORY_REQUESTS = \"1Gi\"\n",
    "\n",
    "CPU_LIMITS = \"1\"\n",
    "MEMORY_LIMITS = \"1Gi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldeployment = {\n",
    "    \"kind\": \"SeldonDeployment\",\n",
    "    \"metadata\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"namespace\": NAMESPACE\n",
    "    },\n",
    "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "    \"spec\": {\n",
    "        \"name\": DEPLOYMENT_NAME,\n",
    "        \"protocol\": \"seldon\",\n",
    "        \"predictors\": [\n",
    "            {\n",
    "                \"componentSpecs\": [\n",
    "                    {\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": f\"{MODEL_NAME}\",\n",
    "                                    \"resources\": {\n",
    "                                        \"requests\": {\n",
    "                                            \"cpu\": CPU_REQUESTS,\n",
    "                                            \"memory\": MEMORY_REQUESTS\n",
    "                                        },\n",
    "                                        \"limits\": {\n",
    "                                            \"cpu\": CPU_LIMITS,\n",
    "                                            \"memory\": MEMORY_LIMITS\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": \"default\",\n",
    "                \"replicas\": 1,\n",
    "                \"traffic\": 70,\n",
    "                \"graph\": {\n",
    "                    \"implementation\": \"TENSORFLOW_SERVER\",\n",
    "                    \"modelUri\": MODEL_LOCATION,\n",
    "                    \"name\": f\"{MODEL_NAME}\",\n",
    "                    \"logger\": {\n",
    "                        \"mode\": \"all\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"componentSpecs\": [\n",
    "                    {\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [\n",
    "                                {\n",
    "                                    \"name\": f\"{CANARY_NAME}\",\n",
    "                                    \"resources\": {\n",
    "                                        \"requests\": {\n",
    "                                            \"cpu\": CPU_REQUESTS,\n",
    "                                            \"memory\": MEMORY_REQUESTS\n",
    "                                        },\n",
    "                                        \"limits\": {\n",
    "                                            \"cpu\": CPU_LIMITS,\n",
    "                                            \"memory\": MEMORY_LIMITS\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": \"canary\",\n",
    "                \"replicas\": 1,\n",
    "                \"annotations\":{\n",
    "                    \"seldon.io/canary\":\"true\"\n",
    "                },\n",
    "                \"traffic\": 30,\n",
    "                \"graph\": {\n",
    "                    \"implementation\": \"TENSORFLOW_SERVER\",\n",
    "                    \"modelUri\": CANARY_LOCATION,\n",
    "                    \"name\": f\"{CANARY_NAME}\",\n",
    "                    \"logger\": {\n",
    "                        \"mode\": \"all\"\n",
    "                    }\n",
    "                    }\n",
    "                }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have specified your `mldeployment` JSON it is a simple API call to create your deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now test your model endpoint in the UI. To access the workshop cluster use the following details: \n",
    "* Seldon Deploy URL: http://34.147.53.165/seldon-deploy/\n",
    "* Username: admin@seldon.io\n",
    "* Password: 12341234\n",
    "\n",
    "Once you have seen your deployment creating you can save a sample image using the `test-scratches` array you worked with earlier. \n",
    "\n",
    "This `test-scratches.json` can be uploaded directly to the UI to test your model endpoint when it is available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seldon_sample = {\n",
    "  \"data\": {\n",
    "    \"names\": [\n",
    "    ],\n",
    "    \"ndarray\": test_scratches.tolist()\n",
    "  }\n",
    "}\n",
    "\n",
    "with open('test-scratches.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(seldon_sample, f, ensure_ascii=False, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Model Metadata\n",
    "\n",
    "You have now created 2 new models within a single deployment. Both of these models have been automatically added to the model catalog. The model catalog acts as a centralised repository for metadata associated with models. Models can be easily deployed directly from the catalog, while metadata acts to speed knowledge transfer between teams and to track models across tools.\n",
    "\n",
    "You will now add some metadata to your InceptionV3 model describing the validation accuracy you managed to achieve, and who authored the model- so you can brag to your colleagues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_catalog_metadata = {\n",
    "      \"URI\": MODEL_LOCATION,\n",
    "      \"name\": \"InceptionV3\",\n",
    "      \"version\": \"v1.0\",\n",
    "      \"artifactType\": \"TENSORFLOW\",\n",
    "      \"taskType\": \"defect classification\",\n",
    "      \"tags\": {\n",
    "        \"auto_created\": \"true\",\n",
    "        \"author\": f\"{YOUR_NAME}\"\n",
    "      },\n",
    "      \"metrics\": {},\n",
    "      \"creationTime\": \"2022-02-15T15:26:26.630592Z\",\n",
    "      \"project\": \"default\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_instance = ModelMetadataServiceApi(auth())\n",
    "api_response = api_instance.model_metadata_service_update_model_metadata(model_catalog_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing the metadata for your newly updated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = api_instance.model_metadata_service_list_model_metadata(uri=MODEL_LOCATION)\n",
    "api_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drift Detection\n",
    "\n",
    "The final stage of the deployment process is going to be adding the advanced monitoring capabilities afforded by a drift detector. \n",
    "\n",
    "In this example you will use Alibi Detect to train a custom drift detector which can flag when the underlying input data distribution has shifted. This can inform decisions about re-training or prompt deeper investigation into data/model behaviours. \n",
    "\n",
    "Seldon Deploy also allows you to setup alerts when drift is detected. \n",
    "\n",
    "---\n",
    "\n",
    "In this example you will use the Maximum Mean Discrepancy method. Covariate or input drift detection relies on creating a distance measure between two distributions; a reference distribution and a new distribution. The MMD drift detector is no different; the mean embeddings of your features are used to generate the distributions and then the distance between them is measured. The training data is used to calculate the reference distribution, while the new distribution comes from your inference data.  \n",
    "\n",
    "More technically, a reproducing kernel Hilbert space is used to generate the mean embeddings, by mapping the highly complex feature space within which most machine learning models operate to a linear Euclidean space. A radial basis function kernel is then used to measure the distance between the two embeddings, and the signifiance of the drift is calculated as a p-value using permutation/resampling tests. More details can be found [here](https://docs.seldon.io/projects/alibi-detect/en/v0.6.1/methods/mmddrift.html).\n",
    "\n",
    "Before you dive into creating your own drift detector, you need to generate a reference dataset. In this case you use 5 batches of the training data set, resulting in 160 images. This has been picked for convenience and speed of training in the workshop, and if this was a production case you would likely want to use the entirety of the training set, or a statistically significant segment.\n",
    "\n",
    "Now, creating your reference set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.reset()\n",
    "\n",
    "drift_ref = train_ds.next()[0]\n",
    "\n",
    "for i in range(4):\n",
    "    drift_ref = np.concatenate((drift_ref, train_ds.next()[0]))\n",
    "\n",
    "drift_ref.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the high dimensionality of image data you will need perform a pre-processing step to make it computationally feasible to detect drift on incoming batches of images. \n",
    "\n",
    "A great option for this is to use an simple encoding neural network. This will squish images from their original shape of `(224, 224, 3)` to a vector of `32`. From there we can feed the vectors to the drift detector and calculate drift values. \n",
    "\n",
    "This is then used to a generate a `preprocess_fn` which will be fed to your MMD drift detector to convert your images ahead of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define encoder\n",
    "encoding_dim = 32\n",
    "encoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=(224, 224, 3)),\n",
    "      Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Flatten(),\n",
    "      Dense(encoding_dim,)\n",
    "  ]\n",
    ")\n",
    "\n",
    "# define preprocessing function\n",
    "preprocess_fn = partial(preprocess_drift, model=encoder_net, batch_size=160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you can initialise the drift detector; this is as simple as a single line API call specifying the:\n",
    "* Reference data\n",
    "* Computational backend\n",
    "* The p-value below which drift is considered to have occurred\n",
    "* Your preprocessing function\n",
    "* The number of permutations taken to calculate the p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise drift detector\n",
    "cd = MMDDrift(drift_ref, backend='tensorflow', p_val=.05, preprocess_fn=preprocess_fn, n_permutations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your shiny new drift detector has trained you can test it on a new batch of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch = train_ds.next()[0]\n",
    "preds = cd.predict(new_batch, return_p_val=True, return_distance=True)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can save the drift detector and upload it to Google storage for deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'defect-drift'\n",
    "save_detector(cd, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r defect-drift gs://tom-seldon-examples/manufacturing/models/<YOUR NAME>/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"gs://tom-seldon-examples/manufacturing/models/<YOUR NAME>/{filepath}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc73de081e7d4a38eca8cd577af2439f92394d26390d4f1b2de20a23a409ad93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
