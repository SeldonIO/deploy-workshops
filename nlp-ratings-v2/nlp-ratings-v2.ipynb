{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Workshop - *Predicting product ratings from reviews* - Using MLServer and V2 Inference Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop is focused on the deployment of a machine learning model for predicting product ratings from reviews.\n",
    "\n",
    "We will use a fine tuned [DistilBERT hugging face transformer model](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert), which is a \"small, fast, cheap and light Transformer model trained by distilling BERT base\". For the sake of time, we will use a pretrained model rather than training the model in the workshop. The model is stored in the ```kelly-seldon``` Google Storage bucket at the path ```nlp-ratings/model/1```.\n",
    "\n",
    "We will deploy our trained model through the Seldon Deploy UI and view our running deployment. We will deploy our model using MLServer, which is on open source inference server for ML models. MLServer has support for for the [standard V2 Inference Protocol](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html) on both the gRPC and REST flavours, which has been standardised and adopted by various model serving frameworks. Full MLServer docs can be found [here](https://mlserver.readthedocs.io/en/latest/).\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Model using MLSever and V2 Inference Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews are preprocessed in a variety of different steps before being passed to the model. In order to account for this there are 2 options we can use:\n",
    "\n",
    "1. **Custom Model:** Incorporate the pre-processing directly in the predict method of a custom model. This provides simplicity when creating the deployment as there is only a single code base to worry about and a single component to be deployed.\n",
    "2. **Input Transformer:** Make use of a separate container to perform all of the input transformation and then pass the vectors to the model for prediction. The schematic below outlines how this would work.\n",
    "\n",
    "```\n",
    "            ________________________________________\n",
    "            |            SeldonDeployment          |\n",
    "            |                                      |\n",
    "Request -->  Input transformer   -->     Model    -->  Response\n",
    "            |  (Pre-processing)                    |\n",
    "            |                                      |\n",
    "            ________________________________________\n",
    "```\n",
    "         \n",
    "The use of an input transformer allows us to separate the pre-processing logic from the prediction logic. This means thar each of the components can be upgraded independently of one another. However, it does introduce additional complexity in the deployment which is generated, and how that then interacts with advanced monitoring components such as outlier and drift detectors. \n",
    "\n",
    "This workshop will focus on the generation of a **custom model for this case**.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up \n",
    "\n",
    "We can define our Seldon custom model. The component parts required to build the custom model are outlined below. Each of the files play a key part in building the eventual docker image.\n",
    "\n",
    "---\n",
    "\n",
    "### ratings.py\n",
    "\n",
    "\n",
    "This is the critical file as it contains the logic associated with the deployment wrapped as part of a class.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, DefaultDataCollator, TFAutoModelForSequenceClassification\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.utils import get_model_uri\n",
    "from mlserver.types import (\n",
    "    InferenceRequest,\n",
    "    InferenceResponse\n",
    ")\n",
    "from mlserver.codecs import NumpyRequestCodec, PandasCodec\n",
    "from mlserver.logging import logger\n",
    "\n",
    "\n",
    "class ReviewRatings(MLModel):\n",
    "    async def load(self) -> bool:\n",
    "        model_uri = await get_model_uri(\n",
    "            self.settings\n",
    "        )\n",
    "\n",
    "        logger.info(\"Loading model\")\n",
    "        self._model = TFAutoModelForSequenceClassification.from_pretrained(model_uri, num_labels=9)\n",
    "        logger.info(\"Model successfully loaded\")\n",
    "\n",
    "        self._wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        logger.info(\"Loading tokenizer and data collator\")\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self._data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "        nltk.download(\"stopwords\", download_dir=\"./nltk\")\n",
    "        nltk.download(\"wordnet\", download_dir=\"./nltk\")\n",
    "        nltk.download(\"omw-1.4\", download_dir=\"./nltk\")\n",
    "        nltk.data.path.append(\"./nltk\")\n",
    "        # Stop words present in the library\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "        self.ready = True\n",
    "        return self.ready\n",
    "\n",
    "    async def predict(self, payload: InferenceRequest) -> InferenceResponse:\n",
    "        review_df = self.decode_request(payload, default_codec=PandasCodec)\n",
    "        pred_proc = self.process_whole(review_df)\n",
    "        response = NumpyRequestCodec.encode_response(model_name=self.name, payload=pred_proc)\n",
    "        response.outputs[0].shape = [response.outputs[0].shape[0], 1]\n",
    "\n",
    "        return response\n",
    "\n",
    "    def preprocess_text(self, df, feature_names):\n",
    "        logger.info(\"Preprocessing text\")\n",
    "        logger.info(\"Removing punctuation\")\n",
    "        df['review'] = df['review'].apply(lambda x: self.remove_punctuation(x))\n",
    "        logger.info(\"Lowercase all characters\")\n",
    "        df['review'] = df['review'].apply(lambda x: x.lower())\n",
    "        logger.info(\"Removing stopwords\")\n",
    "        df['review'] = df['review'].apply(lambda x: self.remove_stopwords(x))\n",
    "        logger.info(\"Carrying out lemmatization\")\n",
    "        df['review'] = df['review'].apply(lambda x: self.lemmatizer(x))\n",
    "\n",
    "        len_df = len(df)\n",
    "        logger.info(f\"{len(df)}\")\n",
    "\n",
    "        dataset = datasets.Dataset.from_pandas(df, preserve_index=False)\n",
    "        logger.info(f\"Dataset created: {dataset}\")\n",
    "\n",
    "        tokenized_revs = dataset.map(self.tokenize, batched=True)\n",
    "        logger.info(f\"Tokenized reviews: {tokenized_revs}\")\n",
    "\n",
    "        logger.info(\"Converting tokenized reviews to tf dataset\")\n",
    "        tf_inf = tokenized_revs.to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\"],\n",
    "            label_cols=[\"labels\"],\n",
    "            shuffle=True,\n",
    "            batch_size=len_df,\n",
    "            collate_fn=self._data_collator\n",
    "        )\n",
    "        logger.info(f\"TF dataset created: {tf_inf}\")\n",
    "\n",
    "        return tf_inf\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        punctuation_free = \"\".join([i for i in text if i not in string.punctuation])\n",
    "        return punctuation_free\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        text = ' '.join([word for word in text.split() if word not in self.stopwords])\n",
    "        return text\n",
    "\n",
    "    def lemmatizer(self, text):\n",
    "        lemm_text = ' '.join([self._wordnet_lemmatizer.lemmatize(word) for word in text.split()])\n",
    "        return lemm_text\n",
    "\n",
    "    def tokenize(self, ds):\n",
    "        return self._tokenizer(ds[\"review\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    def process_output(self, preds):\n",
    "        logger.info(\"Processing model predictions\")\n",
    "        rating_preds = []\n",
    "        for i in preds[\"logits\"]:\n",
    "            rating_preds.append(np.argmax(i, axis=0))\n",
    "\n",
    "        logger.info(\"Create output array for predictions\")\n",
    "        rating_preds = np.array(rating_preds)\n",
    "\n",
    "        return rating_preds\n",
    "\n",
    "    def process_whole(self, text):\n",
    "        logger.info(\"Start processing\")\n",
    "        tf_inf = self.preprocess_text(text, feature_names=None)\n",
    "        logger.info(\"Predictions ready to be made\")\n",
    "        preds = self._model.predict(tf_inf)\n",
    "        logger.info(f\"Prediction type: {type(preds)}\")\n",
    "        logger.info(f\"Predictions: {preds}\")\n",
    "        preds_proc = self.process_output(preds)\n",
    "        logger.info(f\"Processed predictions: {preds_proc}, Processed predictions type: {type(preds_proc)}\")\n",
    "\n",
    "        return preds_proc\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom inference wrapper should be responsible for: \n",
    "\n",
    "1. Loading the model\n",
    "2. Running inference using our model structure\n",
    "\n",
    "Some things to note: \n",
    "\n",
    "- The `ReviewRatings` class inherits from `mlserver.MLModel`.\n",
    "- There are 2 expected `async` methods - `load` and `predict`:\n",
    "    - The `load` method is used to load the model and any other artefacts, in this example the lemmatizer, the tokenizer and the data collator. \n",
    "    - The `predict` method takes the inference request, makes custom predictions, and outputs the inference response.\n",
    "- We use the mlserver `logger` from `mlserver.logging`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Types (and Codecs)\n",
    "\n",
    "Machine learning models generally expect their inputs to be passed down as a particular Python type. Most commonly, this type ranges from “general purpose” NumPy arrays or Pandas DataFrames to more granular definitions, like datetime objects, Pillow images, etc. Unfortunately, the definition of the V2 Inference Protocol doesn’t cover any of the specific use cases. This protocol can be thought of a wider “lower level” spec, which only defines what fields a payload should have.\n",
    "\n",
    "To account for this gap, MLServer introduces support for content types, which offer a way to let MLServer know how it should “decode” V2-compatible payloads. When shaped in the right way, these payloads should “encode” all the information required to extract the higher level Python type that will be required for a model.\n",
    "\n",
    "To let MLServer know that a particular payload must be decoded / encoded as a different Python data type (e.g. NumPy Array, Pandas DataFrame, etc.), you can specifity it through the content_type field of the parameters section of your request.\n",
    "\n",
    "It’s important to keep in mind that content types can be specified at both the request level and the input level. The former will apply to the entire set of inputs, whereas the latter will only apply to a particular input of the payload.\n",
    "\n",
    "\n",
    "\n",
    "### Example Request \n",
    "\n",
    "```\n",
    "{\n",
    "    \"parameters\": {\n",
    "        \"content_type\": \"pd\"\n",
    "    },\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"review\",\n",
    "          \"shape\": [1, 1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"_product is excellent! I love it, it's great!\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Under the hood, the conversion between content types is implemented using codecs. In the MLServer architecture, codecs are an abstraction which know how to encode and decode high-level Python types to and from the V2 Inference Protocol.\n",
    "\n",
    "Depending on the high-level Python type, encoding / decoding operations may require access to multiple input or output heads. For example, a Pandas Dataframe would need to aggregate all of the input-/output-heads present in a V2 Inference Protocol response. However, a Numpy array or a list of strings, could be encoded directly as an input head within a larger request.\n",
    "\n",
    "More information on Content Types (and Codecs) can be found [here](https://mlserver.readthedocs.io/en/latest/user-guide/content-type.html).\n",
    "\n",
    "Within the `predict` method of the above `ratings.py` file, the following 2 lines can be seen in the `predict` method:\n",
    "\n",
    "- Decoding Request:\n",
    "    `review_df = self.decode_request(payload, default_codec=PandasCodec)`\n",
    "    \n",
    "This is a request codec, which works at the request- / response level and is used in this example to decode the V2-compatible payload to a DataFrame (as specified in the example request above).\n",
    "    \n",
    "- Encoding Response: \n",
    "    `response = NumpyRequestCodec.encode_response(model_name=self.name, payload=pred_proc)`\n",
    "\n",
    "This is also a request codec, which works at at the request- / response level and is used in this example to encode the NumPy Array to a V2-compatible payload.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When serving a custom model, MLServer also expects the following 2 configuration files: \n",
    "\n",
    "- `settings.json`: holds the configuration of our server (e.g. ports, log level, etc.).\n",
    "- `model-settings.json`: holds the configuration of our model (e.g. input type, runtime to use, etc.).\n",
    "\n",
    "In this case, our files are as follows: \n",
    "\n",
    "`settings.json` \n",
    "\n",
    "```\n",
    "{\n",
    "    \"debug\": \"true\"\n",
    "}\n",
    "```\n",
    "\n",
    "`model-settings.json`\n",
    "\n",
    "```\n",
    "{\n",
    "    \"implementation\": \"ratings.ReviewRatings\",\n",
    "    \"parameters\": {\n",
    "        \"uri\": \"./1/\"\n",
    "    },\n",
    "    \"parallel_workers\": 0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements.txt\n",
    "\n",
    "We also have our ```requirements.txt``` file, which contains a list of Python packages which the deployment requires to run:\n",
    "\n",
    "```\n",
    "datasets == 2.2.2\n",
    "numpy == 1.21.6\n",
    "pandas == 1.3.5\n",
    "tensorflow == 2.7.3\n",
    "transformers == 4.20.0\n",
    "mlserver\n",
    "nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Locally\n",
    "\n",
    "First, we must download the model artifact so that we have it locally available for testing.  You can run the following command to get it from Google Storage:\n",
    "\n",
    "```\n",
    "gsutil cp -r gs://kelly-seldon/nlp-ratings/1/ .\n",
    "```\n",
    "\n",
    "Now that we have our config in-place, we can start the server by running mlserver start .. This needs to either be ran from the same directory where our config files are or pointing to the folder where they are.\n",
    "\n",
    "```\n",
    "mlserver start .\n",
    "```\n",
    "\n",
    "Since this command will start the server and block the terminal, waiting for requests, this will need to be ran in the background on a separate terminal.\n",
    "\n",
    "We can then test sending requests by running the following python script: \n",
    "\n",
    "`test_request.py`\n",
    "\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "inference_request = {\n",
    "    \"parameters\": {\n",
    "        \"content_type\": \"pd\"\n",
    "    },\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"review\",\n",
    "          \"shape\": [1, 1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"_product is excellent! I love it, it's great!\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "endpoint = \"http://localhost:8080/v2/models/nlp-ratings-v2/infer\"\n",
    "response = requests.post(endpoint, json=inference_request)\n",
    "\n",
    "print(response.json())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment \n",
    "\n",
    "Now that we have written and tested our custom model, the next step is to deploy it. With that goal in mind, the rough outline of steps will be to first build a custom image containing our code, and then deploy it using the SDK.\n",
    "\n",
    "### Building a custom image \n",
    "\n",
    "MLServer offers helpers to build a custom Docker image containing your code. In this example, we will use the mlserver build subcommand to create an image, which we’ll be able to deploy later.\n",
    "\n",
    "Note that this section expects that Docker is available and running in the background.\n",
    "\n",
    "We will run: \n",
    "\n",
    "```\n",
    "mlserver build . -t kellyspry0316/ratings-v2:0.3\n",
    "```\n",
    "\n",
    "To ensure that the image is fully functional, we can spin up a container and then send a test request. To start the container, you can run the following in a separate terminal:\n",
    "\n",
    "```\n",
    "docker run -test --rm -p 8080:8080 kellyspry0316/ratings-v2:0.3\n",
    "```\n",
    "\n",
    "We can then test sending requests again using the above request. \n",
    "\n",
    "Finally, we can push our image to a docker registry, in this case we will just push it to Docker Hub:\n",
    "\n",
    "```\n",
    "docker push kellyspry0316/ratings-v2:0.3\n",
    "```\n",
    "\n",
    "Now that we’ve built a custom image and verified that it works as expected, we can move to the next step and deploy it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying through the UI \n",
    "\n",
    "We will deploy the model using the UI for this session, so we will jump over there now.\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "- Set the `Protocol` to `V2 Inference`. \n",
    "- Set the `Runtime` to `Custom`.\n",
    "- Set the `Docker Image` to `kellyspry0316/ratings-v2:0.3`.\n",
    "- Add an ENVIRONMENT VARIABLE to always use a writable HuggingFace cache location regardless of the user: \n",
    "\n",
    "    - Variable: `TRANSFORMERS_CACHE`\n",
    "    - Value: `/opt/mlserver/.cache`\n",
    "    \n",
    "- Set CPU Requests and Limits to 1\n",
    "- Set Memory Request and Limits to 3Gi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Now you can have a go at sending some requests to your model using the 'Predict' tab in the UI.\n",
    "\n",
    "An example of a good review that we would expect to correspond to a higher rating.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"parameters\": {\n",
    "        \"content_type\": \"pd\"\n",
    "    },\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"review\",\n",
    "          \"shape\": [1, 1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"_product is excellent! I love it, it's great!\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "And an example of a negative review that we would expect to correspond to a lower rating.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"parameters\": {\n",
    "        \"content_type\": \"pd\"\n",
    "    },\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"review\",\n",
    "          \"shape\": [1, 1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"_product_ was terrible, I would not use it again, it was awful!\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "We successfully packaged up and deployed a model on Seldon Deploy using MLServer with the V2 Protocol! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('nlp-ratings')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "378c94c32196270942c46fe88d8715d40a73ef2f3068b5e0754dc97e8bc75c14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
