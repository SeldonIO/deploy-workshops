{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f34745",
   "metadata": {},
   "source": [
    "## Hands-On Workshop: Wine classifier with MLFlow, Seldon, and the v2 protocol\n",
    "\n",
    "In this workshop, we will run an ML Flow model, save the artefacts to MinIO, and create a v2 protocol SeldonDeployment via the SDK\n",
    "\n",
    "We will follow these steps:\n",
    "1. Setup environment\n",
    "2. Run training experiments using MLFlow\n",
    "3. Conda pack the environment\n",
    "4. Push artefacts to Google Storage\n",
    "5. Deploy via the SDK\n",
    "6. Using MLServer codecs\n",
    "7. [Optional] Create a new MLFlow run and deploy a canary model\n",
    "8. [Optional] Train and deploy a drift detector\n",
    "9. [Optional] Train and deploy an explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c6b764",
   "metadata": {},
   "source": [
    "### 1. Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52147ab0",
   "metadata": {},
   "source": [
    "In your terminal, create a new virtual environment:\n",
    "`conda create -n mlflow-wine python=3.8 ipykernel -y`\n",
    "\n",
    "Activate the new environment:\n",
    "`conda activate mlflow-wine`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e0e720",
   "metadata": {},
   "source": [
    "Install required dependencies into your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seldon_deploy_sdk\n",
    "!pip install conda-pack\n",
    "!pip install mlserver\n",
    "!pip install mlserver-mlflow\n",
    "!pip install sklearn\n",
    "!pip install mlflow\n",
    "!sudo apt install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seldon_deploy_sdk import Configuration, PredictApi, ApiClient, SeldonDeploymentsApi, ModelMetadataServiceApi, DriftDetectorApi, BatchJobsApi, BatchJobDefinition\n",
    "from seldon_deploy_sdk.auth import OIDCAuthenticator\n",
    "from seldon_deploy_sdk.rest import ApiException\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e6c32",
   "metadata": {},
   "source": [
    "### 2. Run training experiments using MLFlow\n",
    "For our example, we will use the elastic net wine example from [MLflow's tutorial](https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine).  We'll use Scikit-learn to predict the `quality` of the wine given certain attributes as features.\n",
    "\n",
    "Let's load the data to see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56218ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702014d9",
   "metadata": {},
   "source": [
    "Below we'll define a `train` function that can be run with two hyperparameters as inputs (`alpha` and `l1_ratio`) to train an ElasticNet linear model from Scikit-Learn.  It will download the dataset, define the train/test split, start an MLFLow run, log params and metrics, and save the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409afec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine Quality Sample\n",
    "def train(in_alpha, in_l1_ratio):\n",
    "    import os\n",
    "    import warnings\n",
    "    import sys\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.WARN)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    def eval_metrics(actual, pred):\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        return rmse, mae, r2\n",
    "\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    np.random.seed(40)\n",
    "\n",
    "    # Read the wine-quality csv file from the URL\n",
    "    csv_url =\\\n",
    "        'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "    try:\n",
    "        data = pd.read_csv(csv_url, sep=';')\n",
    "    except Exception as e:\n",
    "        logger.exception(\n",
    "            \"Unable to download training & test CSV, check your internet connection. Error: %s\", e)\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train, test = train_test_split(data)\n",
    "\n",
    "    # The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "    train_x = train.drop([\"quality\"], axis=1)\n",
    "    test_x = test.drop([\"quality\"], axis=1)\n",
    "    train_y = train[[\"quality\"]]\n",
    "    test_y = test[[\"quality\"]]\n",
    "\n",
    "    # Set default values if no alpha is provided\n",
    "    if float(in_alpha) is None:\n",
    "        alpha = 0.5\n",
    "    else:\n",
    "        alpha = float(in_alpha)\n",
    "\n",
    "    # Set default values if no l1_ratio is provided\n",
    "    if float(in_l1_ratio) is None:\n",
    "        l1_ratio = 0.5\n",
    "    else:\n",
    "        l1_ratio = float(in_l1_ratio)\n",
    "\n",
    "    # Useful for multiple runs (only doing one run in this sample notebook)    \n",
    "    with mlflow.start_run():\n",
    "        # Execute ElasticNet\n",
    "        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "        lr.fit(train_x, train_y)\n",
    "\n",
    "        # Evaluate Metrics\n",
    "        predicted_qualities = lr.predict(test_x)\n",
    "        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "        # Print out metrics\n",
    "        print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n",
    "        print(\"  RMSE: %s\" % rmse)\n",
    "        print(\"  MAE: %s\" % mae)\n",
    "        print(\"  R2: %s\" % r2)\n",
    "\n",
    "        # Log parameter, metrics, and model to MLflow\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "        mlflow.sklearn.log_model(lr, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efc9b2",
   "metadata": {},
   "source": [
    "Now we'll train a few models with various combinations of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2926f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6011bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(0.1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff4a2e",
   "metadata": {},
   "source": [
    "The model training runs have been saved in the `mlruns` folder, under the experiment `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 1 mlruns/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc8847",
   "metadata": {},
   "source": [
    "Inside each of these folders, MLflow stores the parameters we used to train our model, any metric we logged during training, and a snapshot of our model. If we look into one of them, we can see the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca6ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree mlruns/0/$(ls mlruns/0 | head -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50cf3e1",
   "metadata": {},
   "source": [
    "In particular, we are interested in `artifacts/model` directory in order to deploy your model.  The `MLmodel` file describes the \"flavor\" for that model, including details on how to serve the model.  This helps deployment tools like Seldon deploy MLFlow models in a consistent way, while retaining the ability to train using various frameworks in MLFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat mlruns/0/$(ls mlruns/0 | head -1)/artifacts/model/MLmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32794277",
   "metadata": {},
   "source": [
    "We can also view the dependencies that will be required to serve the model (note that we Seldon will require the MLServer package as well when deploying using the v2 protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2800bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat mlruns/0/$(ls mlruns/0 | head -1)/artifacts/model/conda.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801ff27",
   "metadata": {},
   "source": [
    "### 4. Push artefacts to Google Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f4327",
   "metadata": {},
   "source": [
    "We'll first select the model that scored the best in terms of `rmse`, and get the run ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658fa507",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df = mlflow.search_runs(filter_string=\"metrics.rmse < 1\")\n",
    "runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = runs_df.loc[runs_df['metrics.rmse'].idxmin()]['run_id']\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r mlruns/0/$run_id/artifacts/model/ gs://andrew-seldon/mlflow/elastic_wine_$run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://andrew-seldon/mlflow/elastic_wine_$run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c919778",
   "metadata": {},
   "source": [
    "### 5. Deploy via the SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa9fb6b",
   "metadata": {},
   "source": [
    "We will now deploy the model to Seldon Deploy using the SDK. Because the MLFlow server will need to create the conda environment, we propose two alternative methods for deployment:\n",
    "1. Manually set the liveness and readiness probes; deploy using the `seldon` protocol\n",
    "2. Conda-pack the environment; deploy using the `v2 protocol`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2695a82",
   "metadata": {},
   "source": [
    "Create a function to authenticate against the cluster.  Remember to replace `XXXXX` with your cluster IP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "SD_IP = \"XXXXX\"\n",
    "\n",
    "config = Configuration()\n",
    "config.host = f\"http://{SD_IP}/seldon-deploy/api/v1alpha1\"\n",
    "config.oidc_client_id = \"sd-api\"\n",
    "config.oidc_server = f\"http://{SD_IP}/auth/realms/deploy-realm\"\n",
    "config.oidc_client_secret = \"sd-api-secret\"\n",
    "config.auth_method = \"client_credentials\"\n",
    "\n",
    "def auth():\n",
    "    auth = OIDCAuthenticator(config)\n",
    "    config.id_token = auth.authenticate()\n",
    "    api_client = ApiClient(configuration=config, authenticator=auth)\n",
    "    return api_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f590f2",
   "metadata": {},
   "source": [
    "#### Method 1: Deploy with the Seldon protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb337b",
   "metadata": {},
   "source": [
    "During initialisation, the built-in reusable server will create the Conda environment specified on your conda.yaml file.  However, because this build can take some time, we must extend the initialDelaySeconds and failureThreshold for the probes on our deployment.\n",
    "\n",
    "To use the built-in MLflow server the following pre-requisites need to be met:\n",
    "\n",
    "* Your MLmodel artifact folder needs to be accessible remotely (e.g. in Google Storage).\n",
    "\n",
    "* Your model needs to be compatible with the python_function flavour.\n",
    "\n",
    "* Your MLproject environment needs to be specified using Conda.\n",
    "\n",
    "Note that your cluster will also need to be able to access the internet to download all the requirements in the conda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251e49e",
   "metadata": {},
   "source": [
    "Please set `your_name` below to avoid namespace clashes in deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b77eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = \"name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab94e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_NAME = f\"wine-{your_name}-seldon\"\n",
    "NAMESPACE = \"seldon-demos\"\n",
    "MODEL_LOCATION = f\"gs://andrew-seldon/mlflow/elastic_wine_{run_id}\"\n",
    "\n",
    "mldeployment = {\n",
    "  \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "  \"kind\": \"SeldonDeployment\",\n",
    "  \"metadata\": {\n",
    "    \"name\": f\"{DEPLOYMENT_NAME}\",\n",
    "    \"namespace\": f\"{NAMESPACE}\"\n",
    "  },\n",
    "  \"spec\": {\n",
    "    \"name\": f\"{DEPLOYMENT_NAME}\",\n",
    "    \"protocol\":\"seldon\",\n",
    "    \"predictors\": [\n",
    "      {\n",
    "        \"componentSpecs\": [\n",
    "          {\n",
    "            \"spec\": {\n",
    "              \"containers\": [\n",
    "                {\n",
    "                  \"name\": f\"{DEPLOYMENT_NAME}-container\",\n",
    "                  # We are setting high failureThreshold as installing conda dependencies\n",
    "                  # can take a long time and we want to avoid k8s killing the container prematurely\n",
    "                  \"livenessProbe\": {\n",
    "                    \"initialDelaySeconds\": 100,\n",
    "                    \"failureThreshold\": 300,\n",
    "                    \"periodSeconds\": 5,\n",
    "                    \"successThreshold\": 1,\n",
    "                    \"httpGet\": {\n",
    "                      \"path\": \"/health/ping\",\n",
    "                      \"port\": \"http\",\n",
    "                      \"scheme\": \"HTTP\"\n",
    "                    }\n",
    "                  },\n",
    "                  \"readinessProbe\": {\n",
    "                    \"initialDelaySeconds\": 100,\n",
    "                    \"failureThreshold\": 300,\n",
    "                    \"periodSeconds\": 5,\n",
    "                    \"successThreshold\": 1,\n",
    "                    \"httpGet\": {\n",
    "                      \"path\": \"/health/ping\",\n",
    "                      \"port\": \"http\",\n",
    "                      \"scheme\": \"HTTP\"\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"graph\": {\n",
    "          \"children\": [],\n",
    "          \"implementation\": \"MLFLOW_SERVER\",\n",
    "          \"modelUri\": f\"{MODEL_LOCATION}\",\n",
    "          \"name\": f\"{DEPLOYMENT_NAME}-container\"\n",
    "        },\n",
    "        \"name\": \"default\",\n",
    "        \"replicas\": 1\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4642b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e9b87",
   "metadata": {},
   "source": [
    "Wait for the deployment to become available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for Seldon Deployment to become available\")\n",
    "deployment_status = 'Not Ready'\n",
    "while deployment_status != \"Available\":\n",
    "    try:\n",
    "        api_response = deployment_api.read_seldon_deployment(DEPLOYMENT_NAME, NAMESPACE)\n",
    "        print(f\"Deployment Status: {api_response.status.state}\")\n",
    "        deployment_status = api_response.status.state\n",
    "    except ApiException as e:\n",
    "        print(\"Exception when calling SeldonDeploymentsApi->read_seldon_deployment: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392a1e59",
   "metadata": {},
   "source": [
    "You can now test a prediction in the Seldon Deploy UI by using the following payload:\n",
    "\n",
    "```\n",
    "{\"data\": {\"names\": [], \"ndarray\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8]]}}\n",
    "```\n",
    "\n",
    "You should see a 200 response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dff0a1",
   "metadata": {},
   "source": [
    "#### Method 2: Deploy with the v2 protocol\n",
    "In order to use the v2 protocol, it is best to use [`conda-pack`](https://conda.github.io/conda-pack/) to locally save the conda environment (including mlserver) to a tar file.  The initialiser can then use this to install required dependencies into the container.  We are planning to simplify this workflow in future releases.\n",
    "\n",
    "Note that if you want to pack a specific conda environment, you can specify that environment like so: \n",
    "\n",
    "`conda pack -n mlflow-wine -o mlruns/0/$run_id/artifacts/model/environment.tar.gz -f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda pack -n mlflow-wine -o mlruns/0/$run_id/artifacts/model/environment.tar.gz -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f92a9",
   "metadata": {},
   "source": [
    "Push the environment.tar.gz file to Google Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r mlruns/0/$run_id/artifacts/model/environment.tar.gz gs://andrew-seldon/mlflow/elastic_wine_$run_id/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f347116",
   "metadata": {},
   "source": [
    "It should be enough to simply specify the v2 protocol.  You no longer need to adjust the liveness and readiness probes.  Note that this deployment can also be done via the UI in Seldon Deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_NAME = f\"wine-{your_name}-v2\"\n",
    "NAMESPACE = \"seldon-demos\"\n",
    "MODEL_LOCATION = f\"gs://andrew-seldon/mlflow/elastic_wine_{run_id}\"\n",
    "\n",
    "\n",
    "mldeployment = {\n",
    "  \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "  \"kind\": \"SeldonDeployment\",\n",
    "  \"metadata\": {\n",
    "    \"name\": f\"{DEPLOYMENT_NAME}\",\n",
    "    \"namespace\": f\"{NAMESPACE}\"\n",
    "  },\n",
    "  \"spec\": {\n",
    "    \"protocol\": \"v2\",\n",
    "    \"name\": f\"{DEPLOYMENT_NAME}\",\n",
    "    \"predictors\": [\n",
    "      {\n",
    "        \"graph\": {\n",
    "          \"children\": [],\n",
    "          \"implementation\": \"MLFLOW_SERVER\",\n",
    "          \"modelUri\": f\"{MODEL_LOCATION}\",\n",
    "          \"name\": f\"{DEPLOYMENT_NAME}-container\"\n",
    "        },\n",
    "        \"name\": \"default\",\n",
    "        \"replicas\": 1\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995cc81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_api = SeldonDeploymentsApi(auth())\n",
    "deployment_api.create_seldon_deployment(namespace=NAMESPACE, mldeployment=mldeployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd1815",
   "metadata": {},
   "source": [
    "Wait until the deployment becomes available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78842c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for Seldon Deployment to become available\")\n",
    "deployment_status = 'Not Ready'\n",
    "while deployment_status != \"Available\":\n",
    "    try:\n",
    "        api_response = deployment_api.read_seldon_deployment(DEPLOYMENT_NAME, NAMESPACE)\n",
    "        print(f\"Deployment Status: {api_response.status.state}\")\n",
    "        deployment_status = api_response.status.state\n",
    "    except ApiException as e:\n",
    "        print(\"Exception when calling SeldonDeploymentsApi->read_seldon_deployment: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef81c4",
   "metadata": {},
   "source": [
    "Once the model is deployed, you can now make an inference request.\n",
    "\n",
    "For the v2 protocol, the endpoint is constructed as follows: \n",
    "\n",
    "> http://`host`/seldon/`namespace`/`deployment-name`/v2/models/`model-name`/infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90caa61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"http://{SD_IP}/seldon/{NAMESPACE}/{DEPLOYMENT_NAME}/v2/models/{DEPLOYMENT_NAME}-container/infer\"\n",
    "endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae41f3",
   "metadata": {},
   "source": [
    "The v2 protocol is slightly more verbose than the seldon protocol.  The full API spec can be found [here](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html).\n",
    "\n",
    "An array of `inputs` in required, with each input requiring a `name`, `shape`, `datatype`, and `data` (optionally parameters to specify the `content_type`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_request = {\n",
    "    \"parameters\": {\n",
    "        \"content_type\": \"pd\"\n",
    "    },\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"fixed acidity\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [7.4]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"volatile acidity\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [0.7000]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"citric acidity\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [0]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"residual sugar\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [1.9]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"chlorides\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [0.076]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"free sulfur dioxide\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [11]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"total sulfur dioxide\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [34]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"density\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [0.9978]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"pH\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [3.51]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"sulphates\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [0.56]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"alcohol\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": [9.4]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefe931",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(endpoint, json=inference_request)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd654e7",
   "metadata": {},
   "source": [
    "### 7. Using MLServer Codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39483f2",
   "metadata": {},
   "source": [
    "Crafting the payload is quite a lot of effort to do manually.  MLServer builds upon the v2 inference protocol and adds the concept of **codecs**.  This enables you to easily encode your data (e.g. pandas dataframes, numpy arrays, strings, etc.) into the v2 protocol, and decode from the v2 protocol back into your preferred data type.  More details on this can be found in the MLServer docs [here](https://mlserver.readthedocs.io/en/latest/user-guide/content-type.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlserver.types import InferenceRequest, InferenceResponse\n",
    "from mlserver.codecs import PandasCodec, NumpyCodec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50f0ff",
   "metadata": {},
   "source": [
    "Let's grab the first 5 lines in our original wine data, removing the target `quality`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189475d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_input = data.drop([\"quality\"], axis=1).head(5)\n",
    "pd_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b5513",
   "metadata": {},
   "source": [
    "Next we will encode the request from a pandas dataframe to the v2 protocol using the `PandasCodec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e26eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = PandasCodec.encode_request(pd_input)\n",
    "request.parameters = {\"content_type\": \"pd\"}\n",
    "request.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf9753",
   "metadata": {},
   "source": [
    "We can now make our request, as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(endpoint, json=request.dict())\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9785dc9",
   "metadata": {},
   "source": [
    "Finally, we parse the output response by turning it into an `InferenceResponse` object, and decoding it using the `NumpyCodec` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e293d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_payload = InferenceResponse.parse_raw(response.text)\n",
    "print(NumpyCodec.decode_output(response_payload.outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd056ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlflow-wine')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8535c43ab30ad058b0bdf8324fe88dfe0ec7bb4f9c83718153357ce41c8aec08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
